{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from multiprocessing import cpu_count\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "from ptb import PTB\n",
    "from utils import to_var, idx2word, expierment_name, AttributeDict\n",
    "from model_legacy import SentenceVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AttrDict{'data_dir': 'data/eccos', 'create_data': False, 'max_sequence_length': 50, 'min_occ': 1, 'test': False, 'epochs': 10, 'batch_size': 32, 'learning_rate': 0.001, 'embedding_size': 300, 'rnn_type': 'gru', 'hidden_size': 256, 'num_layers': 1, 'bidirectional': False, 'latent_size': 16, 'word_dropout': 0, 'embedding_dropout': 0.5, 'anneal_function': 'logistic', 'k': 0.0025, 'x0': 2500, 'print_every': 50, 'tensorboard_logging': True, 'logdir': '/root/user/work/logs/', 'save_model_path': 'bin'}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_dir = 'data/simple-examples'\n",
    "data_dir = 'data/eccos'\n",
    "\n",
    "args = {\n",
    "    'data_dir': data_dir,\n",
    "    'create_data': False,\n",
    "    'max_sequence_length': 50,\n",
    "    'min_occ': 1,\n",
    "    'test': False,\n",
    "\n",
    "    'epochs': 10,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "\n",
    "    'embedding_size': 300,\n",
    "    'rnn_type': 'gru',\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 1,\n",
    "    'bidirectional': False,\n",
    "    'latent_size': 16,\n",
    "    'word_dropout': 0,\n",
    "    'embedding_dropout': 0.5,\n",
    "\n",
    "    'anneal_function': 'logistic',\n",
    "    'k': 0.0025,\n",
    "    'x0': 2500,\n",
    "\n",
    "    'print_every': 50,\n",
    "    'tensorboard_logging': True,\n",
    "    'logdir': '/root/user/work/logs/',\n",
    "    'save_model_path': 'bin',\n",
    "}\n",
    "\n",
    "args = AttributeDict(args)\n",
    "\n",
    "args.rnn_type = args.rnn_type.lower()\n",
    "args.anneal_function = args.anneal_function.lower()\n",
    "\n",
    "assert args.rnn_type in ['rnn', 'lstm', 'gru']\n",
    "assert args.anneal_function in ['logistic', 'linear']\n",
    "assert 0 <= args.word_dropout <= 1\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data/eccos\n",
      "('train', 'src')\n",
      "vocab: 5619, records: 30726\n",
      "('train', 'tgt')\n",
      "vocab: 12106, records: 30726\n",
      "('valid', 'src')\n",
      "vocab: 5619, records: 7682\n",
      "('valid', 'tgt')\n",
      "vocab: 12106, records: 7682\n",
      "CPU times: user 941 ms, sys: 100 ms, total: 1.04 s\n",
      "Wall time: 1.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import itertools\n",
    "splits = ['train', 'valid'] + (['test'] if args.test else [])\n",
    "datasets = OrderedDict()\n",
    "print(f'loading {args.data_dir}')\n",
    "for split, src_tgt in itertools.product(splits, ['src', 'tgt']):\n",
    "    key = (split, src_tgt)\n",
    "    print(key)\n",
    "    datasets[key] = PTB(\n",
    "        data_dir=f'{args.data_dir}/{src_tgt}',\n",
    "        split=split,\n",
    "        create_data=args.create_data,\n",
    "        max_sequence_length=args.max_sequence_length if src_tgt == 'tgt' else args.max_sequence_length_src,\n",
    "        min_occ=args.min_occ\n",
    "    )\n",
    "    print(f'vocab: {datasets[key].vocab_size}, records: {len(datasets[key].data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "■ Input\n",
      "<sos> bbクリーム 進化 クリーム\n",
      "■ Target\n",
      "bbクリーム の 進化 版 ? cc クリーム が 気 に なる ... ! <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "# 実際のデータ確認\n",
    "def ids2text(id_list, ptb):\n",
    "    return ' '.join([ptb.i2w[f'{i}'] for i in id_list])\n",
    "\n",
    "_ptb_src = datasets[('train', 'src')]\n",
    "_ptb_tgt = datasets[('train', 'tgt')]\n",
    "index = str(101)\n",
    "_sample_src, _sample_tgt = _ptb_src.data[index], _ptb_tgt[index]\n",
    "print(f'■ Input\\n{ids2text(_sample_src[\"input\"], _ptb_src)}')\n",
    "print(f'■ Target\\n{ids2text(_sample_tgt[\"target\"], _ptb_tgt)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptb import SOS_INDEX, EOS_INDEX, PAD_INDEX, UNK_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceVAE(\n",
    "    vocab_size=datasets[('train', 'tgt')].vocab_size,\n",
    "    sos_idx=SOS_INDEX,\n",
    "    eos_idx=EOS_INDEX,\n",
    "    pad_idx=PAD_INDEX,\n",
    "    unk_idx=UNK_INDEX,\n",
    "    max_sequence_length=args.max_sequence_length,\n",
    "    embedding_size=args.embedding_size,\n",
    "    rnn_type=args.rnn_type,\n",
    "    hidden_size=args.hidden_size,\n",
    "    word_dropout=args.word_dropout,\n",
    "    embedding_dropout=args.embedding_dropout,\n",
    "    latent_size=args.latent_size,\n",
    "    num_layers=args.num_layers,\n",
    "    bidirectional=args.bidirectional\n",
    "    )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceVAE(\n",
       "  (embedding): Embedding(12106, 300)\n",
       "  (embedding_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (encoder_rnn): GRU(300, 256, batch_first=True)\n",
       "  (decoder_rnn): GRU(300, 256, batch_first=True)\n",
       "  (hidden2mean): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (hidden2logv): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (latent2hidden): Linear(in_features=16, out_features=256, bias=True)\n",
       "  (outputs2vocab): Linear(in_features=256, out_features=12106, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard logging: True\n"
     ]
    }
   ],
   "source": [
    "print(f'tensorboard logging: {args.tensorboard_logging}')\n",
    "ts = time.strftime('%Y-%b-%d-%H:%M:%S', time.gmtime())\n",
    "if args.tensorboard_logging:\n",
    "    writer = SummaryWriter(os.path.join(args.logdir, expierment_name(args,ts)))\n",
    "    writer.add_text(\"model\", str(model))\n",
    "    writer.add_text(\"args\", str(args))\n",
    "    writer.add_text(\"ts\", ts)\n",
    "    \n",
    "save_model_path = os.path.join(args.save_model_path, ts)\n",
    "os.makedirs(save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_anneal_function(anneal_function, step, k, x0):\n",
    "    if anneal_function == 'logistic':\n",
    "        return float(1/(1+np.exp(-k*(step-x0))))\n",
    "    elif anneal_function == 'linear':\n",
    "        return min(1, step/x0)\n",
    "\n",
    "NLL = torch.nn.NLLLoss(reduction='sum', ignore_index=PAD_INDEX)\n",
    "def loss_fn(logp, target, length, mean, logv, anneal_function, step, k, x0):\n",
    "\n",
    "    # cut-off unnecessary padding from target, and flatten\n",
    "    target = target[:, :torch.max(length).data].contiguous().view(-1)\n",
    "    logp = logp.view(-1, logp.size(2))\n",
    "\n",
    "    # Negative Log Likelihood\n",
    "    NLL_loss = NLL(logp, target)\n",
    "\n",
    "    # KL Divergence\n",
    "    KL_loss = -0.5 * torch.sum(1 + logv - mean.pow(2) - logv.exp())\n",
    "    KL_weight = kl_anneal_function(anneal_function, step, k, x0)\n",
    "\n",
    "    return NLL_loss, KL_loss, KL_weight\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys([('train', 'src'), ('train', 'tgt'), ('valid', 'src'), ('valid', 'tgt')])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> 大人気 <unk> ピンク ♡ コンビニ 買える 「 さくら リップ 」 に 限定 色 が 登場 <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "大人気 <unk> ピンク ♡ コンビニ 買える 「 さくら リップ 」 に 限定 色 が 登場 <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "ae_datasets = {split: dataset for (split, src_tgt), dataset in datasets.items() if src_tgt == 'tgt'}\n",
    "print(ids2text(ae_datasets['train'][0]['input'], ae_datasets['train']))\n",
    "print(ids2text(ae_datasets['train'][0]['target'], ae_datasets['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0000/960, Loss  174.4431, NLL-Loss  174.4424, KL-Loss    0.3833, KL-Weight  0.002\n",
      "TRAIN Batch 0050/960, Loss  126.7979, NLL-Loss  126.7803, KL-Loss    8.0805, KL-Weight  0.002\n",
      "TRAIN Batch 0100/960, Loss  114.9524, NLL-Loss  114.8851, KL-Loss   27.2398, KL-Weight  0.002\n",
      "TRAIN Batch 0150/960, Loss   97.1496, NLL-Loss   97.0229, KL-Loss   45.2173, KL-Weight  0.003\n",
      "TRAIN Batch 0200/960, Loss  112.4286, NLL-Loss  112.2623, KL-Loss   52.4073, KL-Weight  0.003\n",
      "TRAIN Batch 0250/960, Loss   98.7685, NLL-Loss   98.5479, KL-Loss   61.3728, KL-Weight  0.004\n",
      "TRAIN Batch 0300/960, Loss  100.7581, NLL-Loss  100.5164, KL-Loss   59.3994, KL-Weight  0.004\n",
      "TRAIN Batch 0350/960, Loss   91.7141, NLL-Loss   91.4150, KL-Loss   64.8698, KL-Weight  0.005\n",
      "TRAIN Batch 0400/960, Loss  102.8237, NLL-Loss  102.4598, KL-Loss   69.7192, KL-Weight  0.005\n",
      "TRAIN Batch 0450/960, Loss   93.7648, NLL-Loss   93.3642, KL-Loss   67.7618, KL-Weight  0.006\n",
      "TRAIN Batch 0500/960, Loss   92.8986, NLL-Loss   92.4696, KL-Loss   64.0864, KL-Weight  0.007\n",
      "TRAIN Batch 0550/960, Loss   90.7643, NLL-Loss   90.2542, KL-Loss   67.3304, KL-Weight  0.008\n",
      "TRAIN Batch 0600/960, Loss   82.5262, NLL-Loss   81.9682, KL-Loss   65.0569, KL-Weight  0.009\n",
      "TRAIN Batch 0650/960, Loss   87.6610, NLL-Loss   87.0236, KL-Loss   65.6535, KL-Weight  0.010\n",
      "TRAIN Batch 0700/960, Loss   81.6860, NLL-Loss   80.9515, KL-Loss   66.8513, KL-Weight  0.011\n",
      "TRAIN Batch 0750/960, Loss   77.7384, NLL-Loss   76.9630, KL-Loss   62.3744, KL-Weight  0.012\n",
      "TRAIN Batch 0800/960, Loss   98.1845, NLL-Loss   97.3038, KL-Loss   62.6271, KL-Weight  0.014\n",
      "TRAIN Batch 0850/960, Loss   85.5248, NLL-Loss   84.5626, KL-Loss   60.4920, KL-Weight  0.016\n",
      "TRAIN Batch 0900/960, Loss   77.3926, NLL-Loss   76.2793, KL-Loss   61.8973, KL-Weight  0.018\n",
      "TRAIN Batch 0950/960, Loss   70.5153, NLL-Loss   69.3385, KL-Loss   57.8774, KL-Weight  0.020\n",
      "TRAIN Batch 0960/960, Loss   89.2319, NLL-Loss   88.0013, KL-Loss   59.0604, KL-Weight  0.021\n",
      "TRAIN Epoch 00/10, Mean ELBO   97.4394\n",
      "Model saved at bin/2019-Nov-25-04:47:31/E0.pytorch\n",
      "VALID Batch 0000/240, Loss   79.0226, NLL-Loss   77.8219, KL-Loss   57.4853, KL-Weight  0.021\n",
      "VALID Batch 0050/240, Loss   80.0969, NLL-Loss   78.9227, KL-Loss   56.2169, KL-Weight  0.021\n",
      "VALID Batch 0100/240, Loss   76.6440, NLL-Loss   75.5096, KL-Loss   54.3087, KL-Weight  0.021\n",
      "VALID Batch 0150/240, Loss   85.3618, NLL-Loss   84.1652, KL-Loss   57.2872, KL-Weight  0.021\n",
      "VALID Batch 0200/240, Loss   76.4486, NLL-Loss   75.3009, KL-Loss   54.9502, KL-Weight  0.021\n",
      "VALID Batch 0240/240, Loss   50.3241, NLL-Loss   49.1721, KL-Loss   55.1521, KL-Weight  0.021\n",
      "VALID Epoch 00/10, Mean ELBO   79.9546\n",
      "TRAIN Batch 0000/960, Loss   85.2949, NLL-Loss   84.1022, KL-Loss   57.0981, KL-Weight  0.021\n",
      "TRAIN Batch 0050/960, Loss   74.9297, NLL-Loss   73.5374, KL-Loss   58.9911, KL-Weight  0.024\n",
      "TRAIN Batch 0100/960, Loss   84.8713, NLL-Loss   83.3328, KL-Loss   57.7028, KL-Weight  0.027\n",
      "TRAIN Batch 0150/960, Loss   78.7155, NLL-Loss   77.0239, KL-Loss   56.1930, KL-Weight  0.030\n",
      "TRAIN Batch 0200/960, Loss   70.8987, NLL-Loss   69.1891, KL-Loss   50.3175, KL-Weight  0.034\n",
      "TRAIN Batch 0250/960, Loss   81.0767, NLL-Loss   79.0244, KL-Loss   53.5470, KL-Weight  0.038\n",
      "TRAIN Batch 0300/960, Loss   82.8076, NLL-Loss   80.6281, KL-Loss   50.4400, KL-Weight  0.043\n",
      "TRAIN Batch 0350/960, Loss   88.5547, NLL-Loss   86.1193, KL-Loss   50.0241, KL-Weight  0.049\n",
      "TRAIN Batch 0400/960, Loss   70.6884, NLL-Loss   68.3056, KL-Loss   43.4723, KL-Weight  0.055\n",
      "TRAIN Batch 0450/960, Loss   81.8454, NLL-Loss   79.1298, KL-Loss   44.0419, KL-Weight  0.062\n",
      "TRAIN Batch 0500/960, Loss   67.9691, NLL-Loss   65.0383, KL-Loss   42.2919, KL-Weight  0.069\n",
      "TRAIN Batch 0550/960, Loss   69.4990, NLL-Loss   66.3118, KL-Loss   40.9624, KL-Weight  0.078\n",
      "TRAIN Batch 0600/960, Loss   81.0362, NLL-Loss   77.6300, KL-Loss   39.0332, KL-Weight  0.087\n",
      "TRAIN Batch 0650/960, Loss   70.2402, NLL-Loss   66.6491, KL-Loss   36.7387, KL-Weight  0.098\n",
      "TRAIN Batch 0700/960, Loss   77.1704, NLL-Loss   73.7155, KL-Loss   31.5982, KL-Weight  0.109\n",
      "TRAIN Batch 0750/960, Loss   72.3882, NLL-Loss   68.4822, KL-Loss   31.9848, KL-Weight  0.122\n",
      "TRAIN Batch 0800/960, Loss   73.5722, NLL-Loss   69.1541, KL-Loss   32.4460, KL-Weight  0.136\n",
      "TRAIN Batch 0850/960, Loss   89.1944, NLL-Loss   84.5068, KL-Loss   30.9311, KL-Weight  0.152\n",
      "TRAIN Batch 0900/960, Loss   81.8727, NLL-Loss   77.1308, KL-Loss   28.1701, KL-Weight  0.168\n",
      "TRAIN Batch 0950/960, Loss   90.5699, NLL-Loss   85.1338, KL-Loss   29.1383, KL-Weight  0.187\n",
      "TRAIN Batch 0960/960, Loss  115.3912, NLL-Loss  109.3907, KL-Loss   31.5173, KL-Weight  0.190\n",
      "TRAIN Epoch 01/10, Mean ELBO   78.8699\n",
      "Model saved at bin/2019-Nov-25-04:47:31/E1.pytorch\n",
      "VALID Batch 0000/240, Loss   77.6483, NLL-Loss   72.2863, KL-Loss   28.1066, KL-Weight  0.191\n",
      "VALID Batch 0050/240, Loss   78.7288, NLL-Loss   73.3680, KL-Loss   28.1006, KL-Weight  0.191\n",
      "VALID Batch 0100/240, Loss   75.8027, NLL-Loss   70.7978, KL-Loss   26.2354, KL-Weight  0.191\n",
      "VALID Batch 0150/240, Loss   83.2910, NLL-Loss   77.8895, KL-Loss   28.3140, KL-Weight  0.191\n",
      "VALID Batch 0200/240, Loss   75.5622, NLL-Loss   70.4104, KL-Loss   27.0050, KL-Weight  0.191\n",
      "VALID Batch 0240/240, Loss   49.0128, NLL-Loss   43.7170, KL-Loss   27.7598, KL-Weight  0.191\n",
      "VALID Epoch 01/10, Mean ELBO   78.6895\n",
      "TRAIN Batch 0000/960, Loss   67.9790, NLL-Loss   62.4307, KL-Loss   29.0839, KL-Weight  0.191\n",
      "TRAIN Batch 0050/960, Loss   74.5724, NLL-Loss   68.8033, KL-Loss   27.3650, KL-Weight  0.211\n",
      "TRAIN Batch 0100/960, Loss   72.2136, NLL-Loss   66.0344, KL-Loss   26.5926, KL-Weight  0.232\n",
      "TRAIN Batch 0150/960, Loss   88.6186, NLL-Loss   81.7481, KL-Loss   26.9004, KL-Weight  0.255\n",
      "TRAIN Batch 0200/960, Loss   69.7060, NLL-Loss   63.4169, KL-Loss   22.4698, KL-Weight  0.280\n",
      "TRAIN Batch 0250/960, Loss   79.0232, NLL-Loss   71.9247, KL-Loss   23.2156, KL-Weight  0.306\n",
      "TRAIN Batch 0300/960, Loss   72.7685, NLL-Loss   65.6392, KL-Loss   21.4144, KL-Weight  0.333\n",
      "TRAIN Batch 0350/960, Loss   88.1938, NLL-Loss   80.7790, KL-Loss   20.5262, KL-Weight  0.361\n",
      "TRAIN Batch 0400/960, Loss   75.4936, NLL-Loss   68.0605, KL-Loss   19.0324, KL-Weight  0.391\n",
      "TRAIN Batch 0450/960, Loss   78.7764, NLL-Loss   70.7407, KL-Loss   19.1018, KL-Weight  0.421\n",
      "TRAIN Batch 0500/960, Loss   91.7817, NLL-Loss   83.5262, KL-Loss   18.2885, KL-Weight  0.451\n",
      "TRAIN Batch 0550/960, Loss   84.3836, NLL-Loss   75.7073, KL-Loss   17.9817, KL-Weight  0.483\n",
      "TRAIN Batch 0600/960, Loss   78.9254, NLL-Loss   70.8667, KL-Loss   15.6862, KL-Weight  0.514\n",
      "TRAIN Batch 0650/960, Loss   90.5649, NLL-Loss   81.7706, KL-Loss   16.1399, KL-Weight  0.545\n",
      "TRAIN Batch 0700/960, Loss   85.0739, NLL-Loss   76.0634, KL-Loss   15.6524, KL-Weight  0.576\n",
      "TRAIN Batch 0750/960, Loss   73.9261, NLL-Loss   64.7630, KL-Loss   15.1239, KL-Weight  0.606\n",
      "TRAIN Batch 0800/960, Loss   85.4719, NLL-Loss   76.8713, KL-Loss   13.5380, KL-Weight  0.635\n",
      "TRAIN Batch 0850/960, Loss   85.0752, NLL-Loss   76.0969, KL-Loss   13.5269, KL-Weight  0.664\n",
      "TRAIN Batch 0900/960, Loss   80.7078, NLL-Loss   71.6119, KL-Loss   13.1626, KL-Weight  0.691\n",
      "TRAIN Batch 0950/960, Loss   93.0265, NLL-Loss   83.0571, KL-Loss   13.9028, KL-Weight  0.717\n",
      "TRAIN Batch 0960/960, Loss   73.2343, NLL-Loss   63.5401, KL-Loss   13.4246, KL-Weight  0.722\n",
      "TRAIN Epoch 02/10, Mean ELBO   80.3553\n",
      "Model saved at bin/2019-Nov-25-04:47:31/E2.pytorch\n",
      "VALID Batch 0000/240, Loss   83.3464, NLL-Loss   74.3094, KL-Loss   12.5058, KL-Weight  0.723\n",
      "VALID Batch 0050/240, Loss   83.9042, NLL-Loss   74.7746, KL-Loss   12.6339, KL-Weight  0.723\n",
      "VALID Batch 0100/240, Loss   80.1706, NLL-Loss   71.7271, KL-Loss   11.6845, KL-Weight  0.723\n",
      "VALID Batch 0150/240, Loss   90.0947, NLL-Loss   80.9856, KL-Loss   12.6056, KL-Weight  0.723\n",
      "VALID Batch 0200/240, Loss   81.0516, NLL-Loss   72.5427, KL-Loss   11.7750, KL-Weight  0.723\n",
      "VALID Batch 0240/240, Loss   54.5486, NLL-Loss   46.4074, KL-Loss   11.2661, KL-Weight  0.723\n",
      "VALID Epoch 02/10, Mean ELBO   84.7433\n",
      "TRAIN Batch 0000/960, Loss   75.7068, NLL-Loss   66.5676, KL-Loss   12.6473, KL-Weight  0.723\n",
      "TRAIN Batch 0050/960, Loss   78.6251, NLL-Loss   68.9254, KL-Loss   12.9854, KL-Weight  0.747\n",
      "TRAIN Batch 0100/960, Loss   86.2997, NLL-Loss   76.3601, KL-Loss   12.9110, KL-Weight  0.770\n",
      "TRAIN Batch 0150/960, Loss   85.5716, NLL-Loss   75.5452, KL-Loss   12.6715, KL-Weight  0.791\n",
      "TRAIN Batch 0200/960, Loss   85.8099, NLL-Loss   76.1669, KL-Loss   11.8880, KL-Weight  0.811\n",
      "TRAIN Batch 0250/960, Loss   74.7722, NLL-Loss   65.3716, KL-Loss   11.3321, KL-Weight  0.830\n",
      "TRAIN Batch 0300/960, Loss   74.5490, NLL-Loss   64.8913, KL-Loss   11.4088, KL-Weight  0.847\n",
      "TRAIN Batch 0350/960, Loss   90.1454, NLL-Loss   79.8941, KL-Loss   11.8917, KL-Weight  0.862\n",
      "TRAIN Batch 0400/960, Loss   75.8269, NLL-Loss   65.9159, KL-Loss   11.3105, KL-Weight  0.876\n",
      "TRAIN Batch 0450/960, Loss   82.3872, NLL-Loss   72.6255, KL-Loss   10.9782, KL-Weight  0.889\n",
      "TRAIN Batch 0500/960, Loss   91.3326, NLL-Loss   81.7264, KL-Loss   10.6627, KL-Weight  0.901\n",
      "TRAIN Batch 0550/960, Loss   89.3441, NLL-Loss   79.4466, KL-Loss   10.8581, KL-Weight  0.912\n",
      "TRAIN Batch 0600/960, Loss   73.0390, NLL-Loss   64.2428, KL-Loss    9.5496, KL-Weight  0.921\n",
      "TRAIN Batch 0650/960, Loss  101.6104, NLL-Loss   91.9601, KL-Loss   10.3797, KL-Weight  0.930\n",
      "TRAIN Batch 0700/960, Loss   94.8655, NLL-Loss   85.0173, KL-Loss   10.5051, KL-Weight  0.937\n",
      "TRAIN Batch 0750/960, Loss   96.9617, NLL-Loss   86.6654, KL-Loss   10.9024, KL-Weight  0.944\n",
      "TRAIN Batch 0800/960, Loss   85.3723, NLL-Loss   76.2367, KL-Loss    9.6102, KL-Weight  0.951\n",
      "TRAIN Batch 0850/960, Loss   77.2058, NLL-Loss   68.2020, KL-Loss    9.4165, KL-Weight  0.956\n",
      "TRAIN Batch 0900/960, Loss   72.0566, NLL-Loss   63.5848, KL-Loss    8.8146, KL-Weight  0.961\n",
      "TRAIN Batch 0950/960, Loss   81.7587, NLL-Loss   72.3768, KL-Loss    9.7168, KL-Weight  0.966\n",
      "TRAIN Batch 0960/960, Loss   76.5335, NLL-Loss   68.6963, KL-Loss    8.1102, KL-Weight  0.966\n",
      "TRAIN Epoch 03/10, Mean ELBO   82.5779\n",
      "Model saved at bin/2019-Nov-25-04:47:31/E3.pytorch\n",
      "VALID Batch 0000/240, Loss   85.3394, NLL-Loss   76.4230, KL-Loss    9.2261, KL-Weight  0.966\n",
      "VALID Batch 0050/240, Loss   85.2089, NLL-Loss   76.3219, KL-Loss    9.1957, KL-Weight  0.966\n",
      "VALID Batch 0100/240, Loss   82.5952, NLL-Loss   74.2649, KL-Loss    8.6197, KL-Weight  0.966\n",
      "VALID Batch 0150/240, Loss   90.1109, NLL-Loss   81.3174, KL-Loss    9.0990, KL-Weight  0.966\n",
      "VALID Batch 0200/240, Loss   81.6833, NLL-Loss   73.5234, KL-Loss    8.4434, KL-Weight  0.966\n",
      "VALID Batch 0240/240, Loss   53.3588, NLL-Loss   45.4380, KL-Loss    8.1960, KL-Weight  0.966\n",
      "VALID Epoch 03/10, Mean ELBO   85.8196\n",
      "TRAIN Batch 0000/960, Loss   71.9844, NLL-Loss   63.4927, KL-Loss    8.7866, KL-Weight  0.966\n",
      "TRAIN Batch 0050/960, Loss   79.3499, NLL-Loss   70.0279, KL-Loss    9.6077, KL-Weight  0.970\n",
      "TRAIN Batch 0100/960, Loss   83.9168, NLL-Loss   74.1853, KL-Loss    9.9948, KL-Weight  0.974\n",
      "TRAIN Batch 0150/960, Loss   82.8700, NLL-Loss   73.1277, KL-Loss    9.9748, KL-Weight  0.977\n",
      "TRAIN Batch 0200/960, Loss   91.6536, NLL-Loss   81.5996, KL-Loss   10.2658, KL-Weight  0.979\n",
      "TRAIN Batch 0250/960, Loss   80.1773, NLL-Loss   70.4622, KL-Loss    9.8957, KL-Weight  0.982\n",
      "TRAIN Batch 0300/960, Loss   79.5535, NLL-Loss   70.5963, KL-Loss    9.1042, KL-Weight  0.984\n",
      "TRAIN Batch 0350/960, Loss   82.4046, NLL-Loss   72.8134, KL-Loss    9.7300, KL-Weight  0.986\n",
      "TRAIN Batch 0400/960, Loss   77.4021, NLL-Loss   68.3944, KL-Loss    9.1228, KL-Weight  0.987\n",
      "TRAIN Batch 0450/960, Loss   79.1615, NLL-Loss   70.3614, KL-Loss    8.8994, KL-Weight  0.989\n",
      "TRAIN Batch 0500/960, Loss   86.2942, NLL-Loss   76.4206, KL-Loss    9.9718, KL-Weight  0.990\n",
      "TRAIN Batch 0550/960, Loss   91.2182, NLL-Loss   82.0899, KL-Loss    9.2084, KL-Weight  0.991\n",
      "TRAIN Batch 0600/960, Loss   74.3112, NLL-Loss   65.4847, KL-Loss    8.8949, KL-Weight  0.992\n",
      "TRAIN Batch 0650/960, Loss   83.4711, NLL-Loss   74.4675, KL-Loss    9.0651, KL-Weight  0.993\n",
      "TRAIN Batch 0700/960, Loss   80.7977, NLL-Loss   72.4164, KL-Loss    8.4319, KL-Weight  0.994\n",
      "TRAIN Batch 0750/960, Loss   80.2452, NLL-Loss   71.5978, KL-Loss    8.6935, KL-Weight  0.995\n",
      "TRAIN Batch 0800/960, Loss   86.7607, NLL-Loss   77.6617, KL-Loss    9.1418, KL-Weight  0.995\n",
      "TRAIN Batch 0850/960, Loss   83.8268, NLL-Loss   74.7092, KL-Loss    9.1555, KL-Weight  0.996\n",
      "TRAIN Batch 0900/960, Loss   93.3925, NLL-Loss   84.0701, KL-Loss    9.3565, KL-Weight  0.996\n",
      "TRAIN Batch 0950/960, Loss   77.6230, NLL-Loss   68.5561, KL-Loss    9.0962, KL-Weight  0.997\n",
      "TRAIN Batch 0960/960, Loss   59.9995, NLL-Loss   51.7148, KL-Loss    8.3108, KL-Weight  0.997\n",
      "TRAIN Epoch 04/10, Mean ELBO   80.7519\n",
      "Model saved at bin/2019-Nov-25-04:47:31/E4.pytorch\n",
      "VALID Batch 0000/240, Loss   84.7632, NLL-Loss   76.0019, KL-Loss    8.7889, KL-Weight  0.997\n",
      "VALID Batch 0050/240, Loss   84.9101, NLL-Loss   75.9240, KL-Loss    9.0143, KL-Weight  0.997\n",
      "VALID Batch 0100/240, Loss   81.1572, NLL-Loss   72.9253, KL-Loss    8.2577, KL-Weight  0.997\n",
      "VALID Batch 0150/240, Loss   89.0987, NLL-Loss   80.3414, KL-Loss    8.7849, KL-Weight  0.997\n",
      "VALID Batch 0200/240, Loss   82.0326, NLL-Loss   73.8095, KL-Loss    8.2489, KL-Weight  0.997\n",
      "VALID Batch 0240/240, Loss   56.1489, NLL-Loss   48.1357, KL-Loss    8.0383, KL-Weight  0.997\n",
      "VALID Epoch 04/10, Mean ELBO   85.2293\n",
      "TRAIN Batch 0000/960, Loss   80.8127, NLL-Loss   71.5799, KL-Loss    9.2618, KL-Weight  0.997\n",
      "TRAIN Batch 0050/960, Loss   81.9455, NLL-Loss   73.3911, KL-Loss    8.5782, KL-Weight  0.997\n",
      "TRAIN Batch 0100/960, Loss   78.4623, NLL-Loss   69.3402, KL-Loss    9.1444, KL-Weight  0.998\n",
      "TRAIN Batch 0150/960, Loss   79.2688, NLL-Loss   69.9545, KL-Loss    9.3344, KL-Weight  0.998\n",
      "TRAIN Batch 0200/960, Loss   81.1348, NLL-Loss   71.7203, KL-Loss    9.4325, KL-Weight  0.998\n",
      "TRAIN Batch 0250/960, Loss   79.2349, NLL-Loss   69.7611, KL-Loss    9.4897, KL-Weight  0.998\n",
      "TRAIN Batch 0300/960, Loss   74.1831, NLL-Loss   65.1261, KL-Loss    9.0705, KL-Weight  0.999\n",
      "TRAIN Batch 0350/960, Loss   92.0056, NLL-Loss   82.4922, KL-Loss    9.5259, KL-Weight  0.999\n",
      "TRAIN Batch 0400/960, Loss   76.1052, NLL-Loss   67.1675, KL-Loss    8.9480, KL-Weight  0.999\n",
      "TRAIN Batch 0450/960, Loss   80.3578, NLL-Loss   71.7389, KL-Loss    8.6276, KL-Weight  0.999\n",
      "TRAIN Batch 0500/960, Loss   89.6078, NLL-Loss   80.1960, KL-Loss    9.4203, KL-Weight  0.999\n",
      "TRAIN Batch 0550/960, Loss   82.3858, NLL-Loss   72.9205, KL-Loss    9.4729, KL-Weight  0.999\n",
      "TRAIN Batch 0600/960, Loss   78.7497, NLL-Loss   69.9246, KL-Loss    8.8313, KL-Weight  0.999\n",
      "TRAIN Batch 0650/960, Loss   76.6727, NLL-Loss   67.8802, KL-Loss    8.7980, KL-Weight  0.999\n",
      "TRAIN Batch 0700/960, Loss   78.3537, NLL-Loss   68.9179, KL-Loss    9.4409, KL-Weight  0.999\n",
      "TRAIN Batch 0750/960, Loss   86.7889, NLL-Loss   78.3411, KL-Loss    8.4519, KL-Weight  1.000\n",
      "TRAIN Batch 0800/960, Loss   86.2714, NLL-Loss   77.5153, KL-Loss    8.7598, KL-Weight  1.000\n",
      "TRAIN Batch 0850/960, Loss   82.0286, NLL-Loss   73.8049, KL-Loss    8.2268, KL-Weight  1.000\n",
      "TRAIN Batch 0900/960, Loss   93.0233, NLL-Loss   83.4396, KL-Loss    9.5869, KL-Weight  1.000\n",
      "TRAIN Batch 0950/960, Loss   69.5800, NLL-Loss   61.7034, KL-Loss    7.8789, KL-Weight  1.000\n",
      "TRAIN Batch 0960/960, Loss   71.1558, NLL-Loss   62.5737, KL-Loss    8.5845, KL-Weight  1.000\n",
      "TRAIN Epoch 05/10, Mean ELBO   78.6313\n",
      "Model saved at bin/2019-Nov-25-04:47:31/E5.pytorch\n",
      "VALID Batch 0000/240, Loss   84.3835, NLL-Loss   75.7066, KL-Loss    8.6793, KL-Weight  1.000\n",
      "VALID Batch 0050/240, Loss   85.1343, NLL-Loss   76.4634, KL-Loss    8.6733, KL-Weight  1.000\n",
      "VALID Batch 0100/240, Loss   81.0204, NLL-Loss   72.8425, KL-Loss    8.1803, KL-Weight  1.000\n",
      "VALID Batch 0150/240, Loss   89.2382, NLL-Loss   80.5739, KL-Loss    8.6668, KL-Weight  1.000\n",
      "VALID Batch 0200/240, Loss   81.9163, NLL-Loss   73.9060, KL-Loss    8.0126, KL-Weight  1.000\n",
      "VALID Batch 0240/240, Loss   49.3119, NLL-Loss   41.1780, KL-Loss    8.1363, KL-Weight  1.000\n",
      "VALID Epoch 05/10, Mean ELBO   84.6143\n",
      "TRAIN Batch 0000/960, Loss   74.7315, NLL-Loss   65.6862, KL-Loss    9.0479, KL-Weight  1.000\n",
      "TRAIN Batch 0050/960, Loss   75.7633, NLL-Loss   65.8488, KL-Loss    9.9169, KL-Weight  1.000\n",
      "TRAIN Batch 0100/960, Loss   79.5137, NLL-Loss   70.5612, KL-Loss    8.9545, KL-Weight  1.000\n",
      "TRAIN Batch 0150/960, Loss   80.0313, NLL-Loss   70.8172, KL-Loss    9.2159, KL-Weight  1.000\n",
      "TRAIN Batch 0200/960, Loss   77.0440, NLL-Loss   67.8176, KL-Loss    9.2280, KL-Weight  1.000\n",
      "TRAIN Batch 0250/960, Loss   78.7282, NLL-Loss   69.2722, KL-Loss    9.4574, KL-Weight  1.000\n",
      "TRAIN Batch 0300/960, Loss   83.4510, NLL-Loss   74.1266, KL-Loss    9.3256, KL-Weight  1.000\n",
      "TRAIN Batch 0350/960, Loss   79.7110, NLL-Loss   70.9758, KL-Loss    8.7362, KL-Weight  1.000\n",
      "TRAIN Batch 0400/960, Loss   77.4971, NLL-Loss   68.1795, KL-Loss    9.3186, KL-Weight  1.000\n",
      "TRAIN Batch 0450/960, Loss   75.6600, NLL-Loss   66.7748, KL-Loss    8.8860, KL-Weight  1.000\n",
      "TRAIN Batch 0500/960, Loss   65.6113, NLL-Loss   56.9845, KL-Loss    8.6274, KL-Weight  1.000\n",
      "TRAIN Batch 0550/960, Loss   67.4153, NLL-Loss   57.6496, KL-Loss    9.7665, KL-Weight  1.000\n",
      "TRAIN Batch 0600/960, Loss   80.3581, NLL-Loss   71.2164, KL-Loss    9.1423, KL-Weight  1.000\n",
      "TRAIN Batch 0650/960, Loss   75.7433, NLL-Loss   66.7916, KL-Loss    8.9522, KL-Weight  1.000\n",
      "TRAIN Batch 0700/960, Loss   80.0534, NLL-Loss   70.7021, KL-Loss    9.3517, KL-Weight  1.000\n",
      "TRAIN Batch 0750/960, Loss   67.7390, NLL-Loss   59.1435, KL-Loss    8.5959, KL-Weight  1.000\n",
      "TRAIN Batch 0800/960, Loss   84.0954, NLL-Loss   75.5273, KL-Loss    8.5684, KL-Weight  1.000\n",
      "TRAIN Batch 0850/960, Loss   77.6461, NLL-Loss   69.0921, KL-Loss    8.5543, KL-Weight  1.000\n",
      "TRAIN Batch 0900/960, Loss   75.3185, NLL-Loss   66.3249, KL-Loss    8.9939, KL-Weight  1.000\n",
      "TRAIN Batch 0950/960, Loss   74.8251, NLL-Loss   66.3841, KL-Loss    8.4412, KL-Weight  1.000\n",
      "TRAIN Batch 0960/960, Loss   84.3607, NLL-Loss   75.0839, KL-Loss    9.2771, KL-Weight  1.000\n",
      "TRAIN Epoch 06/10, Mean ELBO   76.8058\n",
      "Model saved at bin/2019-Nov-25-04:47:31/E6.pytorch\n",
      "VALID Batch 0000/240, Loss   83.4934, NLL-Loss   74.8333, KL-Loss    8.6603, KL-Weight  1.000\n",
      "VALID Batch 0050/240, Loss   84.3073, NLL-Loss   75.2081, KL-Loss    9.0995, KL-Weight  1.000\n",
      "VALID Batch 0100/240, Loss   81.6783, NLL-Loss   73.2554, KL-Loss    8.4231, KL-Weight  1.000\n",
      "VALID Batch 0150/240, Loss   89.3061, NLL-Loss   80.4836, KL-Loss    8.8227, KL-Weight  1.000\n",
      "VALID Batch 0200/240, Loss   80.3542, NLL-Loss   72.1463, KL-Loss    8.2081, KL-Weight  1.000\n",
      "VALID Batch 0240/240, Loss   49.9269, NLL-Loss   41.9601, KL-Loss    7.9670, KL-Weight  1.000\n",
      "VALID Epoch 06/10, Mean ELBO   84.3058\n",
      "TRAIN Batch 0000/960, Loss   81.1991, NLL-Loss   71.8562, KL-Loss    9.3431, KL-Weight  1.000\n",
      "TRAIN Batch 0050/960, Loss   72.2728, NLL-Loss   63.1899, KL-Loss    9.0831, KL-Weight  1.000\n",
      "TRAIN Batch 0100/960, Loss   75.7441, NLL-Loss   65.8769, KL-Loss    9.8674, KL-Weight  1.000\n",
      "TRAIN Batch 0150/960, Loss   71.0656, NLL-Loss   61.6618, KL-Loss    9.4039, KL-Weight  1.000\n",
      "TRAIN Batch 0200/960, Loss   73.7272, NLL-Loss   64.2063, KL-Loss    9.5211, KL-Weight  1.000\n",
      "TRAIN Batch 0250/960, Loss   69.8569, NLL-Loss   61.5445, KL-Loss    8.3125, KL-Weight  1.000\n",
      "TRAIN Batch 0300/960, Loss   77.2493, NLL-Loss   68.3932, KL-Loss    8.8563, KL-Weight  1.000\n",
      "TRAIN Batch 0350/960, Loss   73.2414, NLL-Loss   64.0708, KL-Loss    9.1707, KL-Weight  1.000\n",
      "TRAIN Batch 0400/960, Loss   68.7961, NLL-Loss   59.8617, KL-Loss    8.9344, KL-Weight  1.000\n",
      "TRAIN Batch 0450/960, Loss   76.4840, NLL-Loss   67.1771, KL-Loss    9.3070, KL-Weight  1.000\n",
      "TRAIN Batch 0500/960, Loss   80.6511, NLL-Loss   71.2540, KL-Loss    9.3972, KL-Weight  1.000\n",
      "TRAIN Batch 0550/960, Loss   69.7254, NLL-Loss   61.4325, KL-Loss    8.2930, KL-Weight  1.000\n",
      "TRAIN Batch 0600/960, Loss   77.6870, NLL-Loss   68.4299, KL-Loss    9.2571, KL-Weight  1.000\n",
      "TRAIN Batch 0650/960, Loss   80.5762, NLL-Loss   70.9098, KL-Loss    9.6664, KL-Weight  1.000\n",
      "TRAIN Batch 0700/960, Loss   75.2385, NLL-Loss   65.8753, KL-Loss    9.3632, KL-Weight  1.000\n",
      "TRAIN Batch 0750/960, Loss   76.4440, NLL-Loss   67.4849, KL-Loss    8.9592, KL-Weight  1.000\n",
      "TRAIN Batch 0800/960, Loss   76.7168, NLL-Loss   68.2521, KL-Loss    8.4647, KL-Weight  1.000\n",
      "TRAIN Batch 0850/960, Loss   95.9785, NLL-Loss   86.4502, KL-Loss    9.5283, KL-Weight  1.000\n",
      "TRAIN Batch 0900/960, Loss   69.4161, NLL-Loss   61.2503, KL-Loss    8.1658, KL-Weight  1.000\n",
      "TRAIN Batch 0950/960, Loss   70.6215, NLL-Loss   61.8680, KL-Loss    8.7535, KL-Weight  1.000\n",
      "TRAIN Batch 0960/960, Loss   47.1354, NLL-Loss   39.9297, KL-Loss    7.2057, KL-Weight  1.000\n",
      "TRAIN Epoch 07/10, Mean ELBO   75.2301\n",
      "Model saved at bin/2019-Nov-25-04:47:31/E7.pytorch\n",
      "VALID Batch 0000/240, Loss   83.6465, NLL-Loss   75.1920, KL-Loss    8.4546, KL-Weight  1.000\n",
      "VALID Batch 0050/240, Loss   84.6093, NLL-Loss   75.6448, KL-Loss    8.9644, KL-Weight  1.000\n",
      "VALID Batch 0100/240, Loss   79.9220, NLL-Loss   71.7014, KL-Loss    8.2206, KL-Weight  1.000\n",
      "VALID Batch 0150/240, Loss   88.4771, NLL-Loss   79.7738, KL-Loss    8.7033, KL-Weight  1.000\n",
      "VALID Batch 0200/240, Loss   80.4504, NLL-Loss   72.4692, KL-Loss    7.9812, KL-Weight  1.000\n",
      "VALID Batch 0240/240, Loss   51.9146, NLL-Loss   44.0199, KL-Loss    7.8948, KL-Weight  1.000\n",
      "VALID Epoch 07/10, Mean ELBO   84.0775\n",
      "TRAIN Batch 0000/960, Loss   76.1303, NLL-Loss   67.0929, KL-Loss    9.0374, KL-Weight  1.000\n",
      "TRAIN Batch 0050/960, Loss   67.0986, NLL-Loss   57.7832, KL-Loss    9.3154, KL-Weight  1.000\n",
      "TRAIN Batch 0100/960, Loss   73.4260, NLL-Loss   64.7795, KL-Loss    8.6465, KL-Weight  1.000\n",
      "TRAIN Batch 0150/960, Loss   66.8090, NLL-Loss   58.0766, KL-Loss    8.7324, KL-Weight  1.000\n",
      "TRAIN Batch 0200/960, Loss   75.2806, NLL-Loss   65.7671, KL-Loss    9.5134, KL-Weight  1.000\n",
      "TRAIN Batch 0250/960, Loss   65.6899, NLL-Loss   56.6116, KL-Loss    9.0783, KL-Weight  1.000\n",
      "TRAIN Batch 0300/960, Loss   68.2312, NLL-Loss   59.3227, KL-Loss    8.9085, KL-Weight  1.000\n",
      "TRAIN Batch 0350/960, Loss   71.2452, NLL-Loss   62.4243, KL-Loss    8.8209, KL-Weight  1.000\n",
      "TRAIN Batch 0400/960, Loss   69.9010, NLL-Loss   60.5753, KL-Loss    9.3258, KL-Weight  1.000\n",
      "TRAIN Batch 0450/960, Loss   71.5237, NLL-Loss   62.6570, KL-Loss    8.8667, KL-Weight  1.000\n",
      "TRAIN Batch 0500/960, Loss   73.0838, NLL-Loss   63.7757, KL-Loss    9.3081, KL-Weight  1.000\n",
      "TRAIN Batch 0550/960, Loss   73.6566, NLL-Loss   64.3414, KL-Loss    9.3152, KL-Weight  1.000\n",
      "TRAIN Batch 0600/960, Loss   73.4068, NLL-Loss   64.1554, KL-Loss    9.2514, KL-Weight  1.000\n",
      "TRAIN Batch 0650/960, Loss   84.4454, NLL-Loss   74.7154, KL-Loss    9.7300, KL-Weight  1.000\n",
      "TRAIN Batch 0700/960, Loss   74.3125, NLL-Loss   65.6172, KL-Loss    8.6953, KL-Weight  1.000\n",
      "TRAIN Batch 0750/960, Loss   72.3484, NLL-Loss   63.5907, KL-Loss    8.7577, KL-Weight  1.000\n",
      "TRAIN Batch 0800/960, Loss   71.1795, NLL-Loss   62.1648, KL-Loss    9.0148, KL-Weight  1.000\n",
      "TRAIN Batch 0850/960, Loss   68.2571, NLL-Loss   59.1141, KL-Loss    9.1430, KL-Weight  1.000\n",
      "TRAIN Batch 0900/960, Loss   75.6166, NLL-Loss   66.0174, KL-Loss    9.5992, KL-Weight  1.000\n",
      "TRAIN Batch 0950/960, Loss   81.1683, NLL-Loss   72.2258, KL-Loss    8.9425, KL-Weight  1.000\n",
      "TRAIN Batch 0960/960, Loss   83.0112, NLL-Loss   72.7862, KL-Loss   10.2250, KL-Weight  1.000\n",
      "TRAIN Epoch 08/10, Mean ELBO   73.7922\n",
      "Model saved at bin/2019-Nov-25-04:47:31/E8.pytorch\n",
      "VALID Batch 0000/240, Loss   82.1900, NLL-Loss   73.5018, KL-Loss    8.6882, KL-Weight  1.000\n",
      "VALID Batch 0050/240, Loss   84.6129, NLL-Loss   75.6763, KL-Loss    8.9366, KL-Weight  1.000\n",
      "VALID Batch 0100/240, Loss   80.0988, NLL-Loss   71.8677, KL-Loss    8.2311, KL-Weight  1.000\n",
      "VALID Batch 0150/240, Loss   87.9952, NLL-Loss   79.2793, KL-Loss    8.7159, KL-Weight  1.000\n",
      "VALID Batch 0200/240, Loss   80.0210, NLL-Loss   72.0460, KL-Loss    7.9750, KL-Weight  1.000\n",
      "VALID Batch 0240/240, Loss   51.5657, NLL-Loss   43.6480, KL-Loss    7.9177, KL-Weight  1.000\n",
      "VALID Epoch 08/10, Mean ELBO   84.0083\n",
      "TRAIN Batch 0000/960, Loss   73.8323, NLL-Loss   64.7931, KL-Loss    9.0392, KL-Weight  1.000\n",
      "TRAIN Batch 0050/960, Loss   68.8266, NLL-Loss   59.8738, KL-Loss    8.9528, KL-Weight  1.000\n",
      "TRAIN Batch 0100/960, Loss   77.5255, NLL-Loss   68.4225, KL-Loss    9.1030, KL-Weight  1.000\n",
      "TRAIN Batch 0150/960, Loss   72.5359, NLL-Loss   63.4563, KL-Loss    9.0796, KL-Weight  1.000\n",
      "TRAIN Batch 0200/960, Loss   65.8565, NLL-Loss   56.3399, KL-Loss    9.5166, KL-Weight  1.000\n",
      "TRAIN Batch 0250/960, Loss   68.5807, NLL-Loss   59.4947, KL-Loss    9.0860, KL-Weight  1.000\n",
      "TRAIN Batch 0300/960, Loss   72.1674, NLL-Loss   63.1770, KL-Loss    8.9904, KL-Weight  1.000\n",
      "TRAIN Batch 0350/960, Loss   78.2634, NLL-Loss   69.2983, KL-Loss    8.9651, KL-Weight  1.000\n",
      "TRAIN Batch 0400/960, Loss   80.8481, NLL-Loss   71.2187, KL-Loss    9.6294, KL-Weight  1.000\n",
      "TRAIN Batch 0450/960, Loss   69.9949, NLL-Loss   61.7052, KL-Loss    8.2896, KL-Weight  1.000\n",
      "TRAIN Batch 0500/960, Loss   67.9030, NLL-Loss   59.2838, KL-Loss    8.6192, KL-Weight  1.000\n",
      "TRAIN Batch 0550/960, Loss   69.7710, NLL-Loss   61.1373, KL-Loss    8.6336, KL-Weight  1.000\n",
      "TRAIN Batch 0600/960, Loss   79.1678, NLL-Loss   69.9757, KL-Loss    9.1921, KL-Weight  1.000\n",
      "TRAIN Batch 0650/960, Loss   79.5970, NLL-Loss   70.5856, KL-Loss    9.0114, KL-Weight  1.000\n",
      "TRAIN Batch 0700/960, Loss   66.9852, NLL-Loss   58.5426, KL-Loss    8.4426, KL-Weight  1.000\n",
      "TRAIN Batch 0750/960, Loss   68.4341, NLL-Loss   59.9012, KL-Loss    8.5329, KL-Weight  1.000\n",
      "TRAIN Batch 0800/960, Loss   69.4349, NLL-Loss   60.8810, KL-Loss    8.5540, KL-Weight  1.000\n",
      "TRAIN Batch 0850/960, Loss   71.7167, NLL-Loss   63.0732, KL-Loss    8.6435, KL-Weight  1.000\n",
      "TRAIN Batch 0900/960, Loss   70.1464, NLL-Loss   61.3573, KL-Loss    8.7890, KL-Weight  1.000\n",
      "TRAIN Batch 0950/960, Loss   72.1390, NLL-Loss   62.5782, KL-Loss    9.5608, KL-Weight  1.000\n",
      "TRAIN Batch 0960/960, Loss   71.4582, NLL-Loss   63.3366, KL-Loss    8.1216, KL-Weight  1.000\n",
      "TRAIN Epoch 09/10, Mean ELBO   72.5201\n",
      "Model saved at bin/2019-Nov-25-04:47:31/E9.pytorch\n",
      "VALID Batch 0000/240, Loss   82.9570, NLL-Loss   74.5349, KL-Loss    8.4221, KL-Weight  1.000\n",
      "VALID Batch 0050/240, Loss   83.6772, NLL-Loss   74.6517, KL-Loss    9.0256, KL-Weight  1.000\n",
      "VALID Batch 0100/240, Loss   80.3178, NLL-Loss   72.0399, KL-Loss    8.2780, KL-Weight  1.000\n",
      "VALID Batch 0150/240, Loss   88.0233, NLL-Loss   79.2979, KL-Loss    8.7254, KL-Weight  1.000\n",
      "VALID Batch 0200/240, Loss   80.0054, NLL-Loss   72.0149, KL-Loss    7.9905, KL-Weight  1.000\n",
      "VALID Batch 0240/240, Loss   52.4371, NLL-Loss   44.4820, KL-Loss    7.9552, KL-Weight  1.000\n",
      "VALID Epoch 09/10, Mean ELBO   83.9808\n"
     ]
    }
   ],
   "source": [
    "_datasets = ae_datasets\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    for split in splits:\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            dataset=_datasets[split],\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=split=='train',\n",
    "            num_workers=cpu_count(),\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "\n",
    "        tracker = defaultdict(tensor)\n",
    "\n",
    "        # Enable/Disable Dropout\n",
    "        if split == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        for iteration, batch in enumerate(data_loader):\n",
    "            \n",
    "            batch_size = batch['input'].size(0)\n",
    "            \n",
    "            for k, v in batch.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    batch[k] = to_var(v)\n",
    "                    \n",
    "            # Forward pass\n",
    "            logp, mean, logv, z = model(batch['input'], batch['length'])\n",
    "\n",
    "            # loss calculation\n",
    "            NLL_loss, KL_loss, KL_weight = loss_fn(logp, batch['target'],\n",
    "                batch['length'], mean, logv, args.anneal_function, step, args.k, args.x0)\n",
    "\n",
    "            loss = (NLL_loss + KL_weight * KL_loss)/batch_size\n",
    "\n",
    "            # backward + optimization\n",
    "            if split == 'train':\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                step += 1\n",
    "\n",
    "            # bookkeepeing\n",
    "            tracker['ELBO'] = torch.cat((tracker['ELBO'], loss.data.view(1)))\n",
    "\n",
    "            if args.tensorboard_logging:\n",
    "                writer.add_scalar(\"%s/ELBO\"%split.upper(), loss.data.item(), epoch*len(data_loader) + iteration)\n",
    "                writer.add_scalar(\"%s/NLL Loss\"%split.upper(), NLL_loss.data.item()/batch_size, epoch*len(data_loader) + iteration)\n",
    "                writer.add_scalar(\"%s/KL Loss\"%split.upper(), KL_loss.data.item()/batch_size, epoch*len(data_loader) + iteration)\n",
    "                writer.add_scalar(\"%s/KL Weight\"%split.upper(), KL_weight, epoch*len(data_loader) + iteration)\n",
    "\n",
    "            if iteration % args.print_every == 0 or iteration+1 == len(data_loader):\n",
    "                print(\"%s Batch %04d/%i, Loss %9.4f, NLL-Loss %9.4f, KL-Loss %9.4f, KL-Weight %6.3f\"\n",
    "                    %(split.upper(), iteration, len(data_loader)-1, loss.data.item(), NLL_loss.data.item()/batch_size, KL_loss.data.item()/batch_size, KL_weight))\n",
    "\n",
    "            if split == 'valid':\n",
    "                if 'target_sents' not in tracker:\n",
    "                    tracker['target_sents'] = list()\n",
    "                tracker['target_sents'] += idx2word(batch['target'].data, i2w=_datasets['train'].get_i2w(), pad_idx=_datasets['train'].pad_idx)\n",
    "                tracker['z'] = torch.cat((tracker['z'], z.data), dim=0)\n",
    "\n",
    "        print(\"%s Epoch %02d/%i, Mean ELBO %9.4f\"%(split.upper(), epoch, args.epochs, torch.mean(tracker['ELBO'])))\n",
    "\n",
    "        if args.tensorboard_logging:\n",
    "            writer.add_scalar(\"%s-Epoch/ELBO\"%split.upper(), torch.mean(tracker['ELBO']), epoch)\n",
    "\n",
    "        # save a dump of all sentences and the encoded latent space\n",
    "        if split == 'valid':\n",
    "            dump = {'target_sents':tracker['target_sents'], 'z':tracker['z'].tolist()}\n",
    "            if not os.path.exists(os.path.join('dumps', ts)):\n",
    "                os.makedirs('dumps/'+ts)\n",
    "            with open(os.path.join('dumps/'+ts+'/valid_E%i.json'%epoch), 'w') as dump_file:\n",
    "                json.dump(dump,dump_file)\n",
    "\n",
    "        # save checkpoint\n",
    "        if split == 'train':\n",
    "            checkpoint_path = os.path.join(save_model_path, \"E%i.pytorch\"%(epoch))\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(\"Model saved at %s\"%checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
