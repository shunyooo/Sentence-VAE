{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from multiprocessing import cpu_count\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "from ptb import PTB\n",
    "from utils import to_var, idx2word, experiment_name, AttributeDict\n",
    "from model_bowloss import SentenceVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AttrDict{'data_dir': 'data/eccos', 'create_data': False, 'max_sequence_length': 50, 'min_occ': 1, 'test': False, 'epochs': 10, 'batch_size': 32, 'learning_rate': 0.001, 'embedding_size': 300, 'rnn_type': 'gru', 'hidden_size': 256, 'num_layers': 1, 'bidirectional': False, 'latent_size': 16, 'word_dropout': 0, 'embedding_dropout': 0.5, 'anneal_function': 'logistic', 'k': 0.0025, 'x0': 2500, 'print_every': 50, 'tensorboard_logging': True, 'logdir': '/root/user/work/logs/', 'save_model_path': 'bin', 'experiment_name': 'kw2copy_test'}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_dir = 'data/simple-examples'\n",
    "data_dir = 'data/eccos'\n",
    "\n",
    "args = {\n",
    "    'data_dir': data_dir,\n",
    "    'create_data': False,\n",
    "    'max_sequence_length': 50,\n",
    "    'min_occ': 1,\n",
    "    'test': False,\n",
    "\n",
    "    'epochs': 10,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "\n",
    "    'embedding_size': 300,\n",
    "    'rnn_type': 'gru',\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 1,\n",
    "    'bidirectional': False,\n",
    "    'latent_size': 16,\n",
    "    'word_dropout': 0,\n",
    "    'embedding_dropout': 0.5,\n",
    "\n",
    "    'anneal_function': 'logistic',\n",
    "    'k': 0.0025,\n",
    "    'x0': 2500,\n",
    "\n",
    "    'print_every': 50,\n",
    "    'tensorboard_logging': True,\n",
    "    'logdir': '/root/user/work/logs/',\n",
    "    'save_model_path': 'bin',\n",
    "    'experiment_name': 'kw2copy_test',\n",
    "}\n",
    "\n",
    "args = AttributeDict(args)\n",
    "\n",
    "args.rnn_type = args.rnn_type.lower()\n",
    "args.anneal_function = args.anneal_function.lower()\n",
    "\n",
    "assert args.rnn_type in ['rnn', 'lstm', 'gru']\n",
    "assert args.anneal_function in ['logistic', 'linear']\n",
    "assert 0 <= args.word_dropout <= 1\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data/eccos\n",
      "('train', 'src')\n",
      "vocab: 5619, records: 30726\n",
      "('train', 'tgt')\n",
      "vocab: 12106, records: 30726\n",
      "('valid', 'src')\n",
      "vocab: 5619, records: 7682\n",
      "('valid', 'tgt')\n",
      "vocab: 12106, records: 7682\n",
      "CPU times: user 937 ms, sys: 88.3 ms, total: 1.03 s\n",
      "Wall time: 1.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import itertools\n",
    "splits = ['train', 'valid'] + (['test'] if args.test else [])\n",
    "datasets = OrderedDict()\n",
    "print(f'loading {args.data_dir}')\n",
    "for split, src_tgt in itertools.product(splits, ['src', 'tgt']):\n",
    "    key = (split, src_tgt)\n",
    "    print(key)\n",
    "    datasets[key] = PTB(\n",
    "        data_dir=f'{args.data_dir}/{src_tgt}',\n",
    "        split=split,\n",
    "        create_data=args.create_data,\n",
    "        max_sequence_length=args.max_sequence_length if src_tgt == 'tgt' else args.max_sequence_length_src,\n",
    "        min_occ=args.min_occ\n",
    "    )\n",
    "    print(f'vocab: {datasets[key].vocab_size}, records: {len(datasets[key].data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "■ src-input \n",
      "<sos> bbクリーム 進化 クリーム\n",
      "■ src-target \n",
      "bbクリーム 進化 クリーム <eos>\n",
      "■ tgt-input\n",
      "<sos> bbクリーム の 進化 版 ? cc クリーム が 気 に なる ... ! <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "■ tgt-target\n",
      "bbクリーム の 進化 版 ? cc クリーム が 気 に なる ... ! <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "# 実際のデータ確認\n",
    "def ids2text(id_list, ptb):\n",
    "    return ' '.join([ptb.i2w[f'{i}'] for i in id_list])\n",
    "\n",
    "_ptb_src = datasets[('train', 'src')]\n",
    "_ptb_tgt = datasets[('train', 'tgt')]\n",
    "index = str(101)\n",
    "_sample_src, _sample_tgt = _ptb_src.data[index], _ptb_tgt[index]\n",
    "print(f'■ src-input \\n{ids2text(_sample_src[\"input\"], _ptb_src)}')\n",
    "print(f'■ src-target \\n{ids2text(_sample_src[\"target\"], _ptb_src)}')\n",
    "print(f'■ tgt-input\\n{ids2text(_sample_tgt[\"input\"], _ptb_tgt)}')\n",
    "print(f'■ tgt-target\\n{ids2text(_sample_tgt[\"target\"], _ptb_tgt)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptb import SOS_INDEX, EOS_INDEX, PAD_INDEX, UNK_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "model = SentenceVAE(\n",
    "    vocab_size=datasets[('train', 'src')].vocab_size,\n",
    "    sos_idx=SOS_INDEX,\n",
    "    eos_idx=EOS_INDEX,\n",
    "    pad_idx=PAD_INDEX,\n",
    "    unk_idx=UNK_INDEX,\n",
    "    max_sequence_length=args.max_sequence_length,\n",
    "    embedding_size=args.embedding_size,\n",
    "    rnn_type=args.rnn_type,\n",
    "    hidden_size=args.hidden_size,\n",
    "    word_dropout=args.word_dropout,\n",
    "    embedding_dropout=args.embedding_dropout,\n",
    "    latent_size=args.latent_size,\n",
    "    num_layers=args.num_layers,\n",
    "    bidirectional=args.bidirectional,\n",
    "    \n",
    "    # bow loss\n",
    "    # bow_hidden_size=256,\n",
    "    use_bow_loss=False,\n",
    "    \n",
    "    # kw base\n",
    "    out_vocab_size=datasets[('train', 'tgt')].vocab_size,\n",
    "    )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceVAE(\n",
       "  (embedding): Embedding(5619, 300)\n",
       "  (embedding_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (decoder_embedding): Embedding(12106, 300)\n",
       "  (encoder_rnn): GRU(300, 256, batch_first=True)\n",
       "  (decoder_rnn): GRU(300, 256, batch_first=True)\n",
       "  (hidden2mean): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (hidden2logv): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (latent2hidden): Linear(in_features=16, out_features=256, bias=True)\n",
       "  (outputs2vocab): Linear(in_features=256, out_features=12106, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard logging: True\n",
      "tensorboard logging → /root/user/work/logs/kw2copy_test_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500_TS=2019-Dec-20-12:26:27\n",
      "model save → bin/kw2copy_test_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500_TS=2019-Dec-20-12:26:27\n"
     ]
    }
   ],
   "source": [
    "print(f'tensorboard logging: {args.tensorboard_logging}')\n",
    "ts = time.strftime('%Y-%b-%d-%H:%M:%S', time.gmtime())\n",
    "exp_name = experiment_name(args,ts)\n",
    "\n",
    "if args.tensorboard_logging:\n",
    "    writer_path = os.path.join(args.logdir, exp_name)\n",
    "    writer = SummaryWriter(writer_path)\n",
    "    writer.add_text(\"model\", str(model))\n",
    "    writer.add_text(\"args\", str(args))\n",
    "    writer.add_text(\"ts\", ts)\n",
    "    print(f'tensorboard logging → {writer_path}')\n",
    "    \n",
    "save_model_path = os.path.join(args.save_model_path, exp_name)\n",
    "os.makedirs(save_model_path)\n",
    "print(f'model save → {save_model_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys([('train', 'src'), ('train', 'tgt'), ('valid', 'src'), ('valid', 'tgt')])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> 大人気 <unk> ピンク ♡ コンビニ 買える 「 さくら リップ 」 に 限定 色 が 登場 <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "大人気 <unk> ピンク ♡ コンビニ 買える 「 さくら リップ 」 に 限定 色 が 登場 <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "ae_datasets = {split: dataset for (split, src_tgt), dataset in datasets.items() if src_tgt == 'tgt'}\n",
    "print(ids2text(ae_datasets['train'][0]['input'], ae_datasets['train']))\n",
    "print(ids2text(ae_datasets['train'][0]['target'], ae_datasets['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "_datasets = {}\n",
    "for split in splits:\n",
    "    src_dataset = datasets[(split, 'src')]\n",
    "    tgt_dataset = datasets[(split, 'tgt')]\n",
    "    assert len(src_dataset) == len(tgt_dataset)\n",
    "    dataset = []\n",
    "    for i in range(len(src_dataset)):\n",
    "        src_set, tgt_set = src_dataset[i], tgt_dataset[i]\n",
    "        _data = {}\n",
    "        _data.update({f'src_{k}': v for k,v in src_set.items()})\n",
    "        _data.update({f'tgt_{k}': v for k,v in tgt_set.items()})\n",
    "        dataset.append(_data)\n",
    "    _datasets[split] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': array([  2, 854, 754, 869]),\n",
       " 'target': array([854, 754, 869,   3]),\n",
       " 'length': 4}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'valid'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_input': array([2, 4, 5, 6]),\n",
       " 'src_target': array([4, 5, 6, 3]),\n",
       " 'src_length': 4,\n",
       " 'tgt_input': array([ 2,  4,  1,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'tgt_target': array([ 4,  1,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,  3,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'tgt_length': 16}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ptb.PTB at 0x7fa9bca01cc0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target_ptb = datasets[('train', 'tgt')]\n",
    "train_target_ptb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n",
      "TRAIN Batch 0000/960, Loss  174.3948, NLL-Loss  174.3940, KL-Loss    0.3943, KL-Weight  0.002\n",
      "TRAIN Batch 0050/960, Loss  129.5814, NLL-Loss  129.5426, KL-Loss   17.7855, KL-Weight  0.002\n",
      "TRAIN Batch 0100/960, Loss  113.7719, NLL-Loss  113.6965, KL-Loss   30.5091, KL-Weight  0.002\n",
      "TRAIN Batch 0150/960, Loss   97.6049, NLL-Loss   97.5142, KL-Loss   32.3728, KL-Weight  0.003\n",
      "TRAIN Batch 0200/960, Loss  122.7353, NLL-Loss  122.6225, KL-Loss   35.5411, KL-Weight  0.003\n",
      "TRAIN Batch 0250/960, Loss   95.6496, NLL-Loss   95.5139, KL-Loss   37.7425, KL-Weight  0.004\n",
      "TRAIN Batch 0300/960, Loss  107.8232, NLL-Loss  107.6549, KL-Loss   41.3454, KL-Weight  0.004\n",
      "TRAIN Batch 0350/960, Loss  120.9445, NLL-Loss  120.7799, KL-Loss   35.7126, KL-Weight  0.005\n",
      "TRAIN Batch 0400/960, Loss   85.8580, NLL-Loss   85.6327, KL-Loss   43.1744, KL-Weight  0.005\n",
      "TRAIN Batch 0450/960, Loss  101.0920, NLL-Loss  100.8756, KL-Loss   36.6010, KL-Weight  0.006\n",
      "TRAIN Batch 0500/960, Loss   99.0875, NLL-Loss   98.8193, KL-Loss   40.0758, KL-Weight  0.007\n",
      "TRAIN Batch 0550/960, Loss   98.5134, NLL-Loss   98.2061, KL-Loss   40.5552, KL-Weight  0.008\n",
      "TRAIN Batch 0600/960, Loss   92.1386, NLL-Loss   91.7767, KL-Loss   42.1954, KL-Weight  0.009\n",
      "TRAIN Batch 0650/960, Loss   95.6641, NLL-Loss   95.2590, KL-Loss   41.7214, KL-Weight  0.010\n",
      "TRAIN Batch 0700/960, Loss   88.4551, NLL-Loss   88.0220, KL-Loss   39.4226, KL-Weight  0.011\n",
      "TRAIN Batch 0750/960, Loss   88.2579, NLL-Loss   87.7911, KL-Loss   37.5497, KL-Weight  0.012\n",
      "TRAIN Batch 0800/960, Loss   83.0213, NLL-Loss   82.4657, KL-Loss   39.5054, KL-Weight  0.014\n",
      "TRAIN Batch 0850/960, Loss  101.4800, NLL-Loss  100.8868, KL-Loss   37.2911, KL-Weight  0.016\n",
      "TRAIN Batch 0900/960, Loss   88.6184, NLL-Loss   87.8989, KL-Loss   40.0017, KL-Weight  0.018\n",
      "TRAIN Batch 0950/960, Loss   91.8283, NLL-Loss   91.1131, KL-Loss   35.1747, KL-Weight  0.020\n",
      "TRAIN Batch 0960/960, Loss   63.5213, NLL-Loss   62.7017, KL-Loss   39.3333, KL-Weight  0.021\n",
      "TRAIN Epoch 00/10, Mean ELBO  103.1824\n",
      "Model saved at bin/kw2copy_test_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500_TS=2019-Dec-20-12:26:27/E0.pytorch\n",
      "VALID Batch 0000/240, Loss   86.2486, NLL-Loss   85.5009, KL-Loss   35.7985, KL-Weight  0.021\n",
      "VALID Batch 0050/240, Loss   88.6536, NLL-Loss   87.9322, KL-Loss   34.5416, KL-Weight  0.021\n",
      "VALID Batch 0100/240, Loss   82.4592, NLL-Loss   81.7311, KL-Loss   34.8592, KL-Weight  0.021\n",
      "VALID Batch 0150/240, Loss   94.2629, NLL-Loss   93.5713, KL-Loss   33.1112, KL-Weight  0.021\n",
      "VALID Batch 0200/240, Loss   82.5754, NLL-Loss   81.8228, KL-Loss   36.0298, KL-Weight  0.021\n",
      "VALID Batch 0240/240, Loss   56.2841, NLL-Loss   55.6040, KL-Loss   32.5646, KL-Weight  0.021\n",
      "VALID Epoch 00/10, Mean ELBO   87.8199\n",
      "TRAIN Batch 0000/960, Loss   90.2386, NLL-Loss   89.5070, KL-Loss   35.0260, KL-Weight  0.021\n",
      "TRAIN Batch 0050/960, Loss   96.5937, NLL-Loss   95.7432, KL-Loss   36.0324, KL-Weight  0.024\n",
      "TRAIN Batch 0100/960, Loss   88.3072, NLL-Loss   87.2971, KL-Loss   37.8822, KL-Weight  0.027\n",
      "TRAIN Batch 0150/960, Loss   67.7281, NLL-Loss   66.5679, KL-Loss   38.5383, KL-Weight  0.030\n",
      "TRAIN Batch 0200/960, Loss   91.7113, NLL-Loss   90.5899, KL-Loss   33.0047, KL-Weight  0.034\n",
      "TRAIN Batch 0250/960, Loss   86.4126, NLL-Loss   85.0761, KL-Loss   34.8697, KL-Weight  0.038\n",
      "TRAIN Batch 0300/960, Loss   89.3154, NLL-Loss   87.9677, KL-Loss   31.1897, KL-Weight  0.043\n",
      "TRAIN Batch 0350/960, Loss   92.6116, NLL-Loss   91.2429, KL-Loss   28.1132, KL-Weight  0.049\n",
      "TRAIN Batch 0400/960, Loss   81.7537, NLL-Loss   80.3021, KL-Loss   26.4845, KL-Weight  0.055\n",
      "TRAIN Batch 0450/960, Loss   84.2525, NLL-Loss   82.5979, KL-Loss   26.8345, KL-Weight  0.062\n",
      "TRAIN Batch 0500/960, Loss   76.9972, NLL-Loss   75.2546, KL-Loss   25.1454, KL-Weight  0.069\n",
      "TRAIN Batch 0550/960, Loss   79.9033, NLL-Loss   78.0460, KL-Loss   23.8697, KL-Weight  0.078\n",
      "TRAIN Batch 0600/960, Loss   85.5465, NLL-Loss   83.6928, KL-Loss   21.2425, KL-Weight  0.087\n",
      "TRAIN Batch 0650/960, Loss   91.2546, NLL-Loss   89.0317, KL-Loss   22.7411, KL-Weight  0.098\n",
      "TRAIN Batch 0700/960, Loss   81.3082, NLL-Loss   79.1854, KL-Loss   19.4146, KL-Weight  0.109\n",
      "TRAIN Batch 0750/960, Loss   83.1029, NLL-Loss   80.9898, KL-Loss   17.3029, KL-Weight  0.122\n",
      "TRAIN Batch 0800/960, Loss   90.5229, NLL-Loss   88.1936, KL-Loss   17.1062, KL-Weight  0.136\n",
      "TRAIN Batch 0850/960, Loss   80.6213, NLL-Loss   78.1066, KL-Loss   16.5928, KL-Weight  0.152\n",
      "TRAIN Batch 0900/960, Loss   96.5231, NLL-Loss   94.0967, KL-Loss   14.4144, KL-Weight  0.168\n",
      "TRAIN Batch 0950/960, Loss   96.2575, NLL-Loss   93.4258, KL-Loss   15.1780, KL-Weight  0.187\n",
      "TRAIN Batch 0960/960, Loss   75.8990, NLL-Loss   72.9559, KL-Loss   15.4588, KL-Weight  0.190\n",
      "TRAIN Epoch 01/10, Mean ELBO   86.3184\n",
      "Model saved at bin/kw2copy_test_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500_TS=2019-Dec-20-12:26:27/E1.pytorch\n",
      "VALID Batch 0000/240, Loss   83.3937, NLL-Loss   80.5918, KL-Loss   14.6870, KL-Weight  0.191\n",
      "VALID Batch 0050/240, Loss   84.5233, NLL-Loss   81.8804, KL-Loss   13.8538, KL-Weight  0.191\n",
      "VALID Batch 0100/240, Loss   79.6622, NLL-Loss   76.9818, KL-Loss   14.0502, KL-Weight  0.191\n",
      "VALID Batch 0150/240, Loss   89.8446, NLL-Loss   87.2788, KL-Loss   13.4495, KL-Weight  0.191\n",
      "VALID Batch 0200/240, Loss   80.4190, NLL-Loss   77.6277, KL-Loss   14.6317, KL-Weight  0.191\n",
      "VALID Batch 0240/240, Loss   52.9415, NLL-Loss   50.2639, KL-Loss   14.0356, KL-Weight  0.191\n",
      "VALID Epoch 01/10, Mean ELBO   84.4719\n",
      "TRAIN Batch 0000/960, Loss   87.3880, NLL-Loss   84.6105, KL-Loss   14.5594, KL-Weight  0.191\n",
      "TRAIN Batch 0050/960, Loss   84.1308, NLL-Loss   80.8192, KL-Loss   15.7086, KL-Weight  0.211\n",
      "TRAIN Batch 0100/960, Loss   81.6222, NLL-Loss   78.1087, KL-Loss   15.1207, KL-Weight  0.232\n",
      "TRAIN Batch 0150/960, Loss   78.7418, NLL-Loss   75.1828, KL-Loss   13.9346, KL-Weight  0.255\n",
      "TRAIN Batch 0200/960, Loss   79.7996, NLL-Loss   76.2544, KL-Loss   12.6663, KL-Weight  0.280\n",
      "TRAIN Batch 0250/960, Loss   81.6882, NLL-Loss   77.9928, KL-Loss   12.0857, KL-Weight  0.306\n",
      "TRAIN Batch 0300/960, Loss   74.8712, NLL-Loss   70.7765, KL-Loss   12.2995, KL-Weight  0.333\n",
      "TRAIN Batch 0350/960, Loss   87.5055, NLL-Loss   83.5711, KL-Loss   10.8915, KL-Weight  0.361\n",
      "TRAIN Batch 0400/960, Loss   83.2824, NLL-Loss   79.3665, KL-Loss   10.0266, KL-Weight  0.391\n",
      "TRAIN Batch 0450/960, Loss   92.4685, NLL-Loss   88.2564, KL-Loss   10.0128, KL-Weight  0.421\n",
      "TRAIN Batch 0500/960, Loss   78.0490, NLL-Loss   74.0282, KL-Loss    8.9072, KL-Weight  0.451\n",
      "TRAIN Batch 0550/960, Loss   93.4400, NLL-Loss   89.2709, KL-Loss    8.6406, KL-Weight  0.483\n",
      "TRAIN Batch 0600/960, Loss   84.9177, NLL-Loss   80.7457, KL-Loss    8.1207, KL-Weight  0.514\n",
      "TRAIN Batch 0650/960, Loss   90.7679, NLL-Loss   86.5472, KL-Loss    7.7462, KL-Weight  0.545\n",
      "TRAIN Batch 0700/960, Loss   88.0513, NLL-Loss   83.6748, KL-Loss    7.6025, KL-Weight  0.576\n",
      "TRAIN Batch 0750/960, Loss  104.0515, NLL-Loss  100.0487, KL-Loss    6.6066, KL-Weight  0.606\n",
      "TRAIN Batch 0800/960, Loss   68.0952, NLL-Loss   63.5298, KL-Loss    7.1863, KL-Weight  0.635\n",
      "TRAIN Batch 0850/960, Loss   89.9405, NLL-Loss   85.6634, KL-Loss    6.4439, KL-Weight  0.664\n",
      "TRAIN Batch 0900/960, Loss   78.6492, NLL-Loss   73.9930, KL-Loss    6.7378, KL-Weight  0.691\n",
      "TRAIN Batch 0950/960, Loss   79.0701, NLL-Loss   74.3337, KL-Loss    6.6051, KL-Weight  0.717\n",
      "TRAIN Batch 0960/960, Loss   90.0409, NLL-Loss   85.7438, KL-Loss    5.9507, KL-Weight  0.722\n",
      "TRAIN Epoch 02/10, Mean ELBO   84.0441\n",
      "Model saved at bin/kw2copy_test_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500_TS=2019-Dec-20-12:26:27/E2.pytorch\n",
      "VALID Batch 0000/240, Loss   84.5294, NLL-Loss   80.1532, KL-Loss    6.0560, KL-Weight  0.723\n",
      "VALID Batch 0050/240, Loss   86.2476, NLL-Loss   82.0075, KL-Loss    5.8676, KL-Weight  0.723\n",
      "VALID Batch 0100/240, Loss   81.3845, NLL-Loss   77.2081, KL-Loss    5.7794, KL-Weight  0.723\n",
      "VALID Batch 0150/240, Loss   91.4699, NLL-Loss   87.4092, KL-Loss    5.6194, KL-Weight  0.723\n",
      "VALID Batch 0200/240, Loss   83.2667, NLL-Loss   78.8061, KL-Loss    6.1727, KL-Weight  0.723\n",
      "VALID Batch 0240/240, Loss   54.8836, NLL-Loss   50.5140, KL-Loss    6.0469, KL-Weight  0.723\n",
      "VALID Epoch 02/10, Mean ELBO   86.2078\n",
      "TRAIN Batch 0000/960, Loss   79.8727, NLL-Loss   75.5495, KL-Loss    5.9826, KL-Weight  0.723\n",
      "TRAIN Batch 0050/960, Loss   82.5257, NLL-Loss   77.7412, KL-Loss    6.4053, KL-Weight  0.747\n",
      "TRAIN Batch 0100/960, Loss   83.3904, NLL-Loss   78.5218, KL-Loss    6.3240, KL-Weight  0.770\n"
     ]
    }
   ],
   "source": [
    "%pdb on\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    for split in splits:\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            dataset=_datasets[split],\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=split=='train',\n",
    "            num_workers=cpu_count(),\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "\n",
    "        tracker = defaultdict(tensor)\n",
    "\n",
    "        # Enable/Disable Dropout\n",
    "        if split == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        for iteration, batch in enumerate(data_loader):\n",
    "            \n",
    "            batch_size = batch['src_input'].size(0)\n",
    "            \n",
    "            for k, v in batch.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    batch[k] = to_var(v)\n",
    "            \n",
    "            # loss calculation\n",
    "            logp, mean, logv, z = model(batch['src_input'], batch['src_length'], batch['tgt_input'], batch['tgt_length'])\n",
    "            loss_dict = model.loss(logp, batch['tgt_target'], batch['tgt_length'], mean, logv, args.anneal_function, step, args.k, args.x0, z)\n",
    "            loss, NLL_loss, KL_weight, KL_loss, avg_bow_loss = loss_dict['loss'], loss_dict['NLL_loss'], loss_dict['KL_weight'], loss_dict['KL_loss'], loss_dict.get('avg_bow_loss')\n",
    "\n",
    "            # backward + optimization\n",
    "            if split == 'train':\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                step += 1\n",
    "\n",
    "            # bookkeepeing\n",
    "            tracker['ELBO'] = torch.cat((tracker['ELBO'], loss.data.view(1)))\n",
    "\n",
    "            if args.tensorboard_logging:\n",
    "                writer.add_scalar(\"%s/ELBO\"%split.upper(), loss.data.item(), epoch*len(data_loader) + iteration)\n",
    "                writer.add_scalar(\"%s/NLL Loss\"%split.upper(), NLL_loss.data.item()/batch_size, epoch*len(data_loader) + iteration)\n",
    "                writer.add_scalar(\"%s/KL Loss\"%split.upper(), KL_loss.data.item()/batch_size, epoch*len(data_loader) + iteration)\n",
    "                writer.add_scalar(\"%s/KL Weight\"%split.upper(), KL_weight, epoch*len(data_loader) + iteration)\n",
    "                if avg_bow_loss is not None:\n",
    "                    writer.add_scalar(\"%s/BOW Loss\"%split.upper(), avg_bow_loss, epoch*len(data_loader) + iteration)\n",
    "\n",
    "            if iteration % args.print_every == 0 or iteration+1 == len(data_loader):\n",
    "                print_text = \"%s Batch %04d/%i, Loss %9.4f, NLL-Loss %9.4f, KL-Loss %9.4f, KL-Weight %6.3f\"%(split.upper(), iteration, len(data_loader)-1, loss.data.item(), NLL_loss.data.item()/batch_size, KL_loss.data.item()/batch_size, KL_weight)\n",
    "                if avg_bow_loss is not None:\n",
    "                    print_text += ', BOW Loss %9.4f,'%(avg_bow_loss)\n",
    "                print(print_text)\n",
    "\n",
    "            if split == 'valid':\n",
    "                if 'target_sents' not in tracker:\n",
    "                    tracker['target_sents'] = list()\n",
    "                tracker['target_sents'] += idx2word(batch['tgt_target'].data, i2w=train_target_ptb.get_i2w(), pad_idx=PAD_INDEX)\n",
    "                tracker['z'] = torch.cat((tracker['z'], z.data), dim=0)\n",
    "\n",
    "        print(\"%s Epoch %02d/%i, Mean ELBO %9.4f\"%(split.upper(), epoch, args.epochs, torch.mean(tracker['ELBO'])))\n",
    "\n",
    "        if args.tensorboard_logging:\n",
    "            writer.add_scalar(\"%s-Epoch/ELBO\"%split.upper(), torch.mean(tracker['ELBO']), epoch)\n",
    "\n",
    "        # save a dump of all sentences and the encoded latent space\n",
    "        if split == 'valid':\n",
    "            dump = {'target_sents':tracker['target_sents'], 'z':tracker['z'].tolist()}\n",
    "            if not os.path.exists(os.path.join('dumps', ts)):\n",
    "                os.makedirs('dumps/'+ts)\n",
    "            with open(os.path.join('dumps/'+ts+'/valid_E%i.json'%epoch), 'w') as dump_file:\n",
    "                json.dump(dump,dump_file)\n",
    "\n",
    "        # save checkpoint\n",
    "        if split == 'train':\n",
    "            checkpoint_path = os.path.join(save_model_path, \"E%i.pytorch\"%(epoch))\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(\"Model saved at %s\"%checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
