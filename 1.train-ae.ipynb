{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from multiprocessing import cpu_count\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "from ptb import PTB\n",
    "from utils import to_var, idx2word, expierment_name, AttributeDict\n",
    "from model_src_src import SentenceVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AttrDict{'data_dir': 'data/eccos', 'create_data': False, 'max_sequence_length': 50, 'min_occ': 1, 'test': False, 'epochs': 10, 'batch_size': 32, 'learning_rate': 0.001, 'embedding_size': 300, 'rnn_type': 'gru', 'hidden_size': 256, 'num_layers': 1, 'bidirectional': False, 'latent_size': 16, 'word_dropout': 0, 'embedding_dropout': 0.5, 'anneal_function': 'logistic', 'k': 0.0025, 'x0': 2500, 'print_every': 50, 'tensorboard_logging': False, 'logdir': '/root/user/work/logs/', 'save_model_path': 'bin'}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_dir = 'data/simple-examples'\n",
    "data_dir = 'data/eccos'\n",
    "\n",
    "args = {\n",
    "    'data_dir': data_dir,\n",
    "    'create_data': False,\n",
    "    'max_sequence_length': 50,\n",
    "    'min_occ': 1,\n",
    "    'test': False,\n",
    "\n",
    "    'epochs': 10,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "\n",
    "    'embedding_size': 300,\n",
    "    'rnn_type': 'gru',\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 1,\n",
    "    'bidirectional': False,\n",
    "    'latent_size': 16,\n",
    "    'word_dropout': 0,\n",
    "    'embedding_dropout': 0.5,\n",
    "\n",
    "    'anneal_function': 'logistic',\n",
    "    'k': 0.0025,\n",
    "    'x0': 2500,\n",
    "\n",
    "    'print_every': 50,\n",
    "    'tensorboard_logging': False,\n",
    "    'logdir': '/root/user/work/logs/',\n",
    "    'save_model_path': 'bin',\n",
    "}\n",
    "\n",
    "args = AttributeDict(args)\n",
    "\n",
    "args.rnn_type = args.rnn_type.lower()\n",
    "args.anneal_function = args.anneal_function.lower()\n",
    "\n",
    "assert args.rnn_type in ['rnn', 'lstm', 'gru']\n",
    "assert args.anneal_function in ['logistic', 'linear']\n",
    "assert 0 <= args.word_dropout <= 1\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data/eccos\n",
      "('train', 'src')\n",
      "vocab: 5613, records: 30726\n",
      "('train', 'tgt')\n",
      "vocab: 12112, records: 30726\n",
      "('valid', 'src')\n",
      "vocab: 5613, records: 7682\n",
      "('valid', 'tgt')\n",
      "vocab: 12112, records: 7682\n",
      "CPU times: user 550 ms, sys: 57.7 ms, total: 608 ms\n",
      "Wall time: 612 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import itertools\n",
    "splits = ['train', 'valid'] + (['test'] if args.test else [])\n",
    "datasets = OrderedDict()\n",
    "print(f'loading {args.data_dir}')\n",
    "for split, src_tgt in itertools.product(splits, ['src', 'tgt']):\n",
    "    key = (split, src_tgt)\n",
    "    print(key)\n",
    "    datasets[key] = PTB(\n",
    "        data_dir=f'{args.data_dir}/{src_tgt}',\n",
    "        split=split,\n",
    "        create_data=args.create_data,\n",
    "        max_sequence_length=args.max_sequence_length if src_tgt == 'tgt' else args.max_sequence_length_src,\n",
    "        min_occ=args.min_occ\n",
    "    )\n",
    "    print(f'vocab: {datasets[key].vocab_size}, records: {len(datasets[key].data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "■ Input\n",
      "<sos> 取る 好感度 男女\n",
      "■ Target\n",
      "男女 共 に 好感度 を 総 取り でき ちゃう の 秘密 を 教え ます ! み ー ん な が 好き な サーモン の よう に 、 どんな こと が あっ て も 可愛い 女の子 を 目指し ましょ ♡ <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "# 実際のデータ確認\n",
    "def ids2text(id_list, ptb):\n",
    "    return ' '.join([ptb.i2w[f'{i}'] for i in id_list])\n",
    "\n",
    "_ptb_src = datasets[('train', 'src')]\n",
    "_ptb_tgt = datasets[('train', 'tgt')]\n",
    "index = str(101)\n",
    "_sample_src, _sample_tgt = _ptb_src.data[index], _ptb_tgt[index]\n",
    "print(f'■ Input\\n{ids2text(_sample_src[\"input\"], _ptb_src)}')\n",
    "print(f'■ Target\\n{ids2text(_sample_tgt[\"target\"], _ptb_tgt)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptb import SOS_INDEX, EOS_INDEX, PAD_INDEX, UNK_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "model = SentenceVAE(\n",
    "    vocab_size=datasets[('train', 'tgt')].vocab_size,\n",
    "#     out_vocab_size=datasets[('train', 'tgt')].vocab_size,\n",
    "    sos_idx=SOS_INDEX,\n",
    "    eos_idx=EOS_INDEX,\n",
    "    pad_idx=PAD_INDEX,\n",
    "    unk_idx=UNK_INDEX,\n",
    "    max_sequence_length=args.max_sequence_length,\n",
    "    embedding_size=args.embedding_size,\n",
    "    rnn_type=args.rnn_type,\n",
    "    hidden_size=args.hidden_size,\n",
    "    word_dropout=args.word_dropout,\n",
    "    embedding_dropout=args.embedding_dropout,\n",
    "    latent_size=args.latent_size,\n",
    "    num_layers=args.num_layers,\n",
    "    bidirectional=args.bidirectional\n",
    "    )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceVAE(\n",
       "  (embedding): Embedding(12112, 300)\n",
       "  (embedding_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (encoder_rnn): GRU(300, 256, batch_first=True)\n",
       "  (decoder_rnn): GRU(300, 256, batch_first=True)\n",
       "  (hidden2mean): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (hidden2logv): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (latent2hidden): Linear(in_features=16, out_features=256, bias=True)\n",
       "  (outputs2vocab): Linear(in_features=256, out_features=12112, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard logging: False\n"
     ]
    }
   ],
   "source": [
    "print(f'tensorboard logging: {args.tensorboard_logging}')\n",
    "ts = time.strftime('%Y-%b-%d-%H:%M:%S', time.gmtime())\n",
    "if args.tensorboard_logging:\n",
    "    writer = SummaryWriter(os.path.join(args.logdir, expierment_name(args,ts)))\n",
    "    writer.add_text(\"model\", str(model))\n",
    "    writer.add_text(\"args\", str(args))\n",
    "    writer.add_text(\"ts\", ts)\n",
    "    \n",
    "save_model_path = os.path.join(args.save_model_path, ts)\n",
    "os.makedirs(save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys([('train', 'src'), ('train', 'tgt'), ('valid', 'src'), ('valid', 'tgt')])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> 手洗い は 面倒 な イメージ が あり ます が 、 意外と 手間 が かから ない ん です ! 長持ち し やすく なる 手洗い 方法 を 確認 し て み て ♡ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "手洗い は 面倒 な イメージ が あり ます が 、 意外と 手間 が かから ない ん です ! 長持ち し やすく なる 手洗い 方法 を 確認 し て み て ♡ <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "ae_datasets = {split: dataset for (split, src_tgt), dataset in datasets.items() if src_tgt == 'tgt'}\n",
    "print(ids2text(ae_datasets['train'][0]['input'], ae_datasets['train']))\n",
    "print(ids2text(ae_datasets['train'][0]['target'], ae_datasets['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n",
      "TRAIN Batch 0000/960, Loss  174.5292, NLL-Loss  174.5285, KL-Loss    0.3569, KL-Weight  0.002\n",
      "TRAIN Batch 0050/960, Loss  118.3392, NLL-Loss  118.3227, KL-Loss    7.5676, KL-Weight  0.002\n",
      "TRAIN Batch 0100/960, Loss  125.1290, NLL-Loss  125.0559, KL-Loss   29.5639, KL-Weight  0.002\n",
      "TRAIN Batch 0150/960, Loss  112.4148, NLL-Loss  112.3134, KL-Loss   36.1796, KL-Weight  0.003\n"
     ]
    }
   ],
   "source": [
    "%pdb on\n",
    "_datasets = ae_datasets\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    for split in splits:\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            dataset=_datasets[split],\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=split=='train',\n",
    "            num_workers=cpu_count(),\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "\n",
    "        tracker = defaultdict(tensor)\n",
    "\n",
    "        # Enable/Disable Dropout\n",
    "        if split == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        for iteration, batch in enumerate(data_loader):\n",
    "            \n",
    "            batch_size = batch['input'].size(0)\n",
    "            \n",
    "            for k, v in batch.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    batch[k] = to_var(v)\n",
    "            \n",
    "            # loss calculation\n",
    "            _input, input_length, target, target_length = batch['input'], batch['length'], batch['target'], batch['length']\n",
    "            loss_dict = model.loss(_input, input_length, target, target_length, args.anneal_function, step, args.k, args.x0)\n",
    "            loss, NLL_loss, KL_weight, KL_loss = loss_dict['loss'], loss_dict['NLL_loss'], loss_dict['KL_weight'], loss_dict['KL_loss']\n",
    "\n",
    "            # backward + optimization\n",
    "            if split == 'train':\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                step += 1\n",
    "\n",
    "            # bookkeepeing\n",
    "            tracker['ELBO'] = torch.cat((tracker['ELBO'], loss.data.view(1)))\n",
    "\n",
    "            if args.tensorboard_logging:\n",
    "                writer.add_scalar(\"%s/ELBO\"%split.upper(), loss.data.item(), epoch*len(data_loader) + iteration)\n",
    "                writer.add_scalar(\"%s/NLL Loss\"%split.upper(), NLL_loss.data.item()/batch_size, epoch*len(data_loader) + iteration)\n",
    "                writer.add_scalar(\"%s/KL Loss\"%split.upper(), KL_loss.data.item()/batch_size, epoch*len(data_loader) + iteration)\n",
    "                writer.add_scalar(\"%s/KL Weight\"%split.upper(), KL_weight, epoch*len(data_loader) + iteration)\n",
    "\n",
    "            if iteration % args.print_every == 0 or iteration+1 == len(data_loader):\n",
    "                print(\"%s Batch %04d/%i, Loss %9.4f, NLL-Loss %9.4f, KL-Loss %9.4f, KL-Weight %6.3f\"\n",
    "                    %(split.upper(), iteration, len(data_loader)-1, loss.data.item(), NLL_loss.data.item()/batch_size, KL_loss.data.item()/batch_size, KL_weight))\n",
    "\n",
    "            if split == 'valid':\n",
    "                if 'target_sents' not in tracker:\n",
    "                    tracker['target_sents'] = list()\n",
    "                tracker['target_sents'] += idx2word(batch['target'].data, i2w=_datasets['train'].get_i2w(), pad_idx=_datasets['train'].pad_idx)\n",
    "                tracker['z'] = torch.cat((tracker['z'], z.data), dim=0)\n",
    "\n",
    "        print(\"%s Epoch %02d/%i, Mean ELBO %9.4f\"%(split.upper(), epoch, args.epochs, torch.mean(tracker['ELBO'])))\n",
    "\n",
    "        if args.tensorboard_logging:\n",
    "            writer.add_scalar(\"%s-Epoch/ELBO\"%split.upper(), torch.mean(tracker['ELBO']), epoch)\n",
    "\n",
    "        # save a dump of all sentences and the encoded latent space\n",
    "        if split == 'valid':\n",
    "            dump = {'target_sents':tracker['target_sents'], 'z':tracker['z'].tolist()}\n",
    "            if not os.path.exists(os.path.join('dumps', ts)):\n",
    "                os.makedirs('dumps/'+ts)\n",
    "            with open(os.path.join('dumps/'+ts+'/valid_E%i.json'%epoch), 'w') as dump_file:\n",
    "                json.dump(dump,dump_file)\n",
    "\n",
    "        # save checkpoint\n",
    "        if split == 'train':\n",
    "            checkpoint_path = os.path.join(save_model_path, \"E%i.pytorch\"%(epoch))\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(\"Model saved at %s\"%checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
