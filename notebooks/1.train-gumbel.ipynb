{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from multiprocessing import cpu_count\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "from ptb import PTB\n",
    "from utils import to_var, idx2word, expierment_name, AttributeDict\n",
    "from model_gumbel import SentenceVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AttrDict{'data_dir': 'data/eccos', 'create_data': False, 'max_sequence_length': 50, 'min_occ': 1, 'test': True, 'epochs': 20, 'batch_size': 32, 'learning_rate': 0.001, 'embedding_size': 300, 'rnn_type': 'gru', 'hidden_size': 256, 'num_layers': 1, 'bidirectional': False, 'latent_size': 16, 'word_dropout': 0, 'embedding_dropout': 0.5, 'anneal_function': 'logistic', 'k': 0.001, 'x0': 10000, 'print_every': 50, 'tensorboard_logging': True, 'logdir': '/root/user/work/logs/', 'save_model_path': 'bin', 'expierment_name': 'copy2copy_gumbel_latent16_epoch20_tau=0.1_k=0.001_x0=10000', 'gumbel_tau': 0.1}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_dir = 'data/simple-examples'\n",
    "data_dir = 'data/eccos'\n",
    "epoch=20\n",
    "latent_size=16\n",
    "gumbel_tau=0.1\n",
    "kl_k=0.001\n",
    "kl_x0=10000\n",
    "\n",
    "args = {\n",
    "    'data_dir': data_dir,\n",
    "    'create_data': False,\n",
    "    'max_sequence_length': 50,\n",
    "    'min_occ': 1,\n",
    "    'test': True,\n",
    "\n",
    "    'epochs': epoch,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "\n",
    "    'embedding_size': 300,\n",
    "    'rnn_type': 'gru',\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 1,\n",
    "    'bidirectional': False,\n",
    "    'latent_size': latent_size,\n",
    "    'word_dropout': 0,\n",
    "    'embedding_dropout': 0.5,\n",
    "\n",
    "    'anneal_function': 'logistic',\n",
    "    'k': kl_k,\n",
    "    'x0': kl_x0,\n",
    "\n",
    "    'print_every': 50,\n",
    "    'tensorboard_logging': True,\n",
    "    'logdir': '/root/user/work/logs/',\n",
    "    'save_model_path': 'bin',\n",
    "    'expierment_name': f'copy2copy_gumbel_latent{latent_size}_epoch{epoch}_tau={gumbel_tau}_k={kl_k}_x0={kl_x0}', \n",
    "    \n",
    "    'gumbel_tau': gumbel_tau,\n",
    "}\n",
    "\n",
    "args = AttributeDict(args)\n",
    "\n",
    "args.rnn_type = args.rnn_type.lower()\n",
    "args.anneal_function = args.anneal_function.lower()\n",
    "\n",
    "assert args.rnn_type in ['rnn', 'lstm', 'gru']\n",
    "assert args.anneal_function in ['logistic', 'linear']\n",
    "assert 0 <= args.word_dropout <= 1\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data/eccos\n",
      "('train', 'src')\n",
      "vocab: 5619, records: 30726\n",
      "('train', 'tgt')\n",
      "vocab: 12106, records: 30726\n",
      "('valid', 'src')\n",
      "vocab: 5619, records: 7682\n",
      "('valid', 'tgt')\n",
      "vocab: 12106, records: 7682\n",
      "('test', 'src')\n",
      "vocab: 5619, records: 9603\n",
      "('test', 'tgt')\n",
      "vocab: 12106, records: 9603\n",
      "CPU times: user 1.39 s, sys: 101 ms, total: 1.49 s\n",
      "Wall time: 1.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import itertools\n",
    "splits = ['train', 'valid'] + (['test'] if args.test else [])\n",
    "datasets = OrderedDict()\n",
    "print(f'loading {args.data_dir}')\n",
    "for split, src_tgt in itertools.product(splits, ['src', 'tgt']):\n",
    "    key = (split, src_tgt)\n",
    "    print(key)\n",
    "    datasets[key] = PTB(\n",
    "        data_dir=f'{args.data_dir}/{src_tgt}',\n",
    "        split=split,\n",
    "        create_data=args.create_data,\n",
    "        max_sequence_length=args.max_sequence_length if src_tgt == 'tgt' else args.max_sequence_length_src,\n",
    "        min_occ=args.min_occ\n",
    "    )\n",
    "    print(f'vocab: {datasets[key].vocab_size}, records: {len(datasets[key].data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "■ src-input \n",
      "<sos> bbクリーム 進化 クリーム\n",
      "■ src-target \n",
      "bbクリーム 進化 クリーム <eos>\n",
      "■ tgt-input\n",
      "<sos> bbクリーム の 進化 版 ? cc クリーム が 気 に なる ... ! <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "■ tgt-target\n",
      "bbクリーム の 進化 版 ? cc クリーム が 気 に なる ... ! <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "# 実際のデータ確認\n",
    "def ids2text(id_list, ptb):\n",
    "    return ' '.join([ptb.i2w[f'{i}'] for i in id_list])\n",
    "\n",
    "_ptb_src = datasets[('train', 'src')]\n",
    "_ptb_tgt = datasets[('train', 'tgt')]\n",
    "index = str(101)\n",
    "_sample_src, _sample_tgt = _ptb_src.data[index], _ptb_tgt[index]\n",
    "print(f'■ src-input \\n{ids2text(_sample_src[\"input\"], _ptb_src)}')\n",
    "print(f'■ src-target \\n{ids2text(_sample_src[\"target\"], _ptb_src)}')\n",
    "print(f'■ tgt-input\\n{ids2text(_sample_tgt[\"input\"], _ptb_tgt)}')\n",
    "print(f'■ tgt-target\\n{ids2text(_sample_tgt[\"target\"], _ptb_tgt)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptb import SOS_INDEX, EOS_INDEX, PAD_INDEX, UNK_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "model = SentenceVAE(\n",
    "    vocab_size=datasets[('train', 'tgt')].vocab_size,\n",
    "    sos_idx=SOS_INDEX,\n",
    "    eos_idx=EOS_INDEX,\n",
    "    pad_idx=PAD_INDEX,\n",
    "    unk_idx=UNK_INDEX,\n",
    "    max_sequence_length=args.max_sequence_length,\n",
    "    embedding_size=args.embedding_size,\n",
    "    rnn_type=args.rnn_type,\n",
    "    hidden_size=args.hidden_size,\n",
    "    word_dropout=args.word_dropout,\n",
    "    embedding_dropout=args.embedding_dropout,\n",
    "    latent_size=args.latent_size,\n",
    "    num_layers=args.num_layers,\n",
    "    bidirectional=args.bidirectional,\n",
    "    \n",
    "    # bow loss\n",
    "    # bow_hidden_size=256,\n",
    "    use_bow_loss=False,\n",
    "    \n",
    "    is_gumbel=True,\n",
    "    gumbel_tau=args.gumbel_tau,\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceVAE(\n",
       "  (embedding): Embedding(12106, 300)\n",
       "  (decoder_embedding): Embedding(12106, 300)\n",
       "  (embedding_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (encoder_rnn): GRU(300, 256, batch_first=True)\n",
       "  (decoder_rnn): GRU(300, 256, batch_first=True)\n",
       "  (hidden2gumbel): Linear(in_features=256, out_features=12106, bias=True)\n",
       "  (hidden2mean): Linear(in_features=300, out_features=16, bias=True)\n",
       "  (hidden2logv): Linear(in_features=300, out_features=16, bias=True)\n",
       "  (latent2hidden): Linear(in_features=16, out_features=256, bias=True)\n",
       "  (outputs2vocab): Linear(in_features=256, out_features=12106, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard logging: True\n"
     ]
    }
   ],
   "source": [
    "print(f'tensorboard logging: {args.tensorboard_logging}')\n",
    "ts = time.strftime('%Y-%b-%d-%H:%M:%S', time.gmtime())\n",
    "if args.tensorboard_logging:\n",
    "    writer = SummaryWriter(os.path.join(args.logdir, expierment_name(args,ts)))\n",
    "    writer.add_text(\"model\", str(model))\n",
    "    writer.add_text(\"args\", str(args))\n",
    "    writer.add_text(\"ts\", ts)\n",
    "    \n",
    "save_model_path = os.path.join(args.save_model_path, ts)\n",
    "os.makedirs(save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys([('train', 'src'), ('train', 'tgt'), ('valid', 'src'), ('valid', 'tgt'), ('test', 'src'), ('test', 'tgt')])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> 大人気 <unk> ピンク ♡ コンビニ 買える 「 さくら リップ 」 に 限定 色 が 登場 <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "大人気 <unk> ピンク ♡ コンビニ 買える 「 さくら リップ 」 に 限定 色 が 登場 <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "ae_datasets = {split: dataset for (split, src_tgt), dataset in datasets.items() if src_tgt == 'tgt'}\n",
    "print(ids2text(ae_datasets['train'][0]['input'], ae_datasets['train']))\n",
    "print(ids2text(ae_datasets['train'][0]['target'], ae_datasets['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_datasets = {}\n",
    "for split in splits:\n",
    "    src_dataset = datasets[(split, 'src')]\n",
    "    tgt_dataset = datasets[(split, 'tgt')]\n",
    "    assert len(src_dataset) == len(tgt_dataset)\n",
    "    dataset = []\n",
    "    for i in range(len(src_dataset)):\n",
    "        src_set, tgt_set = src_dataset[i], tgt_dataset[i]\n",
    "        _data = {}\n",
    "        _data.update({f'src_{k}': v for k,v in tgt_set.items()})\n",
    "        _data.update({f'tgt_{k}': v for k,v in tgt_set.items()})\n",
    "        dataset.append(_data)\n",
    "    _datasets[split] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'valid', 'test'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_input': array([ 2,  4,  1,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'src_target': array([ 4,  1,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,  3,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'src_length': 16,\n",
       " 'tgt_input': array([ 2,  4,  1,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'tgt_target': array([ 4,  1,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,  3,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'tgt_length': 16}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ptb.PTB at 0x7f4f48401d30>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target_ptb = datasets[('train', 'tgt')]\n",
    "train_target_ptb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n",
      "TRAIN Batch 0000/960, Loss  180.5082, NLL-Loss  180.5081, KL-Loss    3.7285, KL-Weight  0.000\n",
      "TRAIN Batch 0050/960, Loss  132.1216, NLL-Loss  132.1213, KL-Loss    4.6285, KL-Weight  0.000\n",
      "TRAIN Batch 0100/960, Loss  118.5340, NLL-Loss  118.5338, KL-Loss    3.7039, KL-Weight  0.000\n",
      "TRAIN Batch 0150/960, Loss  112.9919, NLL-Loss  112.9917, KL-Loss    3.9133, KL-Weight  0.000\n",
      "TRAIN Batch 0200/960, Loss  109.7903, NLL-Loss  109.7901, KL-Loss    3.5411, KL-Weight  0.000\n",
      "TRAIN Batch 0250/960, Loss  107.4168, NLL-Loss  107.4165, KL-Loss    4.0667, KL-Weight  0.000\n",
      "TRAIN Batch 0300/960, Loss  109.7005, NLL-Loss  109.7003, KL-Loss    3.3464, KL-Weight  0.000\n",
      "TRAIN Batch 0350/960, Loss  112.8913, NLL-Loss  112.8911, KL-Loss    3.5252, KL-Weight  0.000\n",
      "TRAIN Batch 0400/960, Loss  110.8961, NLL-Loss  110.8958, KL-Loss    4.5946, KL-Weight  0.000\n",
      "TRAIN Batch 0450/960, Loss  101.8585, NLL-Loss  101.8582, KL-Loss    4.7253, KL-Weight  0.000\n",
      "TRAIN Batch 0500/960, Loss   99.2519, NLL-Loss   99.2514, KL-Loss    5.7638, KL-Weight  0.000\n",
      "TRAIN Batch 0550/960, Loss  113.4634, NLL-Loss  113.4625, KL-Loss   11.4002, KL-Weight  0.000\n",
      "TRAIN Batch 0600/960, Loss   85.0801, NLL-Loss   85.0793, KL-Loss    9.1865, KL-Weight  0.000\n",
      "TRAIN Batch 0650/960, Loss  102.0603, NLL-Loss  102.0585, KL-Loss   20.9251, KL-Weight  0.000\n",
      "TRAIN Batch 0700/960, Loss  102.9913, NLL-Loss  102.9898, KL-Loss   16.1810, KL-Weight  0.000\n",
      "TRAIN Batch 0750/960, Loss  103.9744, NLL-Loss  103.9725, KL-Loss   20.0894, KL-Weight  0.000\n",
      "TRAIN Batch 0800/960, Loss  104.7184, NLL-Loss  104.7160, KL-Loss   23.5673, KL-Weight  0.000\n",
      "TRAIN Batch 0850/960, Loss   96.1111, NLL-Loss   96.1082, KL-Loss   27.1720, KL-Weight  0.000\n",
      "TRAIN Batch 0900/960, Loss  103.9125, NLL-Loss  103.9093, KL-Loss   28.6287, KL-Weight  0.000\n",
      "TRAIN Batch 0950/960, Loss   91.4328, NLL-Loss   91.4298, KL-Loss   25.4978, KL-Weight  0.000\n",
      "TRAIN Batch 0960/960, Loss   86.1347, NLL-Loss   86.1319, KL-Loss   23.3836, KL-Weight  0.000\n",
      "TRAIN Epoch 00/20, Mean ELBO  106.8650\n",
      "Model saved at bin/2019-Dec-05-10:01:18/E0.pytorch\n",
      "VALID Batch 0000/240, Loss   91.5073, NLL-Loss   91.5043, KL-Loss   24.5436, KL-Weight  0.000\n",
      "VALID Batch 0050/240, Loss   93.0571, NLL-Loss   93.0539, KL-Loss   26.6744, KL-Weight  0.000\n",
      "VALID Batch 0100/240, Loss   88.8482, NLL-Loss   88.8445, KL-Loss   31.3484, KL-Weight  0.000\n",
      "VALID Batch 0150/240, Loss   97.6928, NLL-Loss   97.6883, KL-Loss   38.5395, KL-Weight  0.000\n",
      "VALID Batch 0200/240, Loss   87.7663, NLL-Loss   87.7621, KL-Loss   35.6190, KL-Weight  0.000\n",
      "VALID Batch 0240/240, Loss   56.8212, NLL-Loss   56.8146, KL-Loss   55.1926, KL-Weight  0.000\n",
      "VALID Epoch 00/20, Mean ELBO   92.7989\n",
      "TEST Batch 0000/300, Loss   91.2310, NLL-Loss   91.2281, KL-Loss   25.0937, KL-Weight  0.000\n",
      "TEST Batch 0050/300, Loss   98.5010, NLL-Loss   98.4974, KL-Loss   30.3753, KL-Weight  0.000\n",
      "TEST Batch 0100/300, Loss   90.7853, NLL-Loss   90.7820, KL-Loss   28.1802, KL-Weight  0.000\n",
      "TEST Batch 0150/300, Loss   96.5605, NLL-Loss   96.5567, KL-Loss   31.6983, KL-Weight  0.000\n",
      "TEST Batch 0200/300, Loss   86.7520, NLL-Loss   86.7482, KL-Loss   31.9352, KL-Weight  0.000\n",
      "TEST Batch 0250/300, Loss  105.9844, NLL-Loss  105.9814, KL-Loss   24.8440, KL-Weight  0.000\n",
      "TEST Batch 0300/300, Loss   90.8216, NLL-Loss   90.8183, KL-Loss   27.7816, KL-Weight  0.000\n",
      "TEST Epoch 00/20, Mean ELBO   92.8794\n",
      "TRAIN Batch 0000/960, Loss   90.9941, NLL-Loss   90.9905, KL-Loss   30.6769, KL-Weight  0.000\n",
      "TRAIN Batch 0050/960, Loss  100.8785, NLL-Loss  100.8747, KL-Loss   30.6418, KL-Weight  0.000\n",
      "TRAIN Batch 0100/960, Loss   79.9686, NLL-Loss   79.9626, KL-Loss   45.4758, KL-Weight  0.000\n",
      "TRAIN Batch 0150/960, Loss   96.0684, NLL-Loss   96.0633, KL-Loss   36.6570, KL-Weight  0.000\n",
      "TRAIN Batch 0200/960, Loss   88.7778, NLL-Loss   88.7728, KL-Loss   34.6059, KL-Weight  0.000\n",
      "TRAIN Batch 0250/960, Loss   94.6624, NLL-Loss   94.6582, KL-Loss   27.7094, KL-Weight  0.000\n",
      "TRAIN Batch 0300/960, Loss   91.1913, NLL-Loss   91.1850, KL-Loss   39.4882, KL-Weight  0.000\n",
      "TRAIN Batch 0350/960, Loss   93.7204, NLL-Loss   93.7147, KL-Loss   33.7513, KL-Weight  0.000\n",
      "TRAIN Batch 0400/960, Loss   87.1196, NLL-Loss   87.1132, KL-Loss   36.0530, KL-Weight  0.000\n",
      "TRAIN Batch 0450/960, Loss   89.3837, NLL-Loss   89.3758, KL-Loss   42.7207, KL-Weight  0.000\n",
      "TRAIN Batch 0500/960, Loss   93.7162, NLL-Loss   93.7089, KL-Loss   37.3280, KL-Weight  0.000\n",
      "TRAIN Batch 0550/960, Loss   88.7517, NLL-Loss   88.7436, KL-Loss   39.6549, KL-Weight  0.000\n",
      "TRAIN Batch 0600/960, Loss   86.9821, NLL-Loss   86.9734, KL-Loss   39.8877, KL-Weight  0.000\n",
      "TRAIN Batch 0650/960, Loss   92.5808, NLL-Loss   92.5709, KL-Loss   43.3037, KL-Weight  0.000\n",
      "TRAIN Batch 0700/960, Loss   92.2894, NLL-Loss   92.2783, KL-Loss   46.3375, KL-Weight  0.000\n",
      "TRAIN Batch 0750/960, Loss   97.6517, NLL-Loss   97.6407, KL-Loss   43.8865, KL-Weight  0.000\n",
      "TRAIN Batch 0800/960, Loss   90.0240, NLL-Loss   90.0147, KL-Loss   35.3095, KL-Weight  0.000\n",
      "TRAIN Batch 0850/960, Loss   88.6210, NLL-Loss   88.6123, KL-Loss   31.4261, KL-Weight  0.000\n",
      "TRAIN Batch 0900/960, Loss   95.6135, NLL-Loss   95.6020, KL-Loss   39.5511, KL-Weight  0.000\n",
      "TRAIN Batch 0950/960, Loss   98.1182, NLL-Loss   98.1031, KL-Loss   49.1414, KL-Weight  0.000\n",
      "TRAIN Batch 0960/960, Loss   60.2638, NLL-Loss   60.2485, KL-Loss   49.3282, KL-Weight  0.000\n",
      "TRAIN Epoch 01/20, Mean ELBO   91.4233\n",
      "Model saved at bin/2019-Dec-05-10:01:18/E1.pytorch\n",
      "VALID Batch 0000/240, Loss   85.4228, NLL-Loss   85.4105, KL-Loss   39.7239, KL-Weight  0.000\n",
      "VALID Batch 0050/240, Loss   86.9874, NLL-Loss   86.9757, KL-Loss   37.7166, KL-Weight  0.000\n",
      "VALID Batch 0100/240, Loss   82.9734, NLL-Loss   82.9627, KL-Loss   34.7770, KL-Weight  0.000\n",
      "VALID Batch 0150/240, Loss   90.9308, NLL-Loss   90.9160, KL-Loss   47.7547, KL-Weight  0.000\n",
      "VALID Batch 0200/240, Loss   82.3132, NLL-Loss   82.3002, KL-Loss   42.0709, KL-Weight  0.000\n",
      "VALID Batch 0240/240, Loss   54.0346, NLL-Loss   54.0172, KL-Loss   56.3504, KL-Weight  0.000\n",
      "VALID Epoch 01/20, Mean ELBO   86.9257\n",
      "TEST Batch 0000/300, Loss   85.6939, NLL-Loss   85.6828, KL-Loss   35.7220, KL-Weight  0.000\n",
      "TEST Batch 0050/300, Loss   93.1021, NLL-Loss   93.0884, KL-Loss   44.2885, KL-Weight  0.000\n",
      "TEST Batch 0100/300, Loss   85.0685, NLL-Loss   85.0549, KL-Loss   43.6751, KL-Weight  0.000\n",
      "TEST Batch 0150/300, Loss   89.1874, NLL-Loss   89.1749, KL-Loss   40.4222, KL-Weight  0.000\n",
      "TEST Batch 0200/300, Loss   80.3385, NLL-Loss   80.3258, KL-Loss   40.9267, KL-Weight  0.000\n",
      "TEST Batch 0250/300, Loss   99.8536, NLL-Loss   99.8425, KL-Loss   36.0567, KL-Weight  0.000\n",
      "TEST Batch 0300/300, Loss   84.2266, NLL-Loss   84.2166, KL-Loss   32.1206, KL-Weight  0.000\n",
      "TEST Epoch 01/20, Mean ELBO   87.0320\n",
      "TRAIN Batch 0000/960, Loss   82.8757, NLL-Loss   82.8627, KL-Loss   42.0348, KL-Weight  0.000\n",
      "TRAIN Batch 0050/960, Loss   81.0564, NLL-Loss   81.0411, KL-Loss   47.0248, KL-Weight  0.000\n",
      "TRAIN Batch 0100/960, Loss   79.7794, NLL-Loss   79.7642, KL-Loss   44.5214, KL-Weight  0.000\n",
      "TRAIN Batch 0150/960, Loss   93.2991, NLL-Loss   93.2815, KL-Loss   49.0008, KL-Weight  0.000\n",
      "TRAIN Batch 0200/960, Loss   84.1107, NLL-Loss   84.0933, KL-Loss   46.0369, KL-Weight  0.000\n",
      "TRAIN Batch 0250/960, Loss   80.4107, NLL-Loss   80.3920, KL-Loss   46.9094, KL-Weight  0.000\n",
      "TRAIN Batch 0300/960, Loss   78.0131, NLL-Loss   77.9942, KL-Loss   45.0239, KL-Weight  0.000\n",
      "TRAIN Batch 0350/960, Loss   94.7924, NLL-Loss   94.7701, KL-Loss   50.7639, KL-Weight  0.000\n",
      "TRAIN Batch 0400/960, Loss   90.2853, NLL-Loss   90.2654, KL-Loss   43.0462, KL-Weight  0.000\n",
      "TRAIN Batch 0450/960, Loss   84.9023, NLL-Loss   84.8803, KL-Loss   45.2781, KL-Weight  0.000\n",
      "TRAIN Batch 0500/960, Loss   67.4986, NLL-Loss   67.4788, KL-Loss   38.6514, KL-Weight  0.001\n",
      "TRAIN Batch 0550/960, Loss   96.4958, NLL-Loss   96.4714, KL-Loss   45.5111, KL-Weight  0.001\n",
      "TRAIN Batch 0600/960, Loss   95.1884, NLL-Loss   95.1600, KL-Loss   50.3584, KL-Weight  0.001\n",
      "TRAIN Batch 0650/960, Loss   76.6261, NLL-Loss   76.5985, KL-Loss   46.4387, KL-Weight  0.001\n",
      "TRAIN Batch 0700/960, Loss   83.5228, NLL-Loss   83.4961, KL-Loss   42.8751, KL-Weight  0.001\n",
      "TRAIN Batch 0750/960, Loss   74.8818, NLL-Loss   74.8543, KL-Loss   41.8831, KL-Weight  0.001\n",
      "TRAIN Batch 0800/960, Loss   77.9122, NLL-Loss   77.8840, KL-Loss   40.8584, KL-Weight  0.001\n",
      "TRAIN Batch 0850/960, Loss   82.0177, NLL-Loss   81.9875, KL-Loss   41.5945, KL-Weight  0.001\n",
      "TRAIN Batch 0900/960, Loss   91.3837, NLL-Loss   91.3542, KL-Loss   38.7427, KL-Weight  0.001\n",
      "TRAIN Batch 0950/960, Loss  103.5415, NLL-Loss  103.5032, KL-Loss   47.7546, KL-Weight  0.001\n",
      "TRAIN Batch 0960/960, Loss   98.6421, NLL-Loss   98.6262, KL-Loss   19.5960, KL-Weight  0.001\n",
      "TRAIN Epoch 02/20, Mean ELBO   85.5408\n",
      "Model saved at bin/2019-Dec-05-10:01:18/E2.pytorch\n",
      "VALID Batch 0000/240, Loss   82.6643, NLL-Loss   82.6331, KL-Loss   38.6007, KL-Weight  0.001\n",
      "VALID Batch 0050/240, Loss   84.3494, NLL-Loss   84.3173, KL-Loss   39.5911, KL-Weight  0.001\n",
      "VALID Batch 0100/240, Loss   80.1668, NLL-Loss   80.1343, KL-Loss   40.0453, KL-Weight  0.001\n",
      "VALID Batch 0150/240, Loss   88.1998, NLL-Loss   88.1591, KL-Loss   50.1736, KL-Weight  0.001\n",
      "VALID Batch 0200/240, Loss   80.6131, NLL-Loss   80.5745, KL-Loss   47.6179, KL-Weight  0.001\n",
      "VALID Batch 0240/240, Loss   49.7422, NLL-Loss   49.6932, KL-Loss   60.5341, KL-Weight  0.001\n",
      "VALID Epoch 02/20, Mean ELBO   84.2850\n",
      "TEST Batch 0000/300, Loss   82.5375, NLL-Loss   82.5066, KL-Loss   38.1373, KL-Weight  0.001\n",
      "TEST Batch 0050/300, Loss   89.7690, NLL-Loss   89.7333, KL-Loss   44.1108, KL-Weight  0.001\n",
      "TEST Batch 0100/300, Loss   82.3556, NLL-Loss   82.3204, KL-Loss   43.4647, KL-Weight  0.001\n",
      "TEST Batch 0150/300, Loss   86.1552, NLL-Loss   86.1210, KL-Loss   42.2726, KL-Weight  0.001\n",
      "TEST Batch 0200/300, Loss   77.6461, NLL-Loss   77.6095, KL-Loss   45.2120, KL-Weight  0.001\n",
      "TEST Batch 0250/300, Loss   96.9819, NLL-Loss   96.9511, KL-Loss   37.9383, KL-Weight  0.001\n",
      "TEST Batch 0300/300, Loss   82.2611, NLL-Loss   82.2322, KL-Loss   35.6158, KL-Weight  0.001\n",
      "TEST Epoch 02/20, Mean ELBO   84.4378\n",
      "TRAIN Batch 0000/960, Loss   93.4043, NLL-Loss   93.3640, KL-Loss   49.7025, KL-Weight  0.001\n",
      "TRAIN Batch 0050/960, Loss   83.7483, NLL-Loss   83.7061, KL-Loss   49.4927, KL-Weight  0.001\n",
      "TRAIN Batch 0100/960, Loss   77.9891, NLL-Loss   77.9478, KL-Loss   46.1295, KL-Weight  0.001\n",
      "TRAIN Batch 0150/960, Loss   85.9452, NLL-Loss   85.9020, KL-Loss   45.7895, KL-Weight  0.001\n",
      "TRAIN Batch 0200/960, Loss   80.3197, NLL-Loss   80.2738, KL-Loss   46.4241, KL-Weight  0.001\n",
      "TRAIN Batch 0250/960, Loss   82.5829, NLL-Loss   82.5388, KL-Loss   42.3500, KL-Weight  0.001\n",
      "TRAIN Batch 0300/960, Loss   73.7207, NLL-Loss   73.6674, KL-Loss   48.7030, KL-Weight  0.001\n",
      "TRAIN Batch 0350/960, Loss   86.4678, NLL-Loss   86.4187, KL-Loss   42.6865, KL-Weight  0.001\n",
      "TRAIN Batch 0400/960, Loss   85.2382, NLL-Loss   85.1844, KL-Loss   44.5265, KL-Weight  0.001\n",
      "TRAIN Batch 0450/960, Loss   83.0452, NLL-Loss   82.9921, KL-Loss   41.7497, KL-Weight  0.001\n",
      "TRAIN Batch 0500/960, Loss   86.4220, NLL-Loss   86.3593, KL-Loss   46.9218, KL-Weight  0.001\n",
      "TRAIN Batch 0550/960, Loss   80.1879, NLL-Loss   80.1304, KL-Loss   40.9474, KL-Weight  0.001\n",
      "TRAIN Batch 0600/960, Loss   82.0050, NLL-Loss   81.9476, KL-Loss   38.8741, KL-Weight  0.001\n",
      "TRAIN Batch 0650/960, Loss   93.9404, NLL-Loss   93.8712, KL-Loss   44.6439, KL-Weight  0.002\n",
      "TRAIN Batch 0700/960, Loss   85.1312, NLL-Loss   85.0663, KL-Loss   39.7595, KL-Weight  0.002\n",
      "TRAIN Batch 0750/960, Loss   79.5858, NLL-Loss   79.5184, KL-Loss   39.3308, KL-Weight  0.002\n",
      "TRAIN Batch 0800/960, Loss   88.6334, NLL-Loss   88.5588, KL-Loss   41.3631, KL-Weight  0.002\n",
      "TRAIN Batch 0850/960, Loss   80.3575, NLL-Loss   80.2880, KL-Loss   36.7295, KL-Weight  0.002\n",
      "TRAIN Batch 0900/960, Loss   80.3851, NLL-Loss   80.3098, KL-Loss   37.7981, KL-Weight  0.002\n",
      "TRAIN Batch 0950/960, Loss   75.8373, NLL-Loss   75.7526, KL-Loss   40.4602, KL-Weight  0.002\n",
      "TRAIN Batch 0960/960, Loss   70.7454, NLL-Loss   70.6687, KL-Loss   36.2853, KL-Weight  0.002\n",
      "TRAIN Epoch 03/20, Mean ELBO   81.5717\n",
      "Model saved at bin/2019-Dec-05-10:01:18/E3.pytorch\n",
      "VALID Batch 0000/240, Loss   80.9524, NLL-Loss   80.8783, KL-Loss   35.0181, KL-Weight  0.002\n",
      "VALID Batch 0050/240, Loss   82.7035, NLL-Loss   82.6270, KL-Loss   36.1276, KL-Weight  0.002\n",
      "VALID Batch 0100/240, Loss   79.0618, NLL-Loss   78.9825, KL-Loss   37.4814, KL-Weight  0.002\n",
      "VALID Batch 0150/240, Loss   86.8045, NLL-Loss   86.7202, KL-Loss   39.8255, KL-Weight  0.002\n",
      "VALID Batch 0200/240, Loss   78.4701, NLL-Loss   78.3835, KL-Loss   40.9181, KL-Weight  0.002\n",
      "VALID Batch 0240/240, Loss   49.4865, NLL-Loss   49.3751, KL-Loss   52.6652, KL-Weight  0.002\n",
      "VALID Epoch 03/20, Mean ELBO   82.7865\n",
      "TEST Batch 0000/300, Loss   81.2742, NLL-Loss   81.2034, KL-Loss   33.4585, KL-Weight  0.002\n",
      "TEST Batch 0050/300, Loss   88.9849, NLL-Loss   88.9034, KL-Loss   38.5125, KL-Weight  0.002\n",
      "TEST Batch 0100/300, Loss   81.0985, NLL-Loss   81.0223, KL-Loss   36.0203, KL-Weight  0.002\n",
      "TEST Batch 0150/300, Loss   84.7753, NLL-Loss   84.6940, KL-Loss   38.4138, KL-Weight  0.002\n",
      "TEST Batch 0200/300, Loss   76.0805, NLL-Loss   75.9946, KL-Loss   40.5708, KL-Weight  0.002\n",
      "TEST Batch 0250/300, Loss   95.9181, NLL-Loss   95.8417, KL-Loss   36.1117, KL-Weight  0.002\n",
      "TEST Batch 0300/300, Loss   78.7967, NLL-Loss   78.7194, KL-Loss   36.4972, KL-Weight  0.002\n",
      "TEST Epoch 03/20, Mean ELBO   82.9193\n",
      "TRAIN Batch 0000/960, Loss   85.2198, NLL-Loss   85.1423, KL-Loss   36.6119, KL-Weight  0.002\n",
      "TRAIN Batch 0050/960, Loss   72.2528, NLL-Loss   72.1712, KL-Loss   36.7059, KL-Weight  0.002\n",
      "TRAIN Batch 0100/960, Loss   63.9097, NLL-Loss   63.8238, KL-Loss   36.7361, KL-Weight  0.002\n",
      "TRAIN Batch 0150/960, Loss   65.6099, NLL-Loss   65.5126, KL-Loss   39.6171, KL-Weight  0.002\n",
      "TRAIN Batch 0200/960, Loss   78.2840, NLL-Loss   78.1861, KL-Loss   37.8586, KL-Weight  0.003\n",
      "TRAIN Batch 0250/960, Loss   72.8770, NLL-Loss   72.7709, KL-Loss   39.0670, KL-Weight  0.003\n",
      "TRAIN Batch 0300/960, Loss   83.1273, NLL-Loss   83.0102, KL-Loss   41.0087, KL-Weight  0.003\n",
      "TRAIN Batch 0350/960, Loss   74.8624, NLL-Loss   74.7534, KL-Loss   36.3501, KL-Weight  0.003\n",
      "TRAIN Batch 0400/960, Loss   81.0215, NLL-Loss   80.9136, KL-Loss   34.2120, KL-Weight  0.003\n",
      "TRAIN Batch 0450/960, Loss   81.2695, NLL-Loss   81.1489, KL-Loss   36.3674, KL-Weight  0.003\n",
      "TRAIN Batch 0500/960, Loss   76.7957, NLL-Loss   76.6825, KL-Loss   32.4980, KL-Weight  0.003\n",
      "TRAIN Batch 0550/960, Loss   88.4608, NLL-Loss   88.3353, KL-Loss   34.2606, KL-Weight  0.004\n",
      "TRAIN Batch 0600/960, Loss   82.1882, NLL-Loss   82.0642, KL-Loss   32.2078, KL-Weight  0.004\n",
      "TRAIN Batch 0650/960, Loss   85.0472, NLL-Loss   84.9160, KL-Loss   32.4319, KL-Weight  0.004\n",
      "TRAIN Batch 0700/960, Loss   77.9576, NLL-Loss   77.8139, KL-Loss   33.7934, KL-Weight  0.004\n",
      "TRAIN Batch 0750/960, Loss   82.4784, NLL-Loss   82.3361, KL-Loss   31.8406, KL-Weight  0.004\n",
      "TRAIN Batch 0800/960, Loss   77.4764, NLL-Loss   77.3211, KL-Loss   33.0516, KL-Weight  0.005\n",
      "TRAIN Batch 0850/960, Loss   83.0032, NLL-Loss   82.8537, KL-Loss   30.2697, KL-Weight  0.005\n",
      "TRAIN Batch 0900/960, Loss   79.2721, NLL-Loss   79.1155, KL-Loss   30.1818, KL-Weight  0.005\n",
      "TRAIN Batch 0950/960, Loss   78.3726, NLL-Loss   78.2249, KL-Loss   27.0781, KL-Weight  0.005\n",
      "TRAIN Batch 0960/960, Loss   73.8240, NLL-Loss   73.6668, KL-Loss   28.5407, KL-Weight  0.006\n",
      "TRAIN Epoch 04/20, Mean ELBO   78.6151\n",
      "Model saved at bin/2019-Dec-05-10:01:18/E4.pytorch\n",
      "VALID Batch 0000/240, Loss   79.5219, NLL-Loss   79.3720, KL-Loss   27.1963, KL-Weight  0.006\n",
      "VALID Batch 0050/240, Loss   81.6518, NLL-Loss   81.5067, KL-Loss   26.3163, KL-Weight  0.006\n",
      "VALID Batch 0100/240, Loss   78.0640, NLL-Loss   77.9105, KL-Loss   27.8433, KL-Weight  0.006\n",
      "VALID Batch 0150/240, Loss   85.6171, NLL-Loss   85.4502, KL-Loss   30.2690, KL-Weight  0.006\n",
      "VALID Batch 0200/240, Loss   78.2968, NLL-Loss   78.1256, KL-Loss   31.0464, KL-Weight  0.006\n",
      "VALID Batch 0240/240, Loss   49.3847, NLL-Loss   49.1508, KL-Loss   42.4148, KL-Weight  0.006\n",
      "VALID Epoch 04/20, Mean ELBO   81.8718\n",
      "TEST Batch 0000/300, Loss   81.0577, NLL-Loss   80.9244, KL-Loss   24.1668, KL-Weight  0.006\n",
      "TEST Batch 0050/300, Loss   87.3300, NLL-Loss   87.1675, KL-Loss   29.4691, KL-Weight  0.006\n",
      "TEST Batch 0100/300, Loss   79.5581, NLL-Loss   79.4108, KL-Loss   26.7007, KL-Weight  0.006\n",
      "TEST Batch 0150/300, Loss   83.3556, NLL-Loss   83.2054, KL-Loss   27.2416, KL-Weight  0.006\n",
      "TEST Batch 0200/300, Loss   75.1931, NLL-Loss   75.0275, KL-Loss   30.0418, KL-Weight  0.006\n",
      "TEST Batch 0250/300, Loss   94.7201, NLL-Loss   94.5697, KL-Loss   27.2719, KL-Weight  0.006\n",
      "TEST Batch 0300/300, Loss   77.6362, NLL-Loss   77.4972, KL-Loss   25.2124, KL-Weight  0.006\n",
      "TEST Epoch 04/20, Mean ELBO   82.0041\n",
      "TRAIN Batch 0000/960, Loss   85.1058, NLL-Loss   84.9358, KL-Loss   30.8184, KL-Weight  0.006\n",
      "TRAIN Batch 0050/960, Loss   78.5980, NLL-Loss   78.4150, KL-Loss   31.5784, KL-Weight  0.006\n",
      "TRAIN Batch 0100/960, Loss   76.8508, NLL-Loss   76.6655, KL-Loss   30.4203, KL-Weight  0.006\n",
      "TRAIN Batch 0150/960, Loss   78.1679, NLL-Loss   77.9645, KL-Loss   31.7783, KL-Weight  0.006\n",
      "TRAIN Batch 0200/960, Loss   79.7589, NLL-Loss   79.5700, KL-Loss   28.0904, KL-Weight  0.007\n",
      "TRAIN Batch 0250/960, Loss   85.3007, NLL-Loss   85.0844, KL-Loss   30.5933, KL-Weight  0.007\n",
      "TRAIN Batch 0300/960, Loss   65.4326, NLL-Loss   65.2372, KL-Loss   26.3097, KL-Weight  0.007\n",
      "TRAIN Batch 0350/960, Loss   75.1473, NLL-Loss   74.9071, KL-Loss   30.7674, KL-Weight  0.008\n",
      "TRAIN Batch 0400/960, Loss   69.5806, NLL-Loss   69.3829, KL-Loss   24.1026, KL-Weight  0.008\n",
      "TRAIN Batch 0450/960, Loss   83.7753, NLL-Loss   83.5598, KL-Loss   25.0036, KL-Weight  0.009\n",
      "TRAIN Batch 0500/960, Loss   84.5918, NLL-Loss   84.3517, KL-Loss   26.4997, KL-Weight  0.009\n",
      "TRAIN Batch 0550/960, Loss   83.4447, NLL-Loss   83.2049, KL-Loss   25.1980, KL-Weight  0.010\n",
      "TRAIN Batch 0600/960, Loss   78.3499, NLL-Loss   78.0945, KL-Loss   25.5322, KL-Weight  0.010\n",
      "TRAIN Batch 0650/960, Loss   72.3931, NLL-Loss   72.1206, KL-Loss   25.9304, KL-Weight  0.011\n",
      "TRAIN Batch 0700/960, Loss   90.4075, NLL-Loss   90.1687, KL-Loss   21.6279, KL-Weight  0.011\n",
      "TRAIN Batch 0750/960, Loss   68.2965, NLL-Loss   68.0547, KL-Loss   20.8431, KL-Weight  0.012\n",
      "TRAIN Batch 0800/960, Loss   68.1964, NLL-Loss   67.9385, KL-Loss   21.1592, KL-Weight  0.012\n",
      "TRAIN Batch 0850/960, Loss   75.7786, NLL-Loss   75.4865, KL-Loss   22.8069, KL-Weight  0.013\n",
      "TRAIN Batch 0900/960, Loss   70.9072, NLL-Loss   70.6494, KL-Loss   19.1605, KL-Weight  0.013\n",
      "TRAIN Batch 0950/960, Loss   59.7941, NLL-Loss   59.5200, KL-Loss   19.3965, KL-Weight  0.014\n",
      "TRAIN Batch 0960/960, Loss   71.4306, NLL-Loss   71.2165, KL-Loss   15.0004, KL-Weight  0.014\n",
      "TRAIN Epoch 05/20, Mean ELBO   76.3444\n",
      "Model saved at bin/2019-Dec-05-10:01:18/E5.pytorch\n",
      "VALID Batch 0000/240, Loss   78.9023, NLL-Loss   78.6336, KL-Loss   18.8080, KL-Weight  0.014\n",
      "VALID Batch 0050/240, Loss   80.9723, NLL-Loss   80.7091, KL-Loss   18.4198, KL-Weight  0.014\n",
      "VALID Batch 0100/240, Loss   77.7043, NLL-Loss   77.4315, KL-Loss   19.0941, KL-Weight  0.014\n",
      "VALID Batch 0150/240, Loss   85.3107, NLL-Loss   85.0260, KL-Loss   19.9269, KL-Weight  0.014\n",
      "VALID Batch 0200/240, Loss   77.6184, NLL-Loss   77.3166, KL-Loss   21.1269, KL-Weight  0.014\n",
      "VALID Batch 0240/240, Loss   47.8498, NLL-Loss   47.4384, KL-Loss   28.8001, KL-Weight  0.014\n",
      "VALID Epoch 05/20, Mean ELBO   81.5698\n",
      "TEST Batch 0000/300, Loss   80.1819, NLL-Loss   79.9669, KL-Loss   15.0491, KL-Weight  0.014\n",
      "TEST Batch 0050/300, Loss   87.1154, NLL-Loss   86.8332, KL-Loss   19.7539, KL-Weight  0.014\n",
      "TEST Batch 0100/300, Loss   79.4302, NLL-Loss   79.1669, KL-Loss   18.4324, KL-Weight  0.014\n",
      "TEST Batch 0150/300, Loss   82.9935, NLL-Loss   82.7190, KL-Loss   19.2134, KL-Weight  0.014\n",
      "TEST Batch 0200/300, Loss   75.2816, NLL-Loss   74.9852, KL-Loss   20.7526, KL-Weight  0.014\n",
      "TEST Batch 0250/300, Loss   94.8015, NLL-Loss   94.5622, KL-Loss   16.7514, KL-Weight  0.014\n",
      "TEST Batch 0300/300, Loss   75.4646, NLL-Loss   75.2179, KL-Loss   17.2693, KL-Weight  0.014\n",
      "TEST Epoch 05/20, Mean ELBO   81.7341\n",
      "TRAIN Batch 0000/960, Loss   74.5266, NLL-Loss   74.2493, KL-Loss   19.4103, KL-Weight  0.014\n",
      "TRAIN Batch 0050/960, Loss   75.7021, NLL-Loss   75.3795, KL-Loss   21.4910, KL-Weight  0.015\n",
      "TRAIN Batch 0100/960, Loss   71.4879, NLL-Loss   71.1417, KL-Loss   21.9570, KL-Weight  0.016\n",
      "TRAIN Batch 0150/960, Loss   72.0168, NLL-Loss   71.7026, KL-Loss   18.9749, KL-Weight  0.017\n",
      "TRAIN Batch 0200/960, Loss   73.8733, NLL-Loss   73.4923, KL-Loss   21.9004, KL-Weight  0.017\n",
      "TRAIN Batch 0250/960, Loss   72.3202, NLL-Loss   72.0107, KL-Loss   16.9400, KL-Weight  0.018\n",
      "TRAIN Batch 0300/960, Loss   75.9404, NLL-Loss   75.5946, KL-Loss   18.0185, KL-Weight  0.019\n",
      "TRAIN Batch 0350/960, Loss   79.9342, NLL-Loss   79.5900, KL-Loss   17.0769, KL-Weight  0.020\n",
      "TRAIN Batch 0400/960, Loss   94.7671, NLL-Loss   94.3953, KL-Loss   17.5662, KL-Weight  0.021\n",
      "TRAIN Batch 0450/960, Loss   83.6099, NLL-Loss   83.2537, KL-Loss   16.0242, KL-Weight  0.022\n",
      "TRAIN Batch 0500/960, Loss   71.0289, NLL-Loss   70.6945, KL-Loss   14.3281, KL-Weight  0.023\n",
      "TRAIN Batch 0550/960, Loss   72.0150, NLL-Loss   71.5995, KL-Loss   16.9544, KL-Weight  0.025\n",
      "TRAIN Batch 0600/960, Loss   67.8837, NLL-Loss   67.5109, KL-Loss   14.4864, KL-Weight  0.026\n",
      "TRAIN Batch 0650/960, Loss   81.7293, NLL-Loss   81.3186, KL-Loss   15.2031, KL-Weight  0.027\n",
      "TRAIN Batch 0700/960, Loss   79.0752, NLL-Loss   78.6877, KL-Loss   13.6653, KL-Weight  0.028\n",
      "TRAIN Batch 0750/960, Loss   88.3485, NLL-Loss   87.9244, KL-Loss   14.2435, KL-Weight  0.030\n",
      "TRAIN Batch 0800/960, Loss   82.2382, NLL-Loss   81.7811, KL-Loss   14.6280, KL-Weight  0.031\n",
      "TRAIN Batch 0850/960, Loss   77.7436, NLL-Loss   77.2199, KL-Loss   15.9680, KL-Weight  0.033\n",
      "TRAIN Batch 0900/960, Loss   85.6123, NLL-Loss   85.0993, KL-Loss   14.9049, KL-Weight  0.034\n",
      "TRAIN Batch 0950/960, Loss   73.3757, NLL-Loss   72.8969, KL-Loss   13.2549, KL-Weight  0.036\n",
      "TRAIN Batch 0960/960, Loss   65.7271, NLL-Loss   65.2839, KL-Loss   12.1519, KL-Weight  0.036\n",
      "TRAIN Epoch 06/20, Mean ELBO   74.5723\n",
      "Model saved at bin/2019-Dec-05-10:01:18/E6.pytorch\n",
      "VALID Batch 0000/240, Loss   78.8874, NLL-Loss   78.4437, KL-Loss   12.1534, KL-Weight  0.037\n",
      "VALID Batch 0050/240, Loss   80.7019, NLL-Loss   80.2823, KL-Loss   11.4928, KL-Weight  0.037\n",
      "VALID Batch 0100/240, Loss   77.5158, NLL-Loss   77.0818, KL-Loss   11.8872, KL-Weight  0.037\n",
      "VALID Batch 0150/240, Loss   83.9689, NLL-Loss   83.5097, KL-Loss   12.5761, KL-Weight  0.037\n",
      "VALID Batch 0200/240, Loss   77.2376, NLL-Loss   76.7524, KL-Loss   13.2893, KL-Weight  0.037\n",
      "VALID Batch 0240/240, Loss   46.1572, NLL-Loss   45.4008, KL-Loss   20.7182, KL-Weight  0.037\n",
      "VALID Epoch 06/20, Mean ELBO   81.2994\n",
      "TEST Batch 0000/300, Loss   80.1941, NLL-Loss   79.8210, KL-Loss   10.2198, KL-Weight  0.037\n",
      "TEST Batch 0050/300, Loss   86.6767, NLL-Loss   86.2169, KL-Loss   12.5950, KL-Weight  0.037\n",
      "TEST Batch 0100/300, Loss   78.9257, NLL-Loss   78.4994, KL-Loss   11.6763, KL-Weight  0.037\n",
      "TEST Batch 0150/300, Loss   83.6708, NLL-Loss   83.2407, KL-Loss   11.7801, KL-Weight  0.037\n",
      "TEST Batch 0200/300, Loss   74.8636, NLL-Loss   74.3845, KL-Loss   13.1204, KL-Weight  0.037\n",
      "TEST Batch 0250/300, Loss   94.2082, NLL-Loss   93.8063, KL-Loss   11.0084, KL-Weight  0.037\n",
      "TEST Batch 0300/300, Loss   74.6461, NLL-Loss   74.3104, KL-Loss    9.1945, KL-Weight  0.037\n",
      "TEST Epoch 06/20, Mean ELBO   81.5364\n",
      "TRAIN Batch 0000/960, Loss   73.2047, NLL-Loss   72.7959, KL-Loss   11.1959, KL-Weight  0.037\n",
      "TRAIN Batch 0050/960, Loss   83.1611, NLL-Loss   82.6538, KL-Loss   13.2425, KL-Weight  0.038\n",
      "TRAIN Batch 0100/960, Loss   66.4958, NLL-Loss   65.9723, KL-Loss   13.0233, KL-Weight  0.040\n",
      "TRAIN Batch 0150/960, Loss   59.4492, NLL-Loss   58.9751, KL-Loss   11.2426, KL-Weight  0.042\n",
      "TRAIN Batch 0200/960, Loss   76.7636, NLL-Loss   76.2112, KL-Loss   12.4891, KL-Weight  0.044\n",
      "TRAIN Batch 0250/960, Loss   77.4967, NLL-Loss   76.9629, KL-Loss   11.5048, KL-Weight  0.046\n",
      "TRAIN Batch 0300/960, Loss   69.6586, NLL-Loss   69.1432, KL-Loss   10.5932, KL-Weight  0.049\n",
      "TRAIN Batch 0350/960, Loss   77.0613, NLL-Loss   76.5308, KL-Loss   10.3954, KL-Weight  0.051\n",
      "TRAIN Batch 0400/960, Loss   75.0442, NLL-Loss   74.4787, KL-Loss   10.5704, KL-Weight  0.054\n",
      "TRAIN Batch 0450/960, Loss   67.6717, NLL-Loss   66.9799, KL-Loss   12.3328, KL-Weight  0.056\n",
      "TRAIN Batch 0500/960, Loss   72.0195, NLL-Loss   71.4906, KL-Loss    8.9944, KL-Weight  0.059\n",
      "TRAIN Batch 0550/960, Loss   76.3619, NLL-Loss   75.7718, KL-Loss    9.5745, KL-Weight  0.062\n",
      "TRAIN Batch 0600/960, Loss   73.4240, NLL-Loss   72.8735, KL-Loss    8.5241, KL-Weight  0.065\n",
      "TRAIN Batch 0650/960, Loss   82.6114, NLL-Loss   82.0337, KL-Loss    8.5364, KL-Weight  0.068\n",
      "TRAIN Batch 0700/960, Loss   67.3067, NLL-Loss   66.7116, KL-Loss    8.3944, KL-Weight  0.071\n",
      "TRAIN Batch 0750/960, Loss   71.8693, NLL-Loss   71.2842, KL-Loss    7.8788, KL-Weight  0.074\n",
      "TRAIN Batch 0800/960, Loss   67.6175, NLL-Loss   66.9474, KL-Loss    8.6168, KL-Weight  0.078\n",
      "TRAIN Batch 0850/960, Loss   74.2409, NLL-Loss   73.5292, KL-Loss    8.7403, KL-Weight  0.081\n",
      "TRAIN Batch 0900/960, Loss   75.5173, NLL-Loss   74.7687, KL-Loss    8.7803, KL-Weight  0.085\n",
      "TRAIN Batch 0950/960, Loss   65.6304, NLL-Loss   64.8937, KL-Loss    8.2560, KL-Weight  0.089\n",
      "TRAIN Batch 0960/960, Loss   80.3941, NLL-Loss   79.7658, KL-Loss    6.9767, KL-Weight  0.090\n",
      "TRAIN Epoch 07/20, Mean ELBO   73.2679\n",
      "Model saved at bin/2019-Dec-05-10:01:18/E7.pytorch\n",
      "VALID Batch 0000/240, Loss   79.2193, NLL-Loss   78.5187, KL-Loss    7.7737, KL-Weight  0.090\n",
      "VALID Batch 0050/240, Loss   81.0864, NLL-Loss   80.4119, KL-Loss    7.4830, KL-Weight  0.090\n",
      "VALID Batch 0100/240, Loss   77.0742, NLL-Loss   76.3668, KL-Loss    7.8481, KL-Weight  0.090\n",
      "VALID Batch 0150/240, Loss   84.8399, NLL-Loss   84.1064, KL-Loss    8.1379, KL-Weight  0.090\n",
      "VALID Batch 0200/240, Loss   77.8497, NLL-Loss   77.0131, KL-Loss    9.2817, KL-Weight  0.090\n",
      "VALID Batch 0240/240, Loss   47.0303, NLL-Loss   45.8017, KL-Loss   13.6313, KL-Weight  0.090\n",
      "VALID Epoch 07/20, Mean ELBO   81.5465\n",
      "TEST Batch 0000/300, Loss   79.8872, NLL-Loss   79.3544, KL-Loss    5.9108, KL-Weight  0.090\n",
      "TEST Batch 0050/300, Loss   87.1255, NLL-Loss   86.3710, KL-Loss    8.3713, KL-Weight  0.090\n",
      "TEST Batch 0100/300, Loss   79.3351, NLL-Loss   78.6658, KL-Loss    7.4257, KL-Weight  0.090\n",
      "TEST Batch 0150/300, Loss   82.8338, NLL-Loss   82.1283, KL-Loss    7.8270, KL-Weight  0.090\n",
      "TEST Batch 0200/300, Loss   74.8859, NLL-Loss   74.0688, KL-Loss    9.0656, KL-Weight  0.090\n",
      "TEST Batch 0250/300, Loss   94.3994, NLL-Loss   93.8090, KL-Loss    6.5503, KL-Weight  0.090\n",
      "TEST Batch 0300/300, Loss   75.3120, NLL-Loss   74.7238, KL-Loss    6.5254, KL-Weight  0.090\n",
      "TEST Epoch 07/20, Mean ELBO   81.6899\n",
      "TRAIN Batch 0000/960, Loss   66.6695, NLL-Loss   65.8074, KL-Loss    9.5648, KL-Weight  0.090\n",
      "TRAIN Batch 0050/960, Loss   70.3371, NLL-Loss   69.6187, KL-Loss    7.6160, KL-Weight  0.094\n",
      "TRAIN Batch 0100/960, Loss   65.9890, NLL-Loss   65.2728, KL-Loss    7.2570, KL-Weight  0.099\n",
      "TRAIN Batch 0150/960, Loss   66.1057, NLL-Loss   65.2833, KL-Loss    7.9678, KL-Weight  0.103\n",
      "TRAIN Batch 0200/960, Loss   65.8487, NLL-Loss   65.0258, KL-Loss    7.6239, KL-Weight  0.108\n",
      "TRAIN Batch 0250/960, Loss   76.0467, NLL-Loss   75.2273, KL-Loss    7.2610, KL-Weight  0.113\n",
      "TRAIN Batch 0300/960, Loss   71.5789, NLL-Loss   70.8434, KL-Loss    6.2354, KL-Weight  0.118\n",
      "TRAIN Batch 0350/960, Loss   73.6374, NLL-Loss   72.7432, KL-Loss    7.2553, KL-Weight  0.123\n",
      "TRAIN Batch 0400/960, Loss   65.7715, NLL-Loss   64.9616, KL-Loss    6.2900, KL-Weight  0.129\n",
      "TRAIN Batch 0450/960, Loss   85.6088, NLL-Loss   84.7863, KL-Loss    6.1171, KL-Weight  0.134\n",
      "TRAIN Batch 0500/960, Loss   73.4877, NLL-Loss   72.6330, KL-Loss    6.0881, KL-Weight  0.140\n",
      "TRAIN Batch 0550/960, Loss   74.6519, NLL-Loss   73.7534, KL-Loss    6.1316, KL-Weight  0.147\n",
      "TRAIN Batch 0600/960, Loss   75.2288, NLL-Loss   74.2277, KL-Loss    6.5470, KL-Weight  0.153\n",
      "TRAIN Batch 0650/960, Loss   65.8169, NLL-Loss   64.9677, KL-Loss    5.3247, KL-Weight  0.159\n",
      "TRAIN Batch 0700/960, Loss   66.7348, NLL-Loss   65.8436, KL-Loss    5.3587, KL-Weight  0.166\n",
      "TRAIN Batch 0750/960, Loss   78.6543, NLL-Loss   77.6912, KL-Loss    5.5552, KL-Weight  0.173\n",
      "TRAIN Batch 0800/960, Loss   87.5289, NLL-Loss   86.6330, KL-Loss    4.9595, KL-Weight  0.181\n",
      "TRAIN Batch 0850/960, Loss   68.6345, NLL-Loss   67.6225, KL-Loss    5.3783, KL-Weight  0.188\n",
      "TRAIN Batch 0900/960, Loss   76.9125, NLL-Loss   75.8852, KL-Loss    5.2435, KL-Weight  0.196\n",
      "TRAIN Batch 0950/960, Loss   75.9488, NLL-Loss   74.8374, KL-Loss    5.4503, KL-Weight  0.204\n",
      "TRAIN Batch 0960/960, Loss   88.9055, NLL-Loss   88.0268, KL-Loss    4.2753, KL-Weight  0.206\n",
      "TRAIN Epoch 08/20, Mean ELBO   72.3386\n",
      "Model saved at bin/2019-Dec-05-10:01:18/E8.pytorch\n",
      "VALID Batch 0000/240, Loss   79.4566, NLL-Loss   78.4285, KL-Loss    4.9980, KL-Weight  0.206\n",
      "VALID Batch 0050/240, Loss   80.5321, NLL-Loss   79.6210, KL-Loss    4.4294, KL-Weight  0.206\n",
      "VALID Batch 0100/240, Loss   78.3219, NLL-Loss   77.3658, KL-Loss    4.6478, KL-Weight  0.206\n",
      "VALID Batch 0150/240, Loss   85.2204, NLL-Loss   84.2385, KL-Loss    4.7735, KL-Weight  0.206\n",
      "VALID Batch 0200/240, Loss   78.4408, NLL-Loss   77.3418, KL-Loss    5.3425, KL-Weight  0.206\n",
      "VALID Batch 0240/240, Loss   48.9105, NLL-Loss   47.4036, KL-Loss    7.3258, KL-Weight  0.206\n",
      "VALID Epoch 08/20, Mean ELBO   81.7441\n",
      "TEST Batch 0000/300, Loss   80.9186, NLL-Loss   80.1123, KL-Loss    3.9201, KL-Weight  0.206\n",
      "TEST Batch 0050/300, Loss   86.3096, NLL-Loss   85.2272, KL-Loss    5.2620, KL-Weight  0.206\n",
      "TEST Batch 0100/300, Loss   79.0844, NLL-Loss   78.1498, KL-Loss    4.5435, KL-Weight  0.206\n",
      "TEST Batch 0150/300, Loss   82.7365, NLL-Loss   81.7589, KL-Loss    4.7527, KL-Weight  0.206\n",
      "TEST Batch 0200/300, Loss   75.8795, NLL-Loss   74.8263, KL-Loss    5.1199, KL-Weight  0.206\n",
      "TEST Batch 0250/300, Loss   94.6710, NLL-Loss   93.7923, KL-Loss    4.2713, KL-Weight  0.206\n",
      "TEST Batch 0300/300, Loss   75.9778, NLL-Loss   75.0774, KL-Loss    4.3772, KL-Weight  0.206\n",
      "TEST Epoch 08/20, Mean ELBO   81.9582\n",
      "TRAIN Batch 0000/960, Loss   63.8166, NLL-Loss   62.8363, KL-Loss    4.7657, KL-Weight  0.206\n",
      "TRAIN Batch 0050/960, Loss   75.1717, NLL-Loss   73.9567, KL-Loss    5.6774, KL-Weight  0.214\n",
      "TRAIN Batch 0100/960, Loss   73.3899, NLL-Loss   72.0066, KL-Loss    6.2161, KL-Weight  0.223\n",
      "TRAIN Batch 0150/960, Loss   78.8656, NLL-Loss   77.6550, KL-Loss    5.2337, KL-Weight  0.231\n",
      "TRAIN Batch 0200/960, Loss   75.3627, NLL-Loss   74.3242, KL-Loss    4.3216, KL-Weight  0.240\n",
      "TRAIN Batch 0250/960, Loss   64.8204, NLL-Loss   63.7030, KL-Loss    4.4777, KL-Weight  0.250\n",
      "TRAIN Batch 0300/960, Loss   75.6890, NLL-Loss   74.6290, KL-Loss    4.0920, KL-Weight  0.259\n",
      "TRAIN Batch 0350/960, Loss   73.7997, NLL-Loss   72.6711, KL-Loss    4.1996, KL-Weight  0.269\n",
      "TRAIN Batch 0400/960, Loss   70.7896, NLL-Loss   69.7056, KL-Loss    3.8897, KL-Weight  0.279\n",
      "TRAIN Batch 0450/960, Loss   76.2451, NLL-Loss   75.1915, KL-Loss    3.6477, KL-Weight  0.289\n",
      "TRAIN Batch 0500/960, Loss   66.8335, NLL-Loss   65.5433, KL-Loss    4.3115, KL-Weight  0.299\n",
      "TRAIN Batch 0550/960, Loss   72.2309, NLL-Loss   71.0842, KL-Loss    3.7011, KL-Weight  0.310\n",
      "TRAIN Batch 0600/960, Loss   78.8928, NLL-Loss   77.7535, KL-Loss    3.5538, KL-Weight  0.321\n",
      "TRAIN Batch 0650/960, Loss   82.5452, NLL-Loss   81.3784, KL-Loss    3.5188, KL-Weight  0.332\n",
      "TRAIN Batch 0700/960, Loss   67.9354, NLL-Loss   66.6713, KL-Loss    3.6880, KL-Weight  0.343\n",
      "TRAIN Batch 0750/960, Loss   81.0245, NLL-Loss   79.8037, KL-Loss    3.4477, KL-Weight  0.354\n",
      "TRAIN Batch 0800/960, Loss   68.2471, NLL-Loss   66.9796, KL-Loss    3.4666, KL-Weight  0.366\n",
      "TRAIN Batch 0850/960, Loss   69.7852, NLL-Loss   68.4552, KL-Loss    3.5250, KL-Weight  0.377\n",
      "TRAIN Batch 0900/960, Loss   77.0713, NLL-Loss   75.8076, KL-Loss    3.2476, KL-Weight  0.389\n",
      "TRAIN Batch 0950/960, Loss   70.4789, NLL-Loss   69.3842, KL-Loss    2.7294, KL-Weight  0.401\n",
      "TRAIN Batch 0960/960, Loss   65.4634, NLL-Loss   64.2608, KL-Loss    2.9804, KL-Weight  0.403\n",
      "TRAIN Epoch 09/20, Mean ELBO   71.6324\n",
      "Model saved at bin/2019-Dec-05-10:01:18/E9.pytorch\n",
      "VALID Batch 0000/240, Loss   79.6374, NLL-Loss   78.3400, KL-Loss    3.2137, KL-Weight  0.404\n",
      "VALID Batch 0050/240, Loss   81.1675, NLL-Loss   79.9760, KL-Loss    2.9513, KL-Weight  0.404\n",
      "VALID Batch 0100/240, Loss   78.4935, NLL-Loss   77.1505, KL-Loss    3.3267, KL-Weight  0.404\n",
      "VALID Batch 0150/240, Loss   85.0282, NLL-Loss   83.7160, KL-Loss    3.2503, KL-Weight  0.404\n",
      "VALID Batch 0200/240, Loss   78.4947, NLL-Loss   77.0425, KL-Loss    3.5971, KL-Weight  0.404\n",
      "VALID Batch 0240/240, Loss   49.2639, NLL-Loss   47.2741, KL-Loss    4.9289, KL-Weight  0.404\n",
      "VALID Epoch 09/20, Mean ELBO   82.2407\n",
      "TEST Batch 0000/300, Loss   80.9608, NLL-Loss   79.8339, KL-Loss    2.7913, KL-Weight  0.404\n",
      "TEST Batch 0050/300, Loss   87.8357, NLL-Loss   86.5446, KL-Loss    3.1981, KL-Weight  0.404\n",
      "TEST Batch 0100/300, Loss   80.5915, NLL-Loss   79.4034, KL-Loss    2.9428, KL-Weight  0.404\n",
      "TEST Batch 0150/300, Loss   84.4520, NLL-Loss   83.1594, KL-Loss    3.2018, KL-Weight  0.404\n",
      "TEST Batch 0200/300, Loss   76.3910, NLL-Loss   75.0386, KL-Loss    3.3500, KL-Weight  0.404\n",
      "TEST Batch 0250/300, Loss   94.3178, NLL-Loss   93.2099, KL-Loss    2.7443, KL-Weight  0.404\n",
      "TEST Batch 0300/300, Loss   75.9724, NLL-Loss   74.9569, KL-Loss    2.5153, KL-Weight  0.404\n",
      "TEST Epoch 09/20, Mean ELBO   82.3643\n",
      "TRAIN Batch 0000/960, Loss   80.9683, NLL-Loss   79.6632, KL-Loss    3.2328, KL-Weight  0.404\n",
      "TRAIN Batch 0050/960, Loss   76.7426, NLL-Loss   75.5187, KL-Loss    2.9433, KL-Weight  0.416\n",
      "TRAIN Batch 0100/960, Loss   74.8061, NLL-Loss   73.3689, KL-Loss    3.3579, KL-Weight  0.428\n",
      "TRAIN Batch 0150/960, Loss   69.5733, NLL-Loss   68.1653, KL-Loss    3.1979, KL-Weight  0.440\n",
      "TRAIN Batch 0200/960, Loss   69.8202, NLL-Loss   68.6356, KL-Loss    2.6172, KL-Weight  0.453\n",
      "TRAIN Batch 0250/960, Loss   72.9888, NLL-Loss   71.4799, KL-Loss    3.2446, KL-Weight  0.465\n",
      "TRAIN Batch 0300/960, Loss   63.0298, NLL-Loss   61.5739, KL-Loss    3.0489, KL-Weight  0.478\n",
      "TRAIN Batch 0350/960, Loss   77.5676, NLL-Loss   76.1917, KL-Loss    2.8080, KL-Weight  0.490\n",
      "TRAIN Batch 0400/960, Loss   74.0423, NLL-Loss   72.7490, KL-Loss    2.5737, KL-Weight  0.502\n",
      "TRAIN Batch 0450/960, Loss   64.7475, NLL-Loss   63.1924, KL-Loss    3.0197, KL-Weight  0.515\n",
      "TRAIN Batch 0500/960, Loss   77.5351, NLL-Loss   76.1241, KL-Loss    2.6752, KL-Weight  0.527\n",
      "TRAIN Batch 0550/960, Loss   73.8900, NLL-Loss   72.3846, KL-Loss    2.7882, KL-Weight  0.540\n",
      "TRAIN Batch 0600/960, Loss   83.2346, NLL-Loss   81.7477, KL-Loss    2.6920, KL-Weight  0.552\n",
      "TRAIN Batch 0650/960, Loss   74.9099, NLL-Loss   73.4607, KL-Loss    2.5666, KL-Weight  0.565\n",
      "TRAIN Batch 0700/960, Loss   73.3688, NLL-Loss   72.0025, KL-Loss    2.3683, KL-Weight  0.577\n",
      "TRAIN Batch 0750/960, Loss   63.2955, NLL-Loss   61.9789, KL-Loss    2.2350, KL-Weight  0.589\n",
      "TRAIN Batch 0800/960, Loss   74.5826, NLL-Loss   73.1955, KL-Loss    2.3076, KL-Weight  0.601\n",
      "TRAIN Batch 0850/960, Loss   74.2824, NLL-Loss   72.5689, KL-Loss    2.7951, KL-Weight  0.613\n",
      "TRAIN Batch 0900/960, Loss   68.7644, NLL-Loss   67.2659, KL-Loss    2.3983, KL-Weight  0.625\n",
      "TRAIN Batch 0950/960, Loss   74.8797, NLL-Loss   73.4847, KL-Loss    2.1917, KL-Weight  0.636\n",
      "TRAIN Batch 0960/960, Loss   84.5279, NLL-Loss   83.1628, KL-Loss    2.1370, KL-Weight  0.639\n",
      "TRAIN Epoch 10/20, Mean ELBO   71.0848\n",
      "Model saved at bin/2019-Dec-05-10:01:18/E10.pytorch\n",
      "VALID Batch 0000/240, Loss   79.8594, NLL-Loss   78.3816, KL-Loss    2.3128, KL-Weight  0.639\n",
      "VALID Batch 0050/240, Loss   82.1468, NLL-Loss   80.8595, KL-Loss    2.0145, KL-Weight  0.639\n",
      "VALID Batch 0100/240, Loss   78.1173, NLL-Loss   76.8117, KL-Loss    2.0432, KL-Weight  0.639\n",
      "VALID Batch 0150/240, Loss   87.1261, NLL-Loss   85.7941, KL-Loss    2.0845, KL-Weight  0.639\n",
      "VALID Batch 0200/240, Loss   79.5204, NLL-Loss   77.9754, KL-Loss    2.4178, KL-Weight  0.639\n",
      "VALID Batch 0240/240, Loss   47.3923, NLL-Loss   45.2943, KL-Loss    3.2833, KL-Weight  0.639\n",
      "VALID Epoch 10/20, Mean ELBO   82.5914\n",
      "TEST Batch 0000/300, Loss   81.4082, NLL-Loss   80.1503, KL-Loss    1.9686, KL-Weight  0.639\n",
      "TEST Batch 0050/300, Loss   86.8929, NLL-Loss   85.5079, KL-Loss    2.1674, KL-Weight  0.639\n",
      "TEST Batch 0100/300, Loss   80.1249, NLL-Loss   78.8393, KL-Loss    2.0119, KL-Weight  0.639\n",
      "TEST Batch 0150/300, Loss   83.8596, NLL-Loss   82.6086, KL-Loss    1.9577, KL-Weight  0.639\n",
      "TEST Batch 0200/300, Loss   76.2623, NLL-Loss   74.8295, KL-Loss    2.2423, KL-Weight  0.639\n",
      "TEST Batch 0250/300, Loss   95.8128, NLL-Loss   94.4263, KL-Loss    2.1699, KL-Weight  0.639\n",
      "TEST Batch 0300/300, Loss   76.4884, NLL-Loss   75.4769, KL-Loss    1.5830, KL-Weight  0.639\n",
      "TEST Epoch 10/20, Mean ELBO   82.7892\n",
      "TRAIN Batch 0000/960, Loss   67.8588, NLL-Loss   66.5134, KL-Loss    2.1055, KL-Weight  0.639\n",
      "TRAIN Batch 0050/960, Loss   72.5725, NLL-Loss   71.1153, KL-Loss    2.2403, KL-Weight  0.650\n",
      "TRAIN Batch 0100/960, Loss   62.5293, NLL-Loss   61.0581, KL-Loss    2.2233, KL-Weight  0.662\n",
      "TRAIN Batch 0150/960, Loss   62.6836, NLL-Loss   61.1855, KL-Loss    2.2265, KL-Weight  0.673\n",
      "TRAIN Batch 0200/960, Loss   72.9293, NLL-Loss   71.5116, KL-Loss    2.0734, KL-Weight  0.684\n",
      "TRAIN Batch 0250/960, Loss   83.0945, NLL-Loss   81.4598, KL-Loss    2.3539, KL-Weight  0.694\n",
      "TRAIN Batch 0300/960, Loss   68.2524, NLL-Loss   66.7349, KL-Loss    2.1526, KL-Weight  0.705\n",
      "TRAIN Batch 0350/960, Loss   72.9101, NLL-Loss   71.6324, KL-Loss    1.7864, KL-Weight  0.715\n",
      "TRAIN Batch 0400/960, Loss   67.1957, NLL-Loss   65.7582, KL-Loss    1.9818, KL-Weight  0.725\n",
      "TRAIN Batch 0450/960, Loss   71.5533, NLL-Loss   69.8326, KL-Loss    2.3405, KL-Weight  0.735\n",
      "TRAIN Batch 0500/960, Loss   78.8156, NLL-Loss   77.0977, KL-Loss    2.3065, KL-Weight  0.745\n",
      "TRAIN Batch 0550/960, Loss   64.6995, NLL-Loss   63.0477, KL-Loss    2.1903, KL-Weight  0.754\n",
      "TRAIN Batch 0600/960, Loss   64.3091, NLL-Loss   62.5442, KL-Loss    2.3121, KL-Weight  0.763\n",
      "TRAIN Batch 0650/960, Loss   72.0151, NLL-Loss   70.4345, KL-Loss    2.0467, KL-Weight  0.772\n",
      "TRAIN Batch 0700/960, Loss   68.3770, NLL-Loss   66.7292, KL-Loss    2.1101, KL-Weight  0.781\n",
      "TRAIN Batch 0750/960, Loss   71.1024, NLL-Loss   69.3615, KL-Loss    2.2055, KL-Weight  0.789\n",
      "TRAIN Batch 0800/960, Loss   68.9799, NLL-Loss   67.4767, KL-Loss    1.8847, KL-Weight  0.798\n",
      "TRAIN Batch 0850/960, Loss   69.9405, NLL-Loss   68.4242, KL-Loss    1.8825, KL-Weight  0.805\n",
      "TRAIN Batch 0900/960, Loss   69.8671, NLL-Loss   68.3200, KL-Loss    1.9024, KL-Weight  0.813\n",
      "TRAIN Batch 0950/960, Loss   76.8031, NLL-Loss   75.4428, KL-Loss    1.6574, KL-Weight  0.821\n",
      "TRAIN Batch 0960/960, Loss   58.9292, NLL-Loss   57.3412, KL-Loss    1.9315, KL-Weight  0.822\n",
      "TRAIN Epoch 11/20, Mean ELBO   70.4661\n",
      "Model saved at bin/2019-Dec-05-10:01:18/E11.pytorch\n",
      "VALID Batch 0000/240, Loss   81.1434, NLL-Loss   79.6896, KL-Loss    1.7680, KL-Weight  0.822\n",
      "VALID Batch 0050/240, Loss   81.8998, NLL-Loss   80.5376, KL-Loss    1.6565, KL-Weight  0.822\n",
      "VALID Batch 0100/240, Loss   78.5418, NLL-Loss   77.0856, KL-Loss    1.7708, KL-Weight  0.822\n",
      "VALID Batch 0150/240, Loss   86.7583, NLL-Loss   85.3216, KL-Loss    1.7473, KL-Weight  0.822\n",
      "VALID Batch 0200/240, Loss   78.8709, NLL-Loss   77.3335, KL-Loss    1.8696, KL-Weight  0.822\n",
      "VALID Batch 0240/240, Loss   48.9699, NLL-Loss   46.9223, KL-Loss    2.4902, KL-Weight  0.822\n",
      "VALID Epoch 11/20, Mean ELBO   82.7437\n",
      "TEST Batch 0000/300, Loss   81.7707, NLL-Loss   80.5127, KL-Loss    1.5298, KL-Weight  0.822\n",
      "TEST Batch 0050/300, Loss   87.4504, NLL-Loss   86.0465, KL-Loss    1.7073, KL-Weight  0.822\n",
      "TEST Batch 0100/300, Loss   80.7903, NLL-Loss   79.5388, KL-Loss    1.5220, KL-Weight  0.822\n",
      "TEST Batch 0150/300, Loss   85.2223, NLL-Loss   83.8527, KL-Loss    1.6656, KL-Weight  0.822\n",
      "TEST Batch 0200/300, Loss   76.4674, NLL-Loss   75.0272, KL-Loss    1.7514, KL-Weight  0.822\n",
      "TEST Batch 0250/300, Loss   94.3234, NLL-Loss   93.0198, KL-Loss    1.5853, KL-Weight  0.822\n",
      "TEST Batch 0300/300, Loss   76.1696, NLL-Loss   74.9759, KL-Loss    1.4517, KL-Weight  0.822\n",
      "TEST Epoch 11/20, Mean ELBO   82.9165\n",
      "TRAIN Batch 0000/960, Loss   59.3538, NLL-Loss   58.0065, KL-Loss    1.6385, KL-Weight  0.822\n",
      "TRAIN Batch 0050/960, Loss   68.5860, NLL-Loss   66.9390, KL-Loss    1.9855, KL-Weight  0.829\n",
      "TRAIN Batch 0100/960, Loss   73.1332, NLL-Loss   71.6681, KL-Loss    1.7516, KL-Weight  0.836\n",
      "TRAIN Batch 0150/960, Loss   65.4126, NLL-Loss   63.8768, KL-Loss    1.8215, KL-Weight  0.843\n",
      "TRAIN Batch 0200/960, Loss   63.6197, NLL-Loss   62.1339, KL-Loss    1.7487, KL-Weight  0.850\n",
      "TRAIN Batch 0250/960, Loss   66.5438, NLL-Loss   65.0080, KL-Loss    1.7943, KL-Weight  0.856\n",
      "TRAIN Batch 0300/960, Loss   68.5012, NLL-Loss   67.0095, KL-Loss    1.7305, KL-Weight  0.862\n",
      "TRAIN Batch 0350/960, Loss   67.3309, NLL-Loss   65.9083, KL-Loss    1.6393, KL-Weight  0.868\n",
      "TRAIN Batch 0400/960, Loss   74.1279, NLL-Loss   72.5583, KL-Loss    1.7969, KL-Weight  0.873\n",
      "TRAIN Batch 0450/960, Loss   67.9237, NLL-Loss   66.1328, KL-Loss    2.0377, KL-Weight  0.879\n",
      "TRAIN Batch 0500/960, Loss   75.5080, NLL-Loss   74.1230, KL-Loss    1.5666, KL-Weight  0.884\n",
      "TRAIN Batch 0550/960, Loss   67.7092, NLL-Loss   66.0872, KL-Loss    1.8242, KL-Weight  0.889\n",
      "TRAIN Batch 0600/960, Loss   69.1409, NLL-Loss   67.5715, KL-Loss    1.7555, KL-Weight  0.894\n",
      "TRAIN Batch 0650/960, Loss   70.8067, NLL-Loss   69.2163, KL-Loss    1.7698, KL-Weight  0.899\n",
      "TRAIN Batch 0700/960, Loss   72.7196, NLL-Loss   71.2541, KL-Loss    1.6227, KL-Weight  0.903\n",
      "TRAIN Batch 0750/960, Loss   74.2844, NLL-Loss   72.6747, KL-Loss    1.7741, KL-Weight  0.907\n",
      "TRAIN Batch 0800/960, Loss   66.9786, NLL-Loss   65.2938, KL-Loss    1.8485, KL-Weight  0.911\n",
      "TRAIN Batch 0850/960, Loss   75.4963, NLL-Loss   74.0943, KL-Loss    1.5315, KL-Weight  0.915\n",
      "TRAIN Batch 0900/960, Loss   72.4978, NLL-Loss   70.8143, KL-Loss    1.8314, KL-Weight  0.919\n",
      "TRAIN Batch 0950/960, Loss   72.7079, NLL-Loss   71.1212, KL-Loss    1.7194, KL-Weight  0.923\n",
      "TRAIN Batch 0960/960, Loss   55.8233, NLL-Loss   54.2256, KL-Loss    1.7299, KL-Weight  0.924\n",
      "TRAIN Epoch 12/20, Mean ELBO   69.7102\n",
      "Model saved at bin/2019-Dec-05-10:01:18/E12.pytorch\n",
      "VALID Batch 0000/240, Loss   80.1690, NLL-Loss   78.7318, KL-Loss    1.5560, KL-Weight  0.924\n",
      "VALID Batch 0050/240, Loss   82.7718, NLL-Loss   81.3686, KL-Loss    1.5192, KL-Weight  0.924\n",
      "VALID Batch 0100/240, Loss   79.5470, NLL-Loss   78.1401, KL-Loss    1.5232, KL-Weight  0.924\n",
      "VALID Batch 0150/240, Loss   86.0193, NLL-Loss   84.4450, KL-Loss    1.7044, KL-Weight  0.924\n",
      "VALID Batch 0200/240, Loss   79.2523, NLL-Loss   77.5821, KL-Loss    1.8083, KL-Weight  0.924\n",
      "VALID Batch 0240/240, Loss   50.4626, NLL-Loss   48.0676, KL-Loss    2.5929, KL-Weight  0.924\n",
      "VALID Epoch 12/20, Mean ELBO   82.9726\n",
      "TEST Batch 0000/300, Loss   81.7605, NLL-Loss   80.4146, KL-Loss    1.4570, KL-Weight  0.924\n",
      "TEST Batch 0050/300, Loss   86.7978, NLL-Loss   85.1595, KL-Loss    1.7737, KL-Weight  0.924\n",
      "TEST Batch 0100/300, Loss   81.0974, NLL-Loss   79.6230, KL-Loss    1.5963, KL-Weight  0.924\n",
      "TEST Batch 0150/300, Loss   83.9408, NLL-Loss   82.6030, KL-Loss    1.4485, KL-Weight  0.924\n",
      "TEST Batch 0200/300, Loss   76.4080, NLL-Loss   74.7666, KL-Loss    1.7771, KL-Weight  0.924\n",
      "TEST Batch 0250/300, Loss   95.9451, NLL-Loss   94.5733, KL-Loss    1.4852, KL-Weight  0.924\n",
      "TEST Batch 0300/300, Loss   75.9008, NLL-Loss   74.8114, KL-Loss    1.1795, KL-Weight  0.924\n",
      "TEST Epoch 12/20, Mean ELBO   83.0920\n",
      "TRAIN Batch 0000/960, Loss   62.2677, NLL-Loss   60.6814, KL-Loss    1.7175, KL-Weight  0.924\n",
      "TRAIN Batch 0050/960, Loss   63.6811, NLL-Loss   62.1544, KL-Loss    1.6468, KL-Weight  0.927\n",
      "TRAIN Batch 0100/960, Loss   67.6472, NLL-Loss   66.2532, KL-Loss    1.4983, KL-Weight  0.930\n",
      "TRAIN Batch 0150/960, Loss   68.7615, NLL-Loss   67.3894, KL-Loss    1.4698, KL-Weight  0.934\n",
      "TRAIN Batch 0200/960, Loss   74.0414, NLL-Loss   72.7599, KL-Loss    1.3682, KL-Weight  0.937\n",
      "TRAIN Batch 0250/960, Loss   68.0540, NLL-Loss   66.6704, KL-Loss    1.4726, KL-Weight  0.940\n",
      "TRAIN Batch 0300/960, Loss   76.7021, NLL-Loss   75.2598, KL-Loss    1.5306, KL-Weight  0.942\n",
      "TRAIN Batch 0350/960, Loss   74.4391, NLL-Loss   73.0829, KL-Loss    1.4352, KL-Weight  0.945\n",
      "TRAIN Batch 0400/960, Loss   69.7151, NLL-Loss   68.1915, KL-Loss    1.6080, KL-Weight  0.947\n",
      "TRAIN Batch 0450/960, Loss   74.0891, NLL-Loss   72.6013, KL-Loss    1.5663, KL-Weight  0.950\n",
      "TRAIN Batch 0500/960, Loss   80.0002, NLL-Loss   78.3487, KL-Loss    1.7343, KL-Weight  0.952\n",
      "TRAIN Batch 0550/960, Loss   64.7930, NLL-Loss   63.4228, KL-Loss    1.4355, KL-Weight  0.954\n",
      "TRAIN Batch 0600/960, Loss   63.6711, NLL-Loss   62.1846, KL-Loss    1.5539, KL-Weight  0.957\n",
      "TRAIN Batch 0650/960, Loss   69.6406, NLL-Loss   67.9652, KL-Loss    1.7477, KL-Weight  0.959\n",
      "TRAIN Batch 0700/960, Loss   77.1711, NLL-Loss   75.7402, KL-Loss    1.4896, KL-Weight  0.961\n",
      "TRAIN Batch 0750/960, Loss   73.2444, NLL-Loss   71.6463, KL-Loss    1.6605, KL-Weight  0.962\n",
      "TRAIN Batch 0800/960, Loss   62.2310, NLL-Loss   61.0060, KL-Loss    1.2705, KL-Weight  0.964\n",
      "TRAIN Batch 0850/960, Loss   70.0978, NLL-Loss   68.7214, KL-Loss    1.4250, KL-Weight  0.966\n",
      "TRAIN Batch 0900/960, Loss   77.1253, NLL-Loss   75.6623, KL-Loss    1.5122, KL-Weight  0.967\n",
      "TRAIN Batch 0950/960, Loss   64.4896, NLL-Loss   63.1090, KL-Loss    1.4247, KL-Weight  0.969\n",
      "TRAIN Batch 0960/960, Loss   82.9287, NLL-Loss   81.7274, KL-Loss    1.2393, KL-Weight  0.969\n",
      "TRAIN Epoch 13/20, Mean ELBO   68.9685\n",
      "Model saved at bin/2019-Dec-05-10:01:18/E13.pytorch\n",
      "VALID Batch 0000/240, Loss   80.1180, NLL-Loss   78.6020, KL-Loss    1.5639, KL-Weight  0.969\n",
      "VALID Batch 0050/240, Loss   81.3918, NLL-Loss   80.0075, KL-Loss    1.4281, KL-Weight  0.969\n",
      "VALID Batch 0100/240, Loss   78.5439, NLL-Loss   77.0768, KL-Loss    1.5135, KL-Weight  0.969\n",
      "VALID Batch 0150/240, Loss   86.0686, NLL-Loss   84.4893, KL-Loss    1.6293, KL-Weight  0.969\n",
      "VALID Batch 0200/240, Loss   78.9314, NLL-Loss   77.2816, KL-Loss    1.7019, KL-Weight  0.969\n",
      "VALID Batch 0240/240, Loss   49.7132, NLL-Loss   47.5722, KL-Loss    2.2088, KL-Weight  0.969\n",
      "VALID Epoch 13/20, Mean ELBO   83.0243\n",
      "TEST Batch 0000/300, Loss   81.4231, NLL-Loss   80.2373, KL-Loss    1.2234, KL-Weight  0.969\n",
      "TEST Batch 0050/300, Loss   87.3021, NLL-Loss   85.7501, KL-Loss    1.6011, KL-Weight  0.969\n",
      "TEST Batch 0100/300, Loss   81.5826, NLL-Loss   80.1016, KL-Loss    1.5278, KL-Weight  0.969\n",
      "TEST Batch 0150/300, Loss   85.1193, NLL-Loss   83.8158, KL-Loss    1.3447, KL-Weight  0.969\n",
      "TEST Batch 0200/300, Loss   76.8932, NLL-Loss   75.3395, KL-Loss    1.6028, KL-Weight  0.969\n",
      "TEST Batch 0250/300, Loss   95.4907, NLL-Loss   94.1521, KL-Loss    1.3809, KL-Weight  0.969\n",
      "TEST Batch 0300/300, Loss   75.9060, NLL-Loss   74.9354, KL-Loss    1.0013, KL-Weight  0.969\n",
      "TEST Epoch 13/20, Mean ELBO   83.1471\n",
      "TRAIN Batch 0000/960, Loss   62.9901, NLL-Loss   61.5328, KL-Loss    1.5034, KL-Weight  0.969\n",
      "TRAIN Batch 0050/960, Loss   68.7201, NLL-Loss   66.9146, KL-Loss    1.8598, KL-Weight  0.971\n",
      "TRAIN Batch 0100/960, Loss   73.5558, NLL-Loss   72.0652, KL-Loss    1.5332, KL-Weight  0.972\n",
      "TRAIN Batch 0150/960, Loss   74.4394, NLL-Loss   72.5321, KL-Loss    1.9592, KL-Weight  0.974\n",
      "TRAIN Batch 0200/960, Loss   66.6122, NLL-Loss   65.0075, KL-Loss    1.6463, KL-Weight  0.975\n",
      "TRAIN Batch 0250/960, Loss   66.0177, NLL-Loss   64.7184, KL-Loss    1.3314, KL-Weight  0.976\n",
      "TRAIN Batch 0300/960, Loss   76.7444, NLL-Loss   75.3408, KL-Loss    1.4365, KL-Weight  0.977\n",
      "TRAIN Batch 0350/960, Loss   70.4429, NLL-Loss   69.1313, KL-Loss    1.3408, KL-Weight  0.978\n",
      "TRAIN Batch 0400/960, Loss   67.4418, NLL-Loss   65.9119, KL-Loss    1.5623, KL-Weight  0.979\n",
      "TRAIN Batch 0450/960, Loss   70.2218, NLL-Loss   68.5343, KL-Loss    1.7215, KL-Weight  0.980\n",
      "TRAIN Batch 0500/960, Loss   74.0784, NLL-Loss   72.6924, KL-Loss    1.4125, KL-Weight  0.981\n",
      "TRAIN Batch 0550/960, Loss   70.1693, NLL-Loss   68.7904, KL-Loss    1.4041, KL-Weight  0.982\n",
      "TRAIN Batch 0600/960, Loss   69.2887, NLL-Loss   67.7722, KL-Loss    1.5428, KL-Weight  0.983\n",
      "TRAIN Batch 0650/960, Loss   67.0928, NLL-Loss   65.6457, KL-Loss    1.4710, KL-Weight  0.984\n",
      "TRAIN Batch 0700/960, Loss   61.0302, NLL-Loss   59.7437, KL-Loss    1.3067, KL-Weight  0.985\n",
      "TRAIN Batch 0750/960, Loss   73.5704, NLL-Loss   72.0507, KL-Loss    1.5425, KL-Weight  0.985\n",
      "TRAIN Batch 0800/960, Loss   77.0419, NLL-Loss   75.6376, KL-Loss    1.4242, KL-Weight  0.986\n",
      "TRAIN Batch 0850/960, Loss   67.1715, NLL-Loss   65.9118, KL-Loss    1.2768, KL-Weight  0.987\n",
      "TRAIN Batch 0900/960, Loss   64.3202, NLL-Loss   63.0012, KL-Loss    1.3360, KL-Weight  0.987\n",
      "TRAIN Batch 0950/960, Loss   68.5068, NLL-Loss   66.7612, KL-Loss    1.7670, KL-Weight  0.988\n",
      "TRAIN Batch 0960/960, Loss   53.3500, NLL-Loss   52.1334, KL-Loss    1.2313, KL-Weight  0.988\n",
      "TRAIN Epoch 14/20, Mean ELBO   68.1706\n",
      "Model saved at bin/2019-Dec-05-10:01:18/E14.pytorch\n",
      "VALID Batch 0000/240, Loss   80.8071, NLL-Loss   79.4339, KL-Loss    1.3898, KL-Weight  0.988\n",
      "VALID Batch 0050/240, Loss   82.0134, NLL-Loss   80.5987, KL-Loss    1.4318, KL-Weight  0.988\n",
      "VALID Batch 0100/240, Loss   78.6294, NLL-Loss   77.2098, KL-Loss    1.4367, KL-Weight  0.988\n",
      "VALID Batch 0150/240, Loss   85.5289, NLL-Loss   83.9286, KL-Loss    1.6197, KL-Weight  0.988\n",
      "VALID Batch 0200/240, Loss   80.0857, NLL-Loss   78.4010, KL-Loss    1.7050, KL-Weight  0.988\n",
      "VALID Batch 0240/240, Loss   50.8867, NLL-Loss   48.4328, KL-Loss    2.4836, KL-Weight  0.988\n",
      "VALID Epoch 14/20, Mean ELBO   83.1571\n",
      "TEST Batch 0000/300, Loss   82.7052, NLL-Loss   81.5854, KL-Loss    1.1334, KL-Weight  0.988\n",
      "TEST Batch 0050/300, Loss   87.3411, NLL-Loss   85.7154, KL-Loss    1.6454, KL-Weight  0.988\n",
      "TEST Batch 0100/300, Loss   80.5038, NLL-Loss   79.0513, KL-Loss    1.4700, KL-Weight  0.988\n",
      "TEST Batch 0150/300, Loss   84.7153, NLL-Loss   83.3972, KL-Loss    1.3341, KL-Weight  0.988\n",
      "TEST Batch 0200/300, Loss   76.8667, NLL-Loss   75.1873, KL-Loss    1.6997, KL-Weight  0.988\n",
      "TEST Batch 0250/300, Loss   95.5109, NLL-Loss   94.1756, KL-Loss    1.3514, KL-Weight  0.988\n",
      "TEST Batch 0300/300, Loss   77.0127, NLL-Loss   75.9597, KL-Loss    1.0657, KL-Weight  0.988\n",
      "TEST Epoch 14/20, Mean ELBO   83.3101\n",
      "TRAIN Batch 0000/960, Loss   64.0009, NLL-Loss   62.6775, KL-Loss    1.3394, KL-Weight  0.988\n",
      "TRAIN Batch 0050/960, Loss   63.7146, NLL-Loss   62.0602, KL-Loss    1.6734, KL-Weight  0.989\n",
      "TRAIN Batch 0100/960, Loss   73.9930, NLL-Loss   72.5673, KL-Loss    1.4413, KL-Weight  0.989\n",
      "TRAIN Batch 0150/960, Loss   67.8410, NLL-Loss   66.6252, KL-Loss    1.2285, KL-Weight  0.990\n",
      "TRAIN Batch 0200/960, Loss   81.0275, NLL-Loss   79.5140, KL-Loss    1.5285, KL-Weight  0.990\n",
      "TRAIN Batch 0250/960, Loss   73.7433, NLL-Loss   72.0425, KL-Loss    1.7168, KL-Weight  0.991\n",
      "TRAIN Batch 0300/960, Loss   64.0510, NLL-Loss   62.5424, KL-Loss    1.5222, KL-Weight  0.991\n",
      "TRAIN Batch 0350/960, Loss   69.1162, NLL-Loss   67.8866, KL-Loss    1.2400, KL-Weight  0.992\n",
      "TRAIN Batch 0400/960, Loss   69.4496, NLL-Loss   68.0488, KL-Loss    1.4122, KL-Weight  0.992\n",
      "TRAIN Batch 0450/960, Loss   72.0356, NLL-Loss   70.6999, KL-Loss    1.3460, KL-Weight  0.992\n",
      "TRAIN Batch 0500/960, Loss   66.8252, NLL-Loss   65.4748, KL-Loss    1.3603, KL-Weight  0.993\n",
      "TRAIN Batch 0550/960, Loss   68.7481, NLL-Loss   67.0149, KL-Loss    1.7452, KL-Weight  0.993\n",
      "TRAIN Batch 0600/960, Loss   69.1992, NLL-Loss   67.7834, KL-Loss    1.4252, KL-Weight  0.993\n",
      "TRAIN Batch 0650/960, Loss   62.5427, NLL-Loss   61.1422, KL-Loss    1.4094, KL-Weight  0.994\n",
      "TRAIN Batch 0700/960, Loss   63.9558, NLL-Loss   62.5802, KL-Loss    1.3839, KL-Weight  0.994\n",
      "TRAIN Batch 0750/960, Loss   69.9231, NLL-Loss   68.4992, KL-Loss    1.4321, KL-Weight  0.994\n",
      "TRAIN Batch 0800/960, Loss   69.9863, NLL-Loss   68.5238, KL-Loss    1.4705, KL-Weight  0.995\n",
      "TRAIN Batch 0850/960, Loss   67.3612, NLL-Loss   65.9142, KL-Loss    1.4545, KL-Weight  0.995\n",
      "TRAIN Batch 0900/960, Loss   67.8140, NLL-Loss   66.4803, KL-Loss    1.3403, KL-Weight  0.995\n",
      "TRAIN Batch 0950/960, Loss   62.8462, NLL-Loss   61.3224, KL-Loss    1.5309, KL-Weight  0.995\n",
      "TRAIN Batch 0960/960, Loss   67.4004, NLL-Loss   65.7570, KL-Loss    1.6510, KL-Weight  0.995\n",
      "TRAIN Epoch 15/20, Mean ELBO   67.4681\n",
      "Model saved at bin/2019-Dec-05-10:01:18/E15.pytorch\n",
      "VALID Batch 0000/240, Loss   80.3548, NLL-Loss   78.9501, KL-Loss    1.4112, KL-Weight  0.995\n",
      "VALID Batch 0050/240, Loss   82.1177, NLL-Loss   80.7744, KL-Loss    1.3496, KL-Weight  0.995\n",
      "VALID Batch 0100/240, Loss   79.2071, NLL-Loss   77.8105, KL-Loss    1.4030, KL-Weight  0.995\n",
      "VALID Batch 0150/240, Loss   85.3684, NLL-Loss   83.7714, KL-Loss    1.6044, KL-Weight  0.995\n",
      "VALID Batch 0200/240, Loss   79.8399, NLL-Loss   78.2148, KL-Loss    1.6326, KL-Weight  0.995\n",
      "VALID Batch 0240/240, Loss   49.6064, NLL-Loss   47.4047, KL-Loss    2.2119, KL-Weight  0.995\n",
      "VALID Epoch 15/20, Mean ELBO   83.2484\n",
      "TEST Batch 0000/300, Loss   82.8670, NLL-Loss   81.7022, KL-Loss    1.1702, KL-Weight  0.995\n",
      "TEST Batch 0050/300, Loss   87.5078, NLL-Loss   85.9634, KL-Loss    1.5515, KL-Weight  0.995\n",
      "TEST Batch 0100/300, Loss   81.6570, NLL-Loss   80.2636, KL-Loss    1.3999, KL-Weight  0.995\n",
      "TEST Batch 0150/300, Loss   85.4996, NLL-Loss   84.2282, KL-Loss    1.2772, KL-Weight  0.995\n",
      "TEST Batch 0200/300, Loss   77.2712, NLL-Loss   75.7412, KL-Loss    1.5370, KL-Weight  0.995\n",
      "TEST Batch 0250/300, Loss   96.5835, NLL-Loss   95.2813, KL-Loss    1.3082, KL-Weight  0.995\n",
      "TEST Batch 0300/300, Loss   77.1010, NLL-Loss   76.1066, KL-Loss    0.9990, KL-Weight  0.995\n",
      "TEST Epoch 15/20, Mean ELBO   83.4373\n",
      "TRAIN Batch 0000/960, Loss   62.9077, NLL-Loss   61.4332, KL-Loss    1.4813, KL-Weight  0.995\n",
      "TRAIN Batch 0050/960, Loss   60.4137, NLL-Loss   59.2660, KL-Loss    1.1528, KL-Weight  0.996\n",
      "TRAIN Batch 0100/960, Loss   68.7278, NLL-Loss   67.1001, KL-Loss    1.6345, KL-Weight  0.996\n",
      "TRAIN Batch 0150/960, Loss   62.8929, NLL-Loss   61.4566, KL-Loss    1.4420, KL-Weight  0.996\n",
      "TRAIN Batch 0200/960, Loss   75.4435, NLL-Loss   73.8393, KL-Loss    1.6103, KL-Weight  0.996\n",
      "TRAIN Batch 0250/960, Loss   71.5330, NLL-Loss   70.0807, KL-Loss    1.4576, KL-Weight  0.996\n",
      "TRAIN Batch 0300/960, Loss   68.9014, NLL-Loss   67.6105, KL-Loss    1.2953, KL-Weight  0.997\n",
      "TRAIN Batch 0350/960, Loss   56.1224, NLL-Loss   54.4929, KL-Loss    1.6348, KL-Weight  0.997\n",
      "TRAIN Batch 0400/960, Loss   61.2806, NLL-Loss   59.8918, KL-Loss    1.3931, KL-Weight  0.997\n",
      "TRAIN Batch 0450/960, Loss   63.8557, NLL-Loss   62.4988, KL-Loss    1.3609, KL-Weight  0.997\n",
      "TRAIN Batch 0500/960, Loss   69.9177, NLL-Loss   68.5096, KL-Loss    1.4120, KL-Weight  0.997\n",
      "TRAIN Batch 0550/960, Loss   66.5145, NLL-Loss   65.2729, KL-Loss    1.2449, KL-Weight  0.997\n",
      "TRAIN Batch 0600/960, Loss   74.1157, NLL-Loss   72.8860, KL-Loss    1.2328, KL-Weight  0.997\n",
      "TRAIN Batch 0650/960, Loss   76.9196, NLL-Loss   75.5046, KL-Loss    1.4185, KL-Weight  0.998\n",
      "TRAIN Batch 0700/960, Loss   68.8872, NLL-Loss   67.5471, KL-Loss    1.3431, KL-Weight  0.998\n",
      "TRAIN Batch 0750/960, Loss   67.1696, NLL-Loss   65.8070, KL-Loss    1.3656, KL-Weight  0.998\n",
      "TRAIN Batch 0800/960, Loss   72.6937, NLL-Loss   71.2115, KL-Loss    1.4853, KL-Weight  0.998\n",
      "TRAIN Batch 0850/960, Loss   81.3707, NLL-Loss   80.0294, KL-Loss    1.3439, KL-Weight  0.998\n",
      "TRAIN Batch 0900/960, Loss   64.0474, NLL-Loss   62.7554, KL-Loss    1.2945, KL-Weight  0.998\n",
      "TRAIN Batch 0950/960, Loss   67.9487, NLL-Loss   66.6914, KL-Loss    1.2596, KL-Weight  0.998\n",
      "TRAIN Batch 0960/960, Loss   71.7040, NLL-Loss   70.6768, KL-Loss    1.0290, KL-Weight  0.998\n",
      "TRAIN Epoch 16/20, Mean ELBO   66.8166\n",
      "Model saved at bin/2019-Dec-05-10:01:18/E16.pytorch\n",
      "VALID Batch 0000/240, Loss   80.9169, NLL-Loss   79.5808, KL-Loss    1.3384, KL-Weight  0.998\n",
      "VALID Batch 0050/240, Loss   82.3373, NLL-Loss   81.1022, KL-Loss    1.2373, KL-Weight  0.998\n",
      "VALID Batch 0100/240, Loss   79.4792, NLL-Loss   78.1912, KL-Loss    1.2902, KL-Weight  0.998\n",
      "VALID Batch 0150/240, Loss   86.3249, NLL-Loss   84.9416, KL-Loss    1.3857, KL-Weight  0.998\n",
      "VALID Batch 0200/240, Loss   79.4171, NLL-Loss   77.9382, KL-Loss    1.4815, KL-Weight  0.998\n",
      "VALID Batch 0240/240, Loss   53.7641, NLL-Loss   51.8088, KL-Loss    1.9588, KL-Weight  0.998\n",
      "VALID Epoch 16/20, Mean ELBO   83.3344\n",
      "TEST Batch 0000/300, Loss   82.4767, NLL-Loss   81.4302, KL-Loss    1.0484, KL-Weight  0.998\n",
      "TEST Batch 0050/300, Loss   87.6979, NLL-Loss   86.2786, KL-Loss    1.4218, KL-Weight  0.998\n",
      "TEST Batch 0100/300, Loss   81.5662, NLL-Loss   80.3231, KL-Loss    1.2453, KL-Weight  0.998\n",
      "TEST Batch 0150/300, Loss   85.2012, NLL-Loss   84.0416, KL-Loss    1.1616, KL-Weight  0.998\n",
      "TEST Batch 0200/300, Loss   77.7411, NLL-Loss   76.3279, KL-Loss    1.4158, KL-Weight  0.998\n",
      "TEST Batch 0250/300, Loss   95.6904, NLL-Loss   94.4938, KL-Loss    1.1987, KL-Weight  0.998\n",
      "TEST Batch 0300/300, Loss   76.7533, NLL-Loss   75.8592, KL-Loss    0.8957, KL-Weight  0.998\n",
      "TEST Epoch 16/20, Mean ELBO   83.4387\n",
      "TRAIN Batch 0000/960, Loss   61.6216, NLL-Loss   60.3080, KL-Loss    1.3159, KL-Weight  0.998\n",
      "TRAIN Batch 0050/960, Loss   61.3853, NLL-Loss   59.7776, KL-Loss    1.6103, KL-Weight  0.998\n",
      "TRAIN Batch 0100/960, Loss   71.8479, NLL-Loss   70.1640, KL-Loss    1.6866, KL-Weight  0.998\n",
      "TRAIN Batch 0150/960, Loss   63.9066, NLL-Loss   62.5293, KL-Loss    1.3794, KL-Weight  0.998\n",
      "TRAIN Batch 0200/960, Loss   61.4598, NLL-Loss   59.9372, KL-Loss    1.5248, KL-Weight  0.999\n",
      "TRAIN Batch 0250/960, Loss   62.4585, NLL-Loss   60.9377, KL-Loss    1.5229, KL-Weight  0.999\n",
      "TRAIN Batch 0300/960, Loss   62.2917, NLL-Loss   60.9295, KL-Loss    1.3639, KL-Weight  0.999\n",
      "TRAIN Batch 0350/960, Loss   68.5554, NLL-Loss   66.9399, KL-Loss    1.6175, KL-Weight  0.999\n",
      "TRAIN Batch 0400/960, Loss   54.2873, NLL-Loss   53.0714, KL-Loss    1.2173, KL-Weight  0.999\n",
      "TRAIN Batch 0450/960, Loss   67.6276, NLL-Loss   66.2657, KL-Loss    1.3635, KL-Weight  0.999\n",
      "TRAIN Batch 0500/960, Loss   73.3072, NLL-Loss   71.7777, KL-Loss    1.5312, KL-Weight  0.999\n",
      "TRAIN Batch 0550/960, Loss   63.9053, NLL-Loss   62.3926, KL-Loss    1.5143, KL-Weight  0.999\n",
      "TRAIN Batch 0600/960, Loss   69.7030, NLL-Loss   68.4076, KL-Loss    1.2966, KL-Weight  0.999\n",
      "TRAIN Batch 0650/960, Loss   61.6509, NLL-Loss   60.0770, KL-Loss    1.5754, KL-Weight  0.999\n",
      "TRAIN Batch 0700/960, Loss   63.3313, NLL-Loss   61.9887, KL-Loss    1.3438, KL-Weight  0.999\n",
      "TRAIN Batch 0750/960, Loss   66.3752, NLL-Loss   65.0664, KL-Loss    1.3099, KL-Weight  0.999\n",
      "TRAIN Batch 0800/960, Loss   61.7902, NLL-Loss   60.5152, KL-Loss    1.2760, KL-Weight  0.999\n",
      "TRAIN Batch 0850/960, Loss   66.7057, NLL-Loss   65.2988, KL-Loss    1.4080, KL-Weight  0.999\n",
      "TRAIN Batch 0900/960, Loss   60.2178, NLL-Loss   59.0241, KL-Loss    1.1946, KL-Weight  0.999\n",
      "TRAIN Batch 0950/960, Loss   64.0939, NLL-Loss   62.6782, KL-Loss    1.4166, KL-Weight  0.999\n",
      "TRAIN Batch 0960/960, Loss   50.4261, NLL-Loss   49.0794, KL-Loss    1.3476, KL-Weight  0.999\n",
      "TRAIN Epoch 17/20, Mean ELBO   66.1604\n",
      "Model saved at bin/2019-Dec-05-10:01:18/E17.pytorch\n",
      "VALID Batch 0000/240, Loss   81.0732, NLL-Loss   79.6930, KL-Loss    1.3811, KL-Weight  0.999\n",
      "VALID Batch 0050/240, Loss   82.5959, NLL-Loss   81.3244, KL-Loss    1.2724, KL-Weight  0.999\n",
      "VALID Batch 0100/240, Loss   79.1963, NLL-Loss   77.7915, KL-Loss    1.4058, KL-Weight  0.999\n",
      "VALID Batch 0150/240, Loss   85.9094, NLL-Loss   84.3871, KL-Loss    1.5234, KL-Weight  0.999\n",
      "VALID Batch 0200/240, Loss   79.1101, NLL-Loss   77.5084, KL-Loss    1.6028, KL-Weight  0.999\n",
      "VALID Batch 0240/240, Loss   50.7902, NLL-Loss   48.7351, KL-Loss    2.0565, KL-Weight  0.999\n",
      "VALID Epoch 17/20, Mean ELBO   83.5255\n",
      "TEST Batch 0000/300, Loss   82.6531, NLL-Loss   81.5304, KL-Loss    1.1235, KL-Weight  0.999\n",
      "TEST Batch 0050/300, Loss   87.2623, NLL-Loss   85.7299, KL-Loss    1.5334, KL-Weight  0.999\n",
      "TEST Batch 0100/300, Loss   82.5364, NLL-Loss   81.1244, KL-Loss    1.4130, KL-Weight  0.999\n",
      "TEST Batch 0150/300, Loss   84.8132, NLL-Loss   83.5788, KL-Loss    1.2353, KL-Weight  0.999\n",
      "TEST Batch 0200/300, Loss   77.1567, NLL-Loss   75.6683, KL-Loss    1.4894, KL-Weight  0.999\n",
      "TEST Batch 0250/300, Loss   97.2432, NLL-Loss   95.8974, KL-Loss    1.3467, KL-Weight  0.999\n",
      "TEST Batch 0300/300, Loss   77.0016, NLL-Loss   76.0823, KL-Loss    0.9199, KL-Weight  0.999\n",
      "TEST Epoch 17/20, Mean ELBO   83.6981\n",
      "TRAIN Batch 0000/960, Loss   61.3674, NLL-Loss   59.9908, KL-Loss    1.3776, KL-Weight  0.999\n",
      "TRAIN Batch 0050/960, Loss   64.6757, NLL-Loss   63.3370, KL-Loss    1.3395, KL-Weight  0.999\n",
      "TRAIN Batch 0100/960, Loss   61.1130, NLL-Loss   59.8493, KL-Loss    1.2645, KL-Weight  0.999\n",
      "TRAIN Batch 0150/960, Loss   70.4105, NLL-Loss   68.9084, KL-Loss    1.5029, KL-Weight  0.999\n",
      "TRAIN Batch 0200/960, Loss   73.8215, NLL-Loss   72.3021, KL-Loss    1.5203, KL-Weight  0.999\n",
      "TRAIN Batch 0250/960, Loss   73.1698, NLL-Loss   71.7666, KL-Loss    1.4039, KL-Weight  0.999\n",
      "TRAIN Batch 0300/960, Loss   71.6137, NLL-Loss   70.2360, KL-Loss    1.3784, KL-Weight  0.999\n",
      "TRAIN Batch 0350/960, Loss   56.1692, NLL-Loss   54.7380, KL-Loss    1.4318, KL-Weight  1.000\n",
      "TRAIN Batch 0400/960, Loss   80.3937, NLL-Loss   79.0654, KL-Loss    1.3289, KL-Weight  1.000\n",
      "TRAIN Batch 0450/960, Loss   71.5112, NLL-Loss   69.9724, KL-Loss    1.5395, KL-Weight  1.000\n",
      "TRAIN Batch 0500/960, Loss   73.2024, NLL-Loss   71.8030, KL-Loss    1.3999, KL-Weight  1.000\n",
      "TRAIN Batch 0550/960, Loss   69.0132, NLL-Loss   67.7170, KL-Loss    1.2967, KL-Weight  1.000\n",
      "TRAIN Batch 0600/960, Loss   66.8469, NLL-Loss   65.4598, KL-Loss    1.3877, KL-Weight  1.000\n",
      "TRAIN Batch 0650/960, Loss   71.2962, NLL-Loss   69.8525, KL-Loss    1.4443, KL-Weight  1.000\n",
      "TRAIN Batch 0700/960, Loss   58.8181, NLL-Loss   57.5412, KL-Loss    1.2774, KL-Weight  1.000\n",
      "TRAIN Batch 0750/960, Loss   64.1827, NLL-Loss   62.9199, KL-Loss    1.2633, KL-Weight  1.000\n",
      "TRAIN Batch 0800/960, Loss   57.7136, NLL-Loss   56.5183, KL-Loss    1.1957, KL-Weight  1.000\n",
      "TRAIN Batch 0850/960, Loss   64.2022, NLL-Loss   62.8096, KL-Loss    1.3930, KL-Weight  1.000\n",
      "TRAIN Batch 0900/960, Loss   63.6622, NLL-Loss   62.3951, KL-Loss    1.2674, KL-Weight  1.000\n",
      "TRAIN Batch 0950/960, Loss   63.5232, NLL-Loss   62.1883, KL-Loss    1.3353, KL-Weight  1.000\n",
      "TRAIN Batch 0960/960, Loss   63.9370, NLL-Loss   62.6495, KL-Loss    1.2879, KL-Weight  1.000\n",
      "TRAIN Epoch 18/20, Mean ELBO   65.5684\n",
      "Model saved at bin/2019-Dec-05-10:01:18/E18.pytorch\n",
      "VALID Batch 0000/240, Loss   80.5172, NLL-Loss   79.0789, KL-Loss    1.4387, KL-Weight  1.000\n",
      "VALID Batch 0050/240, Loss   83.0660, NLL-Loss   81.7218, KL-Loss    1.3446, KL-Weight  1.000\n",
      "VALID Batch 0100/240, Loss   79.5832, NLL-Loss   78.2660, KL-Loss    1.3176, KL-Weight  1.000\n",
      "VALID Batch 0150/240, Loss   86.3575, NLL-Loss   84.7663, KL-Loss    1.5916, KL-Weight  1.000\n",
      "VALID Batch 0200/240, Loss   78.9536, NLL-Loss   77.3033, KL-Loss    1.6508, KL-Weight  1.000\n",
      "VALID Batch 0240/240, Loss   50.3735, NLL-Loss   48.5642, KL-Loss    1.8098, KL-Weight  1.000\n",
      "VALID Epoch 18/20, Mean ELBO   83.6382\n",
      "TEST Batch 0000/300, Loss   82.6581, NLL-Loss   81.4998, KL-Loss    1.1586, KL-Weight  1.000\n",
      "TEST Batch 0050/300, Loss   87.7936, NLL-Loss   86.2446, KL-Loss    1.5494, KL-Weight  1.000\n",
      "TEST Batch 0100/300, Loss   82.3327, NLL-Loss   80.8927, KL-Loss    1.4404, KL-Weight  1.000\n",
      "TEST Batch 0150/300, Loss   85.4654, NLL-Loss   84.2443, KL-Loss    1.2215, KL-Weight  1.000\n",
      "TEST Batch 0200/300, Loss   76.5730, NLL-Loss   75.0821, KL-Loss    1.4912, KL-Weight  1.000\n",
      "TEST Batch 0250/300, Loss   96.1332, NLL-Loss   94.7852, KL-Loss    1.3483, KL-Weight  1.000\n",
      "TEST Batch 0300/300, Loss   76.5384, NLL-Loss   75.5210, KL-Loss    1.0177, KL-Weight  1.000\n",
      "TEST Epoch 18/20, Mean ELBO   83.8051\n",
      "TRAIN Batch 0000/960, Loss   66.1412, NLL-Loss   64.4505, KL-Loss    1.6912, KL-Weight  1.000\n",
      "TRAIN Batch 0050/960, Loss   71.2134, NLL-Loss   69.8909, KL-Loss    1.3227, KL-Weight  1.000\n",
      "TRAIN Batch 0100/960, Loss   58.9587, NLL-Loss   57.5806, KL-Loss    1.3784, KL-Weight  1.000\n",
      "TRAIN Batch 0150/960, Loss   59.6304, NLL-Loss   58.3725, KL-Loss    1.2582, KL-Weight  1.000\n",
      "TRAIN Batch 0200/960, Loss   62.1852, NLL-Loss   60.7484, KL-Loss    1.4372, KL-Weight  1.000\n",
      "TRAIN Batch 0250/960, Loss   68.5000, NLL-Loss   67.1385, KL-Loss    1.3617, KL-Weight  1.000\n",
      "TRAIN Batch 0300/960, Loss   64.0585, NLL-Loss   62.4053, KL-Loss    1.6536, KL-Weight  1.000\n",
      "TRAIN Batch 0350/960, Loss   69.3083, NLL-Loss   68.0234, KL-Loss    1.2851, KL-Weight  1.000\n",
      "TRAIN Batch 0400/960, Loss   73.0734, NLL-Loss   71.7767, KL-Loss    1.2969, KL-Weight  1.000\n",
      "TRAIN Batch 0750/960, Loss   64.0972, NLL-Loss   62.6286, KL-Loss    1.4688, KL-Weight  1.000\n",
      "TRAIN Batch 0800/960, Loss   58.4846, NLL-Loss   57.2665, KL-Loss    1.2183, KL-Weight  1.000\n",
      "TRAIN Batch 0850/960, Loss   66.9333, NLL-Loss   65.4523, KL-Loss    1.4812, KL-Weight  1.000\n",
      "TRAIN Batch 0900/960, Loss   70.4127, NLL-Loss   69.1062, KL-Loss    1.3067, KL-Weight  1.000\n",
      "TRAIN Batch 0950/960, Loss   72.4706, NLL-Loss   71.3459, KL-Loss    1.1249, KL-Weight  1.000\n",
      "TRAIN Batch 0960/960, Loss   75.2190, NLL-Loss   73.8658, KL-Loss    1.3533, KL-Weight  1.000\n",
      "TRAIN Epoch 19/20, Mean ELBO   64.9677\n",
      "Model saved at bin/2019-Dec-05-10:01:18/E19.pytorch\n",
      "VALID Batch 0000/240, Loss   80.5023, NLL-Loss   79.1414, KL-Loss    1.3610, KL-Weight  1.000\n",
      "VALID Batch 0050/240, Loss   82.5350, NLL-Loss   81.2293, KL-Loss    1.3058, KL-Weight  1.000\n",
      "VALID Batch 0100/240, Loss   79.6440, NLL-Loss   78.3724, KL-Loss    1.2718, KL-Weight  1.000\n",
      "VALID Batch 0150/240, Loss   87.2692, NLL-Loss   85.8073, KL-Loss    1.4621, KL-Weight  1.000\n",
      "VALID Batch 0200/240, Loss   79.6042, NLL-Loss   78.0310, KL-Loss    1.5734, KL-Weight  1.000\n",
      "VALID Batch 0240/240, Loss   50.3509, NLL-Loss   48.6461, KL-Loss    1.7050, KL-Weight  1.000\n",
      "VALID Epoch 19/20, Mean ELBO   83.7689\n",
      "TEST Batch 0000/300, Loss   82.5236, NLL-Loss   81.3881, KL-Loss    1.1356, KL-Weight  1.000\n",
      "TEST Batch 0050/300, Loss   87.4884, NLL-Loss   86.0630, KL-Loss    1.4256, KL-Weight  1.000\n",
      "TEST Batch 0100/300, Loss   82.6580, NLL-Loss   81.3142, KL-Loss    1.3440, KL-Weight  1.000\n",
      "TEST Batch 0150/300, Loss   85.1894, NLL-Loss   83.9608, KL-Loss    1.2287, KL-Weight  1.000\n",
      "TEST Batch 0200/300, Loss   77.2724, NLL-Loss   75.8586, KL-Loss    1.4139, KL-Weight  1.000\n",
      "TEST Batch 0250/300, Loss   95.9825, NLL-Loss   94.7716, KL-Loss    1.2110, KL-Weight  1.000\n",
      "TEST Batch 0300/300, Loss   77.2578, NLL-Loss   76.2353, KL-Loss    1.0225, KL-Weight  1.000\n",
      "TEST Epoch 19/20, Mean ELBO   83.9425\n"
     ]
    }
   ],
   "source": [
    "%pdb on\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    for split in splits:\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            dataset=_datasets[split],\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=split=='train',\n",
    "            num_workers=cpu_count(),\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "\n",
    "        tracker = defaultdict(tensor)\n",
    "\n",
    "        # Enable/Disable Dropout\n",
    "        if split == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        for iteration, batch in enumerate(data_loader):\n",
    "            \n",
    "            batch_size = batch['src_input'].size(0)\n",
    "            \n",
    "            for k, v in batch.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    batch[k] = to_var(v)\n",
    "            \n",
    "            # loss calculation | additional Condition ！!!\n",
    "            cal_dict = model(batch['tgt_input'], batch['tgt_length'])\n",
    "            logp, mean, logv, z = cal_dict['logp'], cal_dict['mean'], cal_dict['logv'], cal_dict['z']\n",
    "            \n",
    "            loss_dict = model.loss(logp, batch['tgt_target'], batch['tgt_length'], mean, logv, args.anneal_function, step, args.k, args.x0, bow_input=z)\n",
    "            loss, NLL_loss, KL_weight, KL_loss, avg_bow_loss = loss_dict['loss'], loss_dict['NLL_loss'], loss_dict['KL_weight'], loss_dict['KL_loss'], loss_dict.get('avg_bow_loss')\n",
    "\n",
    "            # backward + optimization\n",
    "            if split == 'train':\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                step += 1\n",
    "\n",
    "            # bookkeepeing\n",
    "            tracker['ELBO'] = torch.cat((tracker['ELBO'], loss.data.view(1)))\n",
    "\n",
    "            if args.tensorboard_logging:\n",
    "                writer.add_scalar(\"%s/ELBO\"%split.upper(), loss.data.item(), epoch*len(data_loader) + iteration)\n",
    "                writer.add_scalar(\"%s/NLL Loss\"%split.upper(), NLL_loss.data.item()/batch_size, epoch*len(data_loader) + iteration)\n",
    "                writer.add_scalar(\"%s/KL Loss\"%split.upper(), KL_loss.data.item()/batch_size, epoch*len(data_loader) + iteration)\n",
    "                writer.add_scalar(\"%s/KL Weight\"%split.upper(), KL_weight, epoch*len(data_loader) + iteration)\n",
    "                if avg_bow_loss is not None:\n",
    "                    writer.add_scalar(\"%s/BOW Loss\"%split.upper(), avg_bow_loss, epoch*len(data_loader) + iteration)\n",
    "\n",
    "            if iteration % args.print_every == 0 or iteration+1 == len(data_loader):\n",
    "                print_text = \"%s Batch %04d/%i, Loss %9.4f, NLL-Loss %9.4f, KL-Loss %9.4f, KL-Weight %6.3f\"%(split.upper(), iteration, len(data_loader)-1, loss.data.item(), NLL_loss.data.item()/batch_size, KL_loss.data.item()/batch_size, KL_weight)\n",
    "                if avg_bow_loss is not None:\n",
    "                    print_text += ', BOW Loss %9.4f,'%(avg_bow_loss)\n",
    "                print(print_text)\n",
    "\n",
    "            if split == 'valid':\n",
    "                if 'target_sents' not in tracker:\n",
    "                    tracker['target_sents'] = list()\n",
    "                tracker['target_sents'] += idx2word(batch['tgt_target'].data, i2w=train_target_ptb.get_i2w(), pad_idx=PAD_INDEX)\n",
    "                tracker['z'] = torch.cat((tracker['z'], z.data), dim=0)\n",
    "\n",
    "        print(\"%s Epoch %02d/%i, Mean ELBO %9.4f\"%(split.upper(), epoch, args.epochs, torch.mean(tracker['ELBO'])))\n",
    "\n",
    "        if args.tensorboard_logging:\n",
    "            writer.add_scalar(\"%s-Epoch/ELBO\"%split.upper(), torch.mean(tracker['ELBO']), epoch)\n",
    "\n",
    "        # save a dump of all sentences and the encoded latent space\n",
    "        if split == 'valid':\n",
    "            dump = {'target_sents':tracker['target_sents'], 'z':tracker['z'].tolist()}\n",
    "            if not os.path.exists(os.path.join('dumps', ts)):\n",
    "                os.makedirs('dumps/'+ts)\n",
    "            with open(os.path.join('dumps/'+ts+'/valid_E%i.json'%epoch), 'w') as dump_file:\n",
    "                json.dump(dump,dump_file)\n",
    "\n",
    "        # save checkpoint\n",
    "        if split == 'train':\n",
    "            checkpoint_path = os.path.join(save_model_path, \"E%i.pytorch\"%(epoch))\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(\"Model saved at %s\"%checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxrss = 2082004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cpu', torch.int64, (3,))\t1\n",
      "('cuda:0', torch.int64, (3, 50))\t4\n",
      "('cuda:0', torch.int64, (3,))\t2\n",
      "('cuda:0', torch.float32, (3, 16))\t3\n",
      "('cuda:0', torch.float32, (3, 20, 12106))\t1\n",
      "('cuda:0', torch.float32, ())\t3\n",
      "('cuda:0', torch.float32, (301,))\t1\n",
      "('cuda:0', torch.float32, (12106, 300))\t3\n",
      "('cuda:0', torch.float32, (768, 300))\t6\n",
      "('cuda:0', torch.float32, (768, 256))\t6\n",
      "('cuda:0', torch.float32, (768,))\t12\n",
      "('cuda:0', torch.float32, (12106, 256))\t6\n",
      "('cuda:0', torch.float32, (12106,))\t6\n",
      "('cuda:0', torch.float32, (16, 300))\t6\n",
      "('cuda:0', torch.float32, (16,))\t6\n",
      "('cuda:0', torch.float32, (256, 16))\t3\n",
      "('cuda:0', torch.float32, (256,))\t3\n"
     ]
    }
   ],
   "source": [
    "def debug_memory():\n",
    "    import collections, gc, resource, torch\n",
    "    print('maxrss = {}'.format(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss))\n",
    "    tensors = collections.Counter((str(o.device), o.dtype, tuple(o.shape)) for o in gc.get_objects() if torch.is_tensor(o))\n",
    "    for line in tensors.items():\n",
    "        print('{}\\t{}'.format(*line))\n",
    "debug_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
