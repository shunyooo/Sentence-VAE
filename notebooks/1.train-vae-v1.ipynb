{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from multiprocessing import cpu_count\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "from ptb import PTB\n",
    "from utils import to_var, idx2word, expierment_name, AttributeDict\n",
    "from model_bowloss import SentenceVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AttrDict{'data_dir': 'data/eccos', 'create_data': False, 'max_sequence_length': 50, 'min_occ': 1, 'test': False, 'epochs': 10, 'batch_size': 32, 'learning_rate': 0.001, 'embedding_size': 300, 'rnn_type': 'gru', 'hidden_size': 256, 'num_layers': 1, 'bidirectional': False, 'latent_size': 16, 'word_dropout': 0, 'embedding_dropout': 0.5, 'anneal_function': 'logistic', 'k': 0.0025, 'x0': 2500, 'print_every': 50, 'tensorboard_logging': True, 'logdir': '/root/user/work/logs/', 'save_model_path': 'bin', 'expierment_name': 'copy2copy_legacy_vae'}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_dir = 'data/simple-examples'\n",
    "data_dir = 'data/eccos'\n",
    "\n",
    "args = {\n",
    "    'data_dir': data_dir,\n",
    "    'create_data': False,\n",
    "    'max_sequence_length': 50,\n",
    "    'min_occ': 1,\n",
    "    'test': False,\n",
    "\n",
    "    'epochs': 10,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "\n",
    "    'embedding_size': 300,\n",
    "    'rnn_type': 'gru',\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 1,\n",
    "    'bidirectional': False,\n",
    "    'latent_size': 16,\n",
    "    'word_dropout': 0,\n",
    "    'embedding_dropout': 0.5,\n",
    "\n",
    "    'anneal_function': 'logistic',\n",
    "    'k': 0.0025,\n",
    "    'x0': 2500,\n",
    "\n",
    "    'print_every': 50,\n",
    "    'tensorboard_logging': True,\n",
    "    'logdir': '/root/user/work/logs/',\n",
    "    'save_model_path': 'bin',\n",
    "    'expierment_name': 'copy2copy_legacy_vae',\n",
    "    \n",
    "}\n",
    "\n",
    "args = AttributeDict(args)\n",
    "\n",
    "args.rnn_type = args.rnn_type.lower()\n",
    "args.anneal_function = args.anneal_function.lower()\n",
    "\n",
    "assert args.rnn_type in ['rnn', 'lstm', 'gru']\n",
    "assert args.anneal_function in ['logistic', 'linear']\n",
    "assert 0 <= args.word_dropout <= 1\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data/eccos\n",
      "('train', 'src')\n",
      "vocab: 5619, records: 30726\n",
      "('train', 'tgt')\n",
      "vocab: 12106, records: 30726\n",
      "('valid', 'src')\n",
      "vocab: 5619, records: 7682\n",
      "('valid', 'tgt')\n",
      "vocab: 12106, records: 7682\n",
      "CPU times: user 988 ms, sys: 69.2 ms, total: 1.06 s\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import itertools\n",
    "splits = ['train', 'valid'] + (['test'] if args.test else [])\n",
    "datasets = OrderedDict()\n",
    "print(f'loading {args.data_dir}')\n",
    "for split, src_tgt in itertools.product(splits, ['src', 'tgt']):\n",
    "    key = (split, src_tgt)\n",
    "    print(key)\n",
    "    datasets[key] = PTB(\n",
    "        data_dir=f'{args.data_dir}/{src_tgt}',\n",
    "        split=split,\n",
    "        create_data=args.create_data,\n",
    "        max_sequence_length=args.max_sequence_length if src_tgt == 'tgt' else args.max_sequence_length_src,\n",
    "        min_occ=args.min_occ\n",
    "    )\n",
    "    print(f'vocab: {datasets[key].vocab_size}, records: {len(datasets[key].data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "■ Input\n",
      "<sos> bbクリーム 進化 クリーム\n",
      "■ Target\n",
      "bbクリーム の 進化 版 ? cc クリーム が 気 に なる ... ! <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "# 実際のデータ確認\n",
    "def ids2text(id_list, ptb):\n",
    "    return ' '.join([ptb.i2w[f'{i}'] for i in id_list])\n",
    "\n",
    "_ptb_src = datasets[('train', 'src')]\n",
    "_ptb_tgt = datasets[('train', 'tgt')]\n",
    "index = str(101)\n",
    "_sample_src, _sample_tgt = _ptb_src.data[index], _ptb_tgt[index]\n",
    "print(f'■ Input\\n{ids2text(_sample_src[\"input\"], _ptb_src)}')\n",
    "print(f'■ Target\\n{ids2text(_sample_tgt[\"target\"], _ptb_tgt)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptb import SOS_INDEX, EOS_INDEX, PAD_INDEX, UNK_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "model = SentenceVAE(\n",
    "    vocab_size=datasets[('train', 'tgt')].vocab_size,\n",
    "    sos_idx=SOS_INDEX,\n",
    "    eos_idx=EOS_INDEX,\n",
    "    pad_idx=PAD_INDEX,\n",
    "    unk_idx=UNK_INDEX,\n",
    "    max_sequence_length=args.max_sequence_length,\n",
    "    embedding_size=args.embedding_size,\n",
    "    rnn_type=args.rnn_type,\n",
    "    hidden_size=args.hidden_size,\n",
    "    word_dropout=args.word_dropout,\n",
    "    embedding_dropout=args.embedding_dropout,\n",
    "    latent_size=args.latent_size,\n",
    "    num_layers=args.num_layers,\n",
    "    bidirectional=args.bidirectional,\n",
    "    \n",
    "    use_bow_loss=False\n",
    "    )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceVAE(\n",
       "  (embedding): Embedding(12106, 300)\n",
       "  (embedding_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (encoder_rnn): GRU(300, 256, batch_first=True)\n",
       "  (decoder_rnn): GRU(300, 256, batch_first=True)\n",
       "  (hidden2mean): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (hidden2logv): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (latent2hidden): Linear(in_features=16, out_features=256, bias=True)\n",
       "  (outputs2vocab): Linear(in_features=256, out_features=12106, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard logging: True\n"
     ]
    }
   ],
   "source": [
    "print(f'tensorboard logging: {args.tensorboard_logging}')\n",
    "ts = time.strftime('%Y-%b-%d-%H:%M:%S', time.gmtime())\n",
    "if args.tensorboard_logging:\n",
    "    writer = SummaryWriter(os.path.join(args.logdir, expierment_name(args,ts)))\n",
    "    writer.add_text(\"model\", str(model))\n",
    "    writer.add_text(\"args\", str(args))\n",
    "    writer.add_text(\"ts\", ts)\n",
    "    \n",
    "save_model_path = os.path.join(args.save_model_path, ts)\n",
    "os.makedirs(save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys([('train', 'src'), ('train', 'tgt'), ('valid', 'src'), ('valid', 'tgt')])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> 大人気 <unk> ピンク ♡ コンビニ 買える 「 さくら リップ 」 に 限定 色 が 登場 <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "大人気 <unk> ピンク ♡ コンビニ 買える 「 さくら リップ 」 に 限定 色 が 登場 <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "ae_datasets = {split: dataset for (split, src_tgt), dataset in datasets.items() if src_tgt == 'tgt'}\n",
    "print(ids2text(ae_datasets['train'][0]['input'], ae_datasets['train']))\n",
    "print(ids2text(ae_datasets['train'][0]['target'], ae_datasets['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_datasets = ae_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n",
      "TRAIN Batch 0000/960, Loss  164.8554, NLL-Loss  164.8547, KL-Loss    0.3618, KL-Weight  0.002\n",
      "TRAIN Batch 0050/960, Loss  125.5944, NLL-Loss  125.5571, KL-Loss   17.0557, KL-Weight  0.002\n",
      "TRAIN Batch 0100/960, Loss  107.2972, NLL-Loss  107.2405, KL-Loss   22.9365, KL-Weight  0.002\n",
      "TRAIN Batch 0150/960, Loss  120.9380, NLL-Loss  120.8284, KL-Loss   39.1450, KL-Weight  0.003\n",
      "TRAIN Batch 0200/960, Loss  106.7408, NLL-Loss  106.5978, KL-Loss   45.0806, KL-Weight  0.003\n",
      "TRAIN Batch 0250/960, Loss  111.4324, NLL-Loss  111.2325, KL-Loss   55.6257, KL-Weight  0.004\n",
      "TRAIN Batch 0300/960, Loss  104.1365, NLL-Loss  103.8802, KL-Loss   62.9739, KL-Weight  0.004\n",
      "TRAIN Batch 0350/960, Loss   95.4259, NLL-Loss   95.1332, KL-Loss   63.4965, KL-Weight  0.005\n",
      "TRAIN Batch 0400/960, Loss  104.1106, NLL-Loss  103.7683, KL-Loss   65.5763, KL-Weight  0.005\n",
      "TRAIN Batch 0450/960, Loss   90.2098, NLL-Loss   89.8395, KL-Loss   62.6430, KL-Weight  0.006\n",
      "TRAIN Batch 0500/960, Loss  106.2779, NLL-Loss  105.8153, KL-Loss   69.1198, KL-Weight  0.007\n",
      "TRAIN Batch 0550/960, Loss   83.8146, NLL-Loss   83.2836, KL-Loss   70.0788, KL-Weight  0.008\n",
      "TRAIN Batch 0600/960, Loss   86.4279, NLL-Loss   85.8552, KL-Loss   66.7707, KL-Weight  0.009\n",
      "TRAIN Batch 0650/960, Loss   81.0325, NLL-Loss   80.4018, KL-Loss   64.9728, KL-Weight  0.010\n",
      "TRAIN Batch 0700/960, Loss   76.8508, NLL-Loss   76.1667, KL-Loss   62.2702, KL-Weight  0.011\n",
      "TRAIN Batch 0750/960, Loss   71.0928, NLL-Loss   70.3632, KL-Loss   58.6885, KL-Weight  0.012\n",
      "TRAIN Batch 0800/960, Loss   83.0210, NLL-Loss   82.1856, KL-Loss   59.4070, KL-Weight  0.014\n",
      "TRAIN Batch 0850/960, Loss  101.4262, NLL-Loss  100.4626, KL-Loss   60.5781, KL-Weight  0.016\n",
      "TRAIN Batch 0900/960, Loss   79.0434, NLL-Loss   77.9554, KL-Loss   60.4880, KL-Weight  0.018\n",
      "TRAIN Batch 0950/960, Loss   78.8680, NLL-Loss   77.7064, KL-Loss   57.1324, KL-Weight  0.020\n",
      "TRAIN Batch 0960/960, Loss  109.9744, NLL-Loss  108.7204, KL-Loss   60.1821, KL-Weight  0.021\n",
      "TRAIN Epoch 00/10, Mean ELBO   97.5025\n",
      "Model saved at bin/2019-Nov-29-05:41:12/E0.pytorch\n",
      "VALID Batch 0000/240, Loss   78.3627, NLL-Loss   77.1446, KL-Loss   58.3185, KL-Weight  0.021\n",
      "VALID Batch 0050/240, Loss   80.7349, NLL-Loss   79.5654, KL-Loss   55.9904, KL-Weight  0.021\n",
      "VALID Batch 0100/240, Loss   77.4511, NLL-Loss   76.3249, KL-Loss   53.9194, KL-Weight  0.021\n",
      "VALID Batch 0150/240, Loss   85.2811, NLL-Loss   84.1174, KL-Loss   55.7122, KL-Weight  0.021\n",
      "VALID Batch 0200/240, Loss   73.7113, NLL-Loss   72.5454, KL-Loss   55.8200, KL-Weight  0.021\n",
      "VALID Batch 0240/240, Loss   49.7077, NLL-Loss   48.6597, KL-Loss   50.1730, KL-Weight  0.021\n",
      "VALID Epoch 00/10, Mean ELBO   80.2190\n",
      "TRAIN Batch 0000/960, Loss   81.5021, NLL-Loss   80.2792, KL-Loss   58.5483, KL-Weight  0.021\n",
      "TRAIN Batch 0050/960, Loss   80.3210, NLL-Loss   78.8947, KL-Loss   60.4297, KL-Weight  0.024\n",
      "TRAIN Batch 0100/960, Loss   77.8591, NLL-Loss   76.3046, KL-Loss   58.3056, KL-Weight  0.027\n",
      "TRAIN Batch 0150/960, Loss   90.3602, NLL-Loss   88.7220, KL-Loss   54.4146, KL-Weight  0.030\n",
      "TRAIN Batch 0200/960, Loss   83.6542, NLL-Loss   81.7521, KL-Loss   55.9807, KL-Weight  0.034\n",
      "TRAIN Batch 0250/960, Loss   71.5807, NLL-Loss   69.5785, KL-Loss   52.2394, KL-Weight  0.038\n",
      "TRAIN Batch 0300/960, Loss   75.6191, NLL-Loss   73.4101, KL-Loss   51.1226, KL-Weight  0.043\n",
      "TRAIN Batch 0350/960, Loss   80.9023, NLL-Loss   78.6341, KL-Loss   46.5899, KL-Weight  0.049\n",
      "TRAIN Batch 0400/960, Loss   77.1726, NLL-Loss   74.7291, KL-Loss   44.5806, KL-Weight  0.055\n",
      "TRAIN Batch 0450/960, Loss   79.0020, NLL-Loss   76.3399, KL-Loss   43.1754, KL-Weight  0.062\n",
      "TRAIN Batch 0500/960, Loss   72.4516, NLL-Loss   69.6589, KL-Loss   40.3000, KL-Weight  0.069\n",
      "TRAIN Batch 0550/960, Loss   77.0860, NLL-Loss   73.8941, KL-Loss   41.0230, KL-Weight  0.078\n",
      "TRAIN Batch 0600/960, Loss   88.1627, NLL-Loss   84.7521, KL-Loss   39.0839, KL-Weight  0.087\n",
      "TRAIN Batch 0650/960, Loss   82.1827, NLL-Loss   78.5513, KL-Loss   37.1507, KL-Weight  0.098\n",
      "TRAIN Batch 0700/960, Loss   85.6416, NLL-Loss   82.0091, KL-Loss   33.2215, KL-Weight  0.109\n",
      "TRAIN Batch 0750/960, Loss   80.7025, NLL-Loss   76.6292, KL-Loss   33.3541, KL-Weight  0.122\n",
      "TRAIN Batch 0800/960, Loss   79.6190, NLL-Loss   75.3257, KL-Loss   31.5297, KL-Weight  0.136\n",
      "TRAIN Batch 0850/960, Loss   77.4595, NLL-Loss   72.8153, KL-Loss   30.6448, KL-Weight  0.152\n",
      "TRAIN Batch 0900/960, Loss   82.8528, NLL-Loss   78.0642, KL-Loss   28.4470, KL-Weight  0.168\n",
      "TRAIN Batch 0950/960, Loss   86.3871, NLL-Loss   81.4106, KL-Loss   26.6747, KL-Weight  0.187\n",
      "TRAIN Batch 0960/960, Loss  101.1921, NLL-Loss   96.0212, KL-Loss   27.1598, KL-Weight  0.190\n",
      "TRAIN Epoch 01/10, Mean ELBO   79.0617\n",
      "Model saved at bin/2019-Nov-29-05:41:12/E1.pytorch\n",
      "VALID Batch 0000/240, Loss   77.5319, NLL-Loss   72.1079, KL-Loss   28.4317, KL-Weight  0.191\n",
      "VALID Batch 0050/240, Loss   79.9012, NLL-Loss   74.7237, KL-Loss   27.1395, KL-Weight  0.191\n",
      "VALID Batch 0100/240, Loss   75.6633, NLL-Loss   70.8258, KL-Loss   25.3576, KL-Weight  0.191\n",
      "VALID Batch 0150/240, Loss   83.7823, NLL-Loss   78.4991, KL-Loss   27.6938, KL-Weight  0.191\n",
      "VALID Batch 0200/240, Loss   73.1500, NLL-Loss   67.9257, KL-Loss   27.3852, KL-Weight  0.191\n",
      "VALID Batch 0240/240, Loss   49.7935, NLL-Loss   45.3680, KL-Loss   23.1977, KL-Weight  0.191\n",
      "VALID Epoch 01/10, Mean ELBO   78.7006\n",
      "TRAIN Batch 0000/960, Loss   73.8395, NLL-Loss   68.5183, KL-Loss   27.8930, KL-Weight  0.191\n",
      "TRAIN Batch 0050/960, Loss   74.4148, NLL-Loss   68.6728, KL-Loss   27.2368, KL-Weight  0.211\n",
      "TRAIN Batch 0100/960, Loss   66.7968, NLL-Loss   60.7621, KL-Loss   25.9708, KL-Weight  0.232\n",
      "TRAIN Batch 0150/960, Loss   71.1373, NLL-Loss   65.0800, KL-Loss   23.7169, KL-Weight  0.255\n",
      "TRAIN Batch 0200/960, Loss   79.3131, NLL-Loss   72.6459, KL-Loss   23.8207, KL-Weight  0.280\n",
      "TRAIN Batch 0250/960, Loss   77.9816, NLL-Loss   71.0561, KL-Loss   22.6496, KL-Weight  0.306\n",
      "TRAIN Batch 0300/960, Loss   82.1973, NLL-Loss   74.5203, KL-Loss   23.0593, KL-Weight  0.333\n",
      "TRAIN Batch 0350/960, Loss   83.1855, NLL-Loss   75.7868, KL-Loss   20.4815, KL-Weight  0.361\n",
      "TRAIN Batch 0400/960, Loss   82.1609, NLL-Loss   74.5891, KL-Loss   19.3874, KL-Weight  0.391\n",
      "TRAIN Batch 0450/960, Loss   78.5505, NLL-Loss   70.5630, KL-Loss   18.9873, KL-Weight  0.421\n",
      "TRAIN Batch 0500/960, Loss   77.0449, NLL-Loss   68.8658, KL-Loss   18.1191, KL-Weight  0.451\n",
      "TRAIN Batch 0550/960, Loss   80.1580, NLL-Loss   71.3459, KL-Loss   18.2631, KL-Weight  0.483\n",
      "TRAIN Batch 0600/960, Loss   89.1412, NLL-Loss   80.3305, KL-Loss   17.1499, KL-Weight  0.514\n",
      "TRAIN Batch 0650/960, Loss   92.2704, NLL-Loss   83.0423, KL-Loss   16.9361, KL-Weight  0.545\n",
      "TRAIN Batch 0700/960, Loss   79.1975, NLL-Loss   70.9677, KL-Loss   14.2961, KL-Weight  0.576\n",
      "TRAIN Batch 0750/960, Loss   91.3899, NLL-Loss   82.6003, KL-Loss   14.5074, KL-Weight  0.606\n",
      "TRAIN Batch 0800/960, Loss   80.7929, NLL-Loss   72.4988, KL-Loss   13.0555, KL-Weight  0.635\n",
      "TRAIN Batch 0850/960, Loss   78.9032, NLL-Loss   69.7621, KL-Loss   13.7722, KL-Weight  0.664\n",
      "TRAIN Batch 0900/960, Loss   80.3334, NLL-Loss   70.7909, KL-Loss   13.8088, KL-Weight  0.691\n",
      "TRAIN Batch 0950/960, Loss   91.9464, NLL-Loss   82.2003, KL-Loss   13.5915, KL-Weight  0.717\n",
      "TRAIN Batch 0960/960, Loss   60.1037, NLL-Loss   51.3100, KL-Loss   12.1777, KL-Weight  0.722\n",
      "TRAIN Epoch 02/10, Mean ELBO   80.5291\n",
      "Model saved at bin/2019-Nov-29-05:41:12/E2.pytorch\n",
      "VALID Batch 0000/240, Loss   84.6003, NLL-Loss   75.1723, KL-Loss   13.0470, KL-Weight  0.723\n",
      "VALID Batch 0050/240, Loss   85.6129, NLL-Loss   76.3075, KL-Loss   12.8772, KL-Weight  0.723\n",
      "VALID Batch 0100/240, Loss   82.6973, NLL-Loss   74.1672, KL-Loss   11.8044, KL-Weight  0.723\n",
      "VALID Batch 0150/240, Loss   90.4326, NLL-Loss   81.1380, KL-Loss   12.8622, KL-Weight  0.723\n",
      "VALID Batch 0200/240, Loss   79.6503, NLL-Loss   70.4404, KL-Loss   12.7452, KL-Weight  0.723\n",
      "VALID Batch 0240/240, Loss   55.8185, NLL-Loss   48.1149, KL-Loss   10.6607, KL-Weight  0.723\n",
      "VALID Epoch 02/10, Mean ELBO   85.0434\n",
      "TRAIN Batch 0000/960, Loss   65.0906, NLL-Loss   56.0468, KL-Loss   12.5152, KL-Weight  0.723\n",
      "TRAIN Batch 0050/960, Loss   72.2938, NLL-Loss   62.8942, KL-Loss   12.5836, KL-Weight  0.747\n",
      "TRAIN Batch 0100/960, Loss   77.1446, NLL-Loss   66.9331, KL-Loss   13.2642, KL-Weight  0.770\n",
      "TRAIN Batch 0150/960, Loss   77.7965, NLL-Loss   67.6172, KL-Loss   12.8648, KL-Weight  0.791\n",
      "TRAIN Batch 0200/960, Loss   77.3842, NLL-Loss   68.6177, KL-Loss   10.8074, KL-Weight  0.811\n",
      "TRAIN Batch 0250/960, Loss   88.8577, NLL-Loss   79.1229, KL-Loss   11.7349, KL-Weight  0.830\n",
      "TRAIN Batch 0300/960, Loss   84.2141, NLL-Loss   74.2816, KL-Loss   11.7334, KL-Weight  0.847\n",
      "TRAIN Batch 0350/960, Loss   76.4068, NLL-Loss   66.6867, KL-Loss   11.2753, KL-Weight  0.862\n",
      "TRAIN Batch 0400/960, Loss   85.0773, NLL-Loss   75.5452, KL-Loss   10.8782, KL-Weight  0.876\n",
      "TRAIN Batch 0450/960, Loss   88.6111, NLL-Loss   78.8493, KL-Loss   10.9783, KL-Weight  0.889\n",
      "TRAIN Batch 0500/960, Loss   82.8944, NLL-Loss   73.4897, KL-Loss   10.4390, KL-Weight  0.901\n",
      "TRAIN Batch 0550/960, Loss   77.2129, NLL-Loss   67.9362, KL-Loss   10.1770, KL-Weight  0.912\n",
      "TRAIN Batch 0600/960, Loss   89.9818, NLL-Loss   80.5300, KL-Loss   10.2613, KL-Weight  0.921\n",
      "TRAIN Batch 0650/960, Loss   83.0898, NLL-Loss   73.5249, KL-Loss   10.2879, KL-Weight  0.930\n",
      "TRAIN Batch 0700/960, Loss   84.7896, NLL-Loss   74.9126, KL-Loss   10.5358, KL-Weight  0.937\n",
      "TRAIN Batch 0750/960, Loss   78.6940, NLL-Loss   68.8668, KL-Loss   10.4057, KL-Weight  0.944\n",
      "TRAIN Batch 0800/960, Loss   90.1292, NLL-Loss   80.4959, KL-Loss   10.1337, KL-Weight  0.951\n",
      "TRAIN Batch 0850/960, Loss   82.2890, NLL-Loss   72.9205, KL-Loss    9.7980, KL-Weight  0.956\n",
      "TRAIN Batch 0900/960, Loss   75.8474, NLL-Loss   67.1363, KL-Loss    9.0635, KL-Weight  0.961\n",
      "TRAIN Batch 0950/960, Loss   85.3047, NLL-Loss   75.9372, KL-Loss    9.7020, KL-Weight  0.966\n",
      "TRAIN Batch 0960/960, Loss   88.1172, NLL-Loss   79.5361, KL-Loss    8.8799, KL-Weight  0.966\n",
      "TRAIN Epoch 03/10, Mean ELBO   82.6920\n",
      "Model saved at bin/2019-Nov-29-05:41:12/E3.pytorch\n",
      "VALID Batch 0000/240, Loss   84.5167, NLL-Loss   75.0570, KL-Loss    9.7884, KL-Weight  0.966\n",
      "VALID Batch 0050/240, Loss   87.0044, NLL-Loss   77.6033, KL-Loss    9.7276, KL-Weight  0.966\n",
      "VALID Batch 0100/240, Loss   83.5604, NLL-Loss   75.1702, KL-Loss    8.6816, KL-Weight  0.966\n",
      "VALID Batch 0150/240, Loss   90.4037, NLL-Loss   81.2153, KL-Loss    9.5076, KL-Weight  0.966\n",
      "VALID Batch 0200/240, Loss   80.8574, NLL-Loss   71.8636, KL-Loss    9.3063, KL-Weight  0.966\n",
      "VALID Batch 0240/240, Loss   53.4084, NLL-Loss   45.1715, KL-Loss    8.5231, KL-Weight  0.966\n",
      "VALID Epoch 03/10, Mean ELBO   85.7904\n",
      "TRAIN Batch 0000/960, Loss   83.6025, NLL-Loss   73.9880, KL-Loss    9.9485, KL-Weight  0.966\n",
      "TRAIN Batch 0050/960, Loss   91.8794, NLL-Loss   82.0860, KL-Loss   10.0937, KL-Weight  0.970\n",
      "TRAIN Batch 0100/960, Loss   82.4650, NLL-Loss   72.7109, KL-Loss   10.0180, KL-Weight  0.974\n",
      "TRAIN Batch 0150/960, Loss   78.6087, NLL-Loss   69.2457, KL-Loss    9.5865, KL-Weight  0.977\n",
      "TRAIN Batch 0200/960, Loss   85.3231, NLL-Loss   76.1104, KL-Loss    9.4068, KL-Weight  0.979\n",
      "TRAIN Batch 0250/960, Loss   80.9039, NLL-Loss   71.9844, KL-Loss    9.0853, KL-Weight  0.982\n",
      "TRAIN Batch 0300/960, Loss   82.9776, NLL-Loss   73.9243, KL-Loss    9.2018, KL-Weight  0.984\n",
      "TRAIN Batch 0350/960, Loss   85.8636, NLL-Loss   76.3632, KL-Loss    9.6380, KL-Weight  0.986\n",
      "TRAIN Batch 0400/960, Loss   65.5373, NLL-Loss   57.1214, KL-Loss    8.5235, KL-Weight  0.987\n",
      "TRAIN Batch 0450/960, Loss   75.5058, NLL-Loss   66.5848, KL-Loss    9.0216, KL-Weight  0.989\n",
      "TRAIN Batch 0500/960, Loss   90.9712, NLL-Loss   81.0843, KL-Loss    9.9852, KL-Weight  0.990\n",
      "TRAIN Batch 0550/960, Loss   85.6594, NLL-Loss   76.3182, KL-Loss    9.4232, KL-Weight  0.991\n",
      "TRAIN Batch 0600/960, Loss   80.3058, NLL-Loss   71.3071, KL-Loss    9.0684, KL-Weight  0.992\n",
      "TRAIN Batch 0650/960, Loss   81.6486, NLL-Loss   73.0426, KL-Loss    8.6648, KL-Weight  0.993\n",
      "TRAIN Batch 0700/960, Loss   92.5918, NLL-Loss   83.1032, KL-Loss    9.5459, KL-Weight  0.994\n",
      "TRAIN Batch 0750/960, Loss   76.8670, NLL-Loss   67.8210, KL-Loss    9.0942, KL-Weight  0.995\n",
      "TRAIN Batch 0800/960, Loss   79.8346, NLL-Loss   70.6731, KL-Loss    9.2046, KL-Weight  0.995\n",
      "TRAIN Batch 0850/960, Loss   72.4918, NLL-Loss   63.8330, KL-Loss    8.6947, KL-Weight  0.996\n",
      "TRAIN Batch 0900/960, Loss   88.1800, NLL-Loss   79.2869, KL-Loss    8.9257, KL-Weight  0.996\n",
      "TRAIN Batch 0950/960, Loss   81.4318, NLL-Loss   72.4912, KL-Loss    8.9695, KL-Weight  0.997\n",
      "TRAIN Batch 0960/960, Loss   73.6325, NLL-Loss   65.7442, KL-Loss    7.9132, KL-Weight  0.997\n",
      "TRAIN Epoch 04/10, Mean ELBO   80.8971\n",
      "Model saved at bin/2019-Nov-29-05:41:12/E4.pytorch\n",
      "VALID Batch 0000/240, Loss   83.2238, NLL-Loss   73.9336, KL-Loss    9.3194, KL-Weight  0.997\n",
      "VALID Batch 0050/240, Loss   85.8009, NLL-Loss   76.6104, KL-Loss    9.2193, KL-Weight  0.997\n",
      "VALID Batch 0100/240, Loss   81.9351, NLL-Loss   73.8095, KL-Loss    8.1512, KL-Weight  0.997\n",
      "VALID Batch 0150/240, Loss   91.0041, NLL-Loss   81.7687, KL-Loss    9.2644, KL-Weight  0.997\n",
      "VALID Batch 0200/240, Loss   81.5641, NLL-Loss   72.7245, KL-Loss    8.8675, KL-Weight  0.997\n",
      "VALID Batch 0240/240, Loss   51.2921, NLL-Loss   43.2096, KL-Loss    8.1079, KL-Weight  0.997\n",
      "VALID Epoch 04/10, Mean ELBO   85.2282\n",
      "TRAIN Batch 0000/960, Loss   77.1739, NLL-Loss   68.3094, KL-Loss    8.8923, KL-Weight  0.997\n",
      "TRAIN Batch 0050/960, Loss   76.9035, NLL-Loss   67.5092, KL-Loss    9.4203, KL-Weight  0.997\n",
      "TRAIN Batch 0100/960, Loss   87.0683, NLL-Loss   77.6961, KL-Loss    9.3951, KL-Weight  0.998\n",
      "TRAIN Batch 0150/960, Loss   68.9935, NLL-Loss   59.8262, KL-Loss    9.1871, KL-Weight  0.998\n",
      "TRAIN Batch 0200/960, Loss   77.0639, NLL-Loss   68.4278, KL-Loss    8.6526, KL-Weight  0.998\n",
      "TRAIN Batch 0250/960, Loss   74.9191, NLL-Loss   65.9494, KL-Loss    8.9848, KL-Weight  0.998\n",
      "TRAIN Batch 0300/960, Loss   78.9339, NLL-Loss   70.0116, KL-Loss    8.9356, KL-Weight  0.999\n",
      "TRAIN Batch 0350/960, Loss   82.7413, NLL-Loss   73.0633, KL-Loss    9.6907, KL-Weight  0.999\n",
      "TRAIN Batch 0400/960, Loss   83.3194, NLL-Loss   73.9081, KL-Loss    9.4222, KL-Weight  0.999\n",
      "TRAIN Batch 0450/960, Loss   69.4637, NLL-Loss   60.7864, KL-Loss    8.6861, KL-Weight  0.999\n",
      "TRAIN Batch 0500/960, Loss   72.8545, NLL-Loss   63.6103, KL-Loss    9.2525, KL-Weight  0.999\n",
      "TRAIN Batch 0550/960, Loss   72.2426, NLL-Loss   63.7493, KL-Loss    8.5001, KL-Weight  0.999\n",
      "TRAIN Batch 0600/960, Loss   85.2138, NLL-Loss   75.3634, KL-Loss    9.8572, KL-Weight  0.999\n",
      "TRAIN Batch 0650/960, Loss   80.8094, NLL-Loss   72.5508, KL-Loss    8.2636, KL-Weight  0.999\n",
      "TRAIN Batch 0700/960, Loss   73.3705, NLL-Loss   64.8031, KL-Loss    8.5721, KL-Weight  0.999\n",
      "TRAIN Batch 0750/960, Loss   86.6358, NLL-Loss   77.5273, KL-Loss    9.1129, KL-Weight  1.000\n",
      "TRAIN Batch 0800/960, Loss   78.0871, NLL-Loss   69.5952, KL-Loss    8.4955, KL-Weight  1.000\n",
      "TRAIN Batch 0850/960, Loss   76.7757, NLL-Loss   68.3122, KL-Loss    8.4667, KL-Weight  1.000\n",
      "TRAIN Batch 0900/960, Loss   80.6848, NLL-Loss   72.0258, KL-Loss    8.6618, KL-Weight  1.000\n",
      "TRAIN Batch 0950/960, Loss   87.3343, NLL-Loss   77.9211, KL-Loss    9.4160, KL-Weight  1.000\n",
      "TRAIN Batch 0960/960, Loss   94.8581, NLL-Loss   85.7007, KL-Loss    9.1600, KL-Weight  1.000\n",
      "TRAIN Epoch 05/10, Mean ELBO   78.7563\n",
      "Model saved at bin/2019-Nov-29-05:41:12/E5.pytorch\n",
      "VALID Batch 0000/240, Loss   83.4980, NLL-Loss   74.6368, KL-Loss    8.8637, KL-Weight  1.000\n",
      "VALID Batch 0050/240, Loss   84.5681, NLL-Loss   75.7170, KL-Loss    8.8536, KL-Weight  1.000\n",
      "VALID Batch 0100/240, Loss   80.9028, NLL-Loss   73.0675, KL-Loss    7.8376, KL-Weight  1.000\n",
      "VALID Batch 0150/240, Loss   88.9198, NLL-Loss   80.2103, KL-Loss    8.7120, KL-Weight  1.000\n",
      "VALID Batch 0200/240, Loss   80.6802, NLL-Loss   72.3479, KL-Loss    8.3347, KL-Weight  1.000\n",
      "VALID Batch 0240/240, Loss   54.9682, NLL-Loss   47.9610, KL-Loss    7.0092, KL-Weight  1.000\n",
      "VALID Epoch 05/10, Mean ELBO   84.5797\n",
      "TRAIN Batch 0000/960, Loss   80.9022, NLL-Loss   71.6227, KL-Loss    9.2821, KL-Weight  1.000\n",
      "TRAIN Batch 0050/960, Loss   76.9197, NLL-Loss   67.8437, KL-Loss    9.0783, KL-Weight  1.000\n",
      "TRAIN Batch 0100/960, Loss   76.4658, NLL-Loss   67.4357, KL-Loss    9.0321, KL-Weight  1.000\n",
      "TRAIN Batch 0150/960, Loss   71.1953, NLL-Loss   62.5293, KL-Loss    8.6678, KL-Weight  1.000\n",
      "TRAIN Batch 0200/960, Loss   64.0406, NLL-Loss   55.5363, KL-Loss    8.5057, KL-Weight  1.000\n",
      "TRAIN Batch 0250/960, Loss   72.5626, NLL-Loss   64.3073, KL-Loss    8.2566, KL-Weight  1.000\n",
      "TRAIN Batch 0300/960, Loss   74.4102, NLL-Loss   65.7649, KL-Loss    8.6465, KL-Weight  1.000\n",
      "TRAIN Batch 0350/960, Loss   75.8641, NLL-Loss   66.8297, KL-Loss    9.0355, KL-Weight  1.000\n",
      "TRAIN Batch 0400/960, Loss   77.6462, NLL-Loss   68.6757, KL-Loss    8.9714, KL-Weight  1.000\n",
      "TRAIN Batch 0450/960, Loss   81.9980, NLL-Loss   72.9620, KL-Loss    9.0368, KL-Weight  1.000\n",
      "TRAIN Batch 0500/960, Loss   77.2051, NLL-Loss   68.5180, KL-Loss    8.6877, KL-Weight  1.000\n",
      "TRAIN Batch 0550/960, Loss   78.7925, NLL-Loss   70.3563, KL-Loss    8.4368, KL-Weight  1.000\n",
      "TRAIN Batch 0600/960, Loss   68.3015, NLL-Loss   59.8953, KL-Loss    8.4067, KL-Weight  1.000\n",
      "TRAIN Batch 0650/960, Loss   75.2353, NLL-Loss   66.4874, KL-Loss    8.7484, KL-Weight  1.000\n",
      "TRAIN Batch 0700/960, Loss   72.9326, NLL-Loss   64.7621, KL-Loss    8.1710, KL-Weight  1.000\n",
      "TRAIN Batch 0750/960, Loss   83.6922, NLL-Loss   74.5814, KL-Loss    9.1112, KL-Weight  1.000\n",
      "TRAIN Batch 0800/960, Loss   80.7028, NLL-Loss   71.0313, KL-Loss    9.6719, KL-Weight  1.000\n",
      "TRAIN Batch 0850/960, Loss   73.6466, NLL-Loss   64.6148, KL-Loss    9.0321, KL-Weight  1.000\n",
      "TRAIN Batch 0900/960, Loss   69.6611, NLL-Loss   61.2699, KL-Loss    8.3915, KL-Weight  1.000\n",
      "TRAIN Batch 0950/960, Loss   84.0612, NLL-Loss   74.5231, KL-Loss    9.5384, KL-Weight  1.000\n",
      "TRAIN Batch 0960/960, Loss   69.6986, NLL-Loss   61.3290, KL-Loss    8.3698, KL-Weight  1.000\n",
      "TRAIN Epoch 06/10, Mean ELBO   76.7829\n",
      "Model saved at bin/2019-Nov-29-05:41:12/E6.pytorch\n",
      "VALID Batch 0000/240, Loss   81.5976, NLL-Loss   72.9239, KL-Loss    8.6740, KL-Weight  1.000\n",
      "VALID Batch 0050/240, Loss   85.1008, NLL-Loss   76.1635, KL-Loss    8.9376, KL-Weight  1.000\n",
      "VALID Batch 0100/240, Loss   80.2132, NLL-Loss   72.3981, KL-Loss    7.8153, KL-Weight  1.000\n",
      "VALID Batch 0150/240, Loss   89.1209, NLL-Loss   80.4029, KL-Loss    8.7182, KL-Weight  1.000\n",
      "VALID Batch 0200/240, Loss   78.9884, NLL-Loss   70.9086, KL-Loss    8.0800, KL-Weight  1.000\n",
      "VALID Batch 0240/240, Loss   54.6741, NLL-Loss   47.8321, KL-Loss    6.8422, KL-Weight  1.000\n",
      "VALID Epoch 06/10, Mean ELBO   84.3210\n",
      "TRAIN Batch 0000/960, Loss   73.8046, NLL-Loss   64.7102, KL-Loss    9.0947, KL-Weight  1.000\n",
      "TRAIN Batch 0050/960, Loss   72.7960, NLL-Loss   63.7010, KL-Loss    9.0952, KL-Weight  1.000\n",
      "TRAIN Batch 0100/960, Loss   76.2653, NLL-Loss   66.9574, KL-Loss    9.3080, KL-Weight  1.000\n",
      "TRAIN Batch 0150/960, Loss   72.6904, NLL-Loss   63.5581, KL-Loss    9.1324, KL-Weight  1.000\n",
      "TRAIN Batch 0200/960, Loss   70.1030, NLL-Loss   61.7188, KL-Loss    8.3844, KL-Weight  1.000\n",
      "TRAIN Batch 0250/960, Loss   77.3126, NLL-Loss   68.2362, KL-Loss    9.0765, KL-Weight  1.000\n",
      "TRAIN Batch 0300/960, Loss   79.9513, NLL-Loss   70.7391, KL-Loss    9.2124, KL-Weight  1.000\n",
      "TRAIN Batch 0350/960, Loss   67.1652, NLL-Loss   57.6387, KL-Loss    9.5266, KL-Weight  1.000\n",
      "TRAIN Batch 0400/960, Loss   81.1993, NLL-Loss   72.6079, KL-Loss    8.5915, KL-Weight  1.000\n",
      "TRAIN Batch 0450/960, Loss   69.6134, NLL-Loss   60.9329, KL-Loss    8.6806, KL-Weight  1.000\n",
      "TRAIN Batch 0500/960, Loss   75.9378, NLL-Loss   66.9712, KL-Loss    8.9667, KL-Weight  1.000\n"
     ]
    }
   ],
   "source": [
    "%pdb on\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    for split in splits:\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            dataset=_datasets[split],\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=split=='train',\n",
    "            num_workers=cpu_count(),\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "\n",
    "        tracker = defaultdict(tensor)\n",
    "\n",
    "        # Enable/Disable Dropout\n",
    "        if split == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        for iteration, batch in enumerate(data_loader):\n",
    "            \n",
    "            batch_size = batch['input'].size(0)\n",
    "            \n",
    "            for k, v in batch.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    batch[k] = to_var(v)\n",
    "            \n",
    "            # loss calculation\n",
    "            logp, mean, logv, z = model(batch['input'], batch['length'])\n",
    "            loss_dict = model.loss(logp, batch['target'], batch['length'], mean, logv, args.anneal_function, step, args.k, args.x0)\n",
    "            loss, NLL_loss, KL_weight, KL_loss, avg_bow_loss = loss_dict['loss'], loss_dict['NLL_loss'], loss_dict['KL_weight'], loss_dict['KL_loss'], loss_dict.get('avg_bow_loss')\n",
    "\n",
    "            # backward + optimization\n",
    "            if split == 'train':\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                step += 1\n",
    "\n",
    "            # bookkeepeing\n",
    "            tracker['ELBO'] = torch.cat((tracker['ELBO'], loss.data.view(1)))\n",
    "\n",
    "            if args.tensorboard_logging:\n",
    "                writer.add_scalar(\"%s/ELBO\"%split.upper(), loss.data.item(), epoch*len(data_loader) + iteration)\n",
    "                writer.add_scalar(\"%s/NLL Loss\"%split.upper(), NLL_loss.data.item()/batch_size, epoch*len(data_loader) + iteration)\n",
    "                writer.add_scalar(\"%s/KL Loss\"%split.upper(), KL_loss.data.item()/batch_size, epoch*len(data_loader) + iteration)\n",
    "                writer.add_scalar(\"%s/KL Weight\"%split.upper(), KL_weight, epoch*len(data_loader) + iteration)\n",
    "                if avg_bow_loss is not None:\n",
    "                    writer.add_scalar(\"%s/BOW Loss\"%split.upper(), avg_bow_loss, epoch*len(data_loader) + iteration)\n",
    "\n",
    "            if iteration % args.print_every == 0 or iteration+1 == len(data_loader):\n",
    "                print_text = \"%s Batch %04d/%i, Loss %9.4f, NLL-Loss %9.4f, KL-Loss %9.4f, KL-Weight %6.3f\"%(split.upper(), iteration, len(data_loader)-1, loss.data.item(), NLL_loss.data.item()/batch_size, KL_loss.data.item()/batch_size, KL_weight)\n",
    "                if avg_bow_loss is not None:\n",
    "                    print_text += ', BOW Loss %9.4f,'%(avg_bow_loss)\n",
    "                print(print_text)\n",
    "\n",
    "            if split == 'valid':\n",
    "                if 'target_sents' not in tracker:\n",
    "                    tracker['target_sents'] = list()\n",
    "                tracker['target_sents'] += idx2word(batch['target'].data, i2w=_datasets['train'].get_i2w(), pad_idx=PAD_INDEX)\n",
    "                tracker['z'] = torch.cat((tracker['z'], z.data), dim=0)\n",
    "\n",
    "        print(\"%s Epoch %02d/%i, Mean ELBO %9.4f\"%(split.upper(), epoch, args.epochs, torch.mean(tracker['ELBO'])))\n",
    "\n",
    "        if args.tensorboard_logging:\n",
    "            writer.add_scalar(\"%s-Epoch/ELBO\"%split.upper(), torch.mean(tracker['ELBO']), epoch)\n",
    "\n",
    "        # save a dump of all sentences and the encoded latent space\n",
    "        if split == 'valid':\n",
    "            dump = {'target_sents':tracker['target_sents'], 'z':tracker['z'].tolist()}\n",
    "            if not os.path.exists(os.path.join('dumps', ts)):\n",
    "                os.makedirs('dumps/'+ts)\n",
    "            with open(os.path.join('dumps/'+ts+'/valid_E%i.json'%epoch), 'w') as dump_file:\n",
    "                json.dump(dump,dump_file)\n",
    "\n",
    "        # save checkpoint\n",
    "        if split == 'train':\n",
    "            checkpoint_path = os.path.join(save_model_path, \"E%i.pytorch\"%(epoch))\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(\"Model saved at %s\"%checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
