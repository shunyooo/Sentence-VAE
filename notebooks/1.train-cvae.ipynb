{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from multiprocessing import cpu_count\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "from ptb import PTB\n",
    "from utils import to_var, idx2word, expierment_name, AttributeDict\n",
    "from model_cvae import SentenceVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AttrDict{'data_dir': 'data/eccos', 'create_data': False, 'max_sequence_length': 50, 'min_occ': 1, 'test': False, 'epochs': 10, 'batch_size': 32, 'learning_rate': 0.001, 'embedding_size': 300, 'rnn_type': 'gru', 'hidden_size': 256, 'num_layers': 1, 'bidirectional': False, 'latent_size': 200, 'word_dropout': 0, 'embedding_dropout': 0.5, 'anneal_function': 'logistic', 'k': 0.0025, 'x0': 2500, 'print_every': 50, 'tensorboard_logging': True, 'logdir': '/root/user/work/logs/', 'save_model_path': 'bin', 'expierment_name': 'kw2copy_cvae_latent200'}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_dir = 'data/simple-examples'\n",
    "data_dir = 'data/eccos'\n",
    "\n",
    "args = {\n",
    "    'data_dir': data_dir,\n",
    "    'create_data': False,\n",
    "    'max_sequence_length': 50,\n",
    "    'min_occ': 1,\n",
    "    'test': False,\n",
    "\n",
    "    'epochs': 10,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "\n",
    "    'embedding_size': 300,\n",
    "    'rnn_type': 'gru',\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 1,\n",
    "    'bidirectional': False,\n",
    "    'latent_size': 200,\n",
    "    'word_dropout': 0,\n",
    "    'embedding_dropout': 0.5,\n",
    "\n",
    "    'anneal_function': 'logistic',\n",
    "    'k': 0.0025,\n",
    "    'x0': 2500,\n",
    "\n",
    "    'print_every': 50,\n",
    "    'tensorboard_logging': True,\n",
    "    'logdir': '/root/user/work/logs/',\n",
    "    'save_model_path': 'bin',\n",
    "    'expierment_name': 'kw2copy_cvae_latent200', \n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "args = AttributeDict(args)\n",
    "\n",
    "args.rnn_type = args.rnn_type.lower()\n",
    "args.anneal_function = args.anneal_function.lower()\n",
    "\n",
    "assert args.rnn_type in ['rnn', 'lstm', 'gru']\n",
    "assert args.anneal_function in ['logistic', 'linear']\n",
    "assert 0 <= args.word_dropout <= 1\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data/eccos\n",
      "('train', 'src')\n",
      "vocab: 5619, records: 30726\n",
      "('train', 'tgt')\n",
      "vocab: 12106, records: 30726\n",
      "('valid', 'src')\n",
      "vocab: 5619, records: 7682\n",
      "('valid', 'tgt')\n",
      "vocab: 12106, records: 7682\n",
      "CPU times: user 1 s, sys: 89.7 ms, total: 1.09 s\n",
      "Wall time: 1.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import itertools\n",
    "splits = ['train', 'valid'] + (['test'] if args.test else [])\n",
    "datasets = OrderedDict()\n",
    "print(f'loading {args.data_dir}')\n",
    "for split, src_tgt in itertools.product(splits, ['src', 'tgt']):\n",
    "    key = (split, src_tgt)\n",
    "    print(key)\n",
    "    datasets[key] = PTB(\n",
    "        data_dir=f'{args.data_dir}/{src_tgt}',\n",
    "        split=split,\n",
    "        create_data=args.create_data,\n",
    "        max_sequence_length=args.max_sequence_length if src_tgt == 'tgt' else args.max_sequence_length_src,\n",
    "        min_occ=args.min_occ\n",
    "    )\n",
    "    print(f'vocab: {datasets[key].vocab_size}, records: {len(datasets[key].data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "■ src-input \n",
      "<sos> bbクリーム 進化 クリーム\n",
      "■ src-target \n",
      "bbクリーム 進化 クリーム <eos>\n",
      "■ tgt-input\n",
      "<sos> bbクリーム の 進化 版 ? cc クリーム が 気 に なる ... ! <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "■ tgt-target\n",
      "bbクリーム の 進化 版 ? cc クリーム が 気 に なる ... ! <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "# 実際のデータ確認\n",
    "def ids2text(id_list, ptb):\n",
    "    return ' '.join([ptb.i2w[f'{i}'] for i in id_list])\n",
    "\n",
    "_ptb_src = datasets[('train', 'src')]\n",
    "_ptb_tgt = datasets[('train', 'tgt')]\n",
    "index = str(101)\n",
    "_sample_src, _sample_tgt = _ptb_src.data[index], _ptb_tgt[index]\n",
    "print(f'■ src-input \\n{ids2text(_sample_src[\"input\"], _ptb_src)}')\n",
    "print(f'■ src-target \\n{ids2text(_sample_src[\"target\"], _ptb_src)}')\n",
    "print(f'■ tgt-input\\n{ids2text(_sample_tgt[\"input\"], _ptb_tgt)}')\n",
    "print(f'■ tgt-target\\n{ids2text(_sample_tgt[\"target\"], _ptb_tgt)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptb import SOS_INDEX, EOS_INDEX, PAD_INDEX, UNK_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "model = SentenceVAE(\n",
    "    vocab_size=datasets[('train', 'tgt')].vocab_size,\n",
    "    sos_idx=SOS_INDEX,\n",
    "    eos_idx=EOS_INDEX,\n",
    "    pad_idx=PAD_INDEX,\n",
    "    unk_idx=UNK_INDEX,\n",
    "    max_sequence_length=args.max_sequence_length,\n",
    "    embedding_size=args.embedding_size,\n",
    "    rnn_type=args.rnn_type,\n",
    "    hidden_size=args.hidden_size,\n",
    "    word_dropout=args.word_dropout,\n",
    "    embedding_dropout=args.embedding_dropout,\n",
    "    latent_size=args.latent_size,\n",
    "    num_layers=args.num_layers,\n",
    "    bidirectional=args.bidirectional,\n",
    "    \n",
    "    # bow loss\n",
    "    # bow_hidden_size=256,\n",
    "    use_bow_loss=False,\n",
    "    \n",
    "    # kw base\n",
    "    # out_vocab_size=datasets[('train', 'tgt')].vocab_size,\n",
    "    \n",
    "    # condional\n",
    "    cond_vocab_size=datasets[('train', 'src')].vocab_size, \n",
    "    cond_embedding_size=args.embedding_size,\n",
    "    cond_hidden_size=256,\n",
    "    )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceVAE(\n",
       "  (embedding): Embedding(12106, 300)\n",
       "  (embedding_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (decoder_embedding): Embedding(12106, 300)\n",
       "  (encoder_rnn): GRU(300, 512, batch_first=True)\n",
       "  (decoder_rnn): GRU(300, 512, batch_first=True)\n",
       "  (cond_embedding): Embedding(5619, 300)\n",
       "  (cond_encoder_rnn): GRU(300, 256, batch_first=True)\n",
       "  (cond_hidden2mean): Linear(in_features=256, out_features=200, bias=True)\n",
       "  (cond_hidden2logv): Linear(in_features=256, out_features=200, bias=True)\n",
       "  (hidden2mean): Linear(in_features=768, out_features=200, bias=True)\n",
       "  (hidden2logv): Linear(in_features=768, out_features=200, bias=True)\n",
       "  (latent2hidden): Linear(in_features=456, out_features=512, bias=True)\n",
       "  (outputs2vocab): Linear(in_features=512, out_features=12106, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard logging: True\n"
     ]
    }
   ],
   "source": [
    "print(f'tensorboard logging: {args.tensorboard_logging}')\n",
    "ts = time.strftime('%Y-%b-%d-%H:%M:%S', time.gmtime())\n",
    "if args.tensorboard_logging:\n",
    "    writer = SummaryWriter(os.path.join(args.logdir, expierment_name(args,ts)))\n",
    "    writer.add_text(\"model\", str(model))\n",
    "    writer.add_text(\"args\", str(args))\n",
    "    writer.add_text(\"ts\", ts)\n",
    "    \n",
    "save_model_path = os.path.join(args.save_model_path, ts)\n",
    "os.makedirs(save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys([('train', 'src'), ('train', 'tgt'), ('valid', 'src'), ('valid', 'tgt')])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> 大人気 <unk> ピンク ♡ コンビニ 買える 「 さくら リップ 」 に 限定 色 が 登場 <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "大人気 <unk> ピンク ♡ コンビニ 買える 「 さくら リップ 」 に 限定 色 が 登場 <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "ae_datasets = {split: dataset for (split, src_tgt), dataset in datasets.items() if src_tgt == 'tgt'}\n",
    "print(ids2text(ae_datasets['train'][0]['input'], ae_datasets['train']))\n",
    "print(ids2text(ae_datasets['train'][0]['target'], ae_datasets['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_datasets = {}\n",
    "for split in splits:\n",
    "    src_dataset = datasets[(split, 'src')]\n",
    "    tgt_dataset = datasets[(split, 'tgt')]\n",
    "    assert len(src_dataset) == len(tgt_dataset)\n",
    "    dataset = []\n",
    "    for i in range(len(src_dataset)):\n",
    "        src_set, tgt_set = src_dataset[i], tgt_dataset[i]\n",
    "        _data = {}\n",
    "        _data.update({f'src_{k}': v for k,v in src_set.items()})\n",
    "        _data.update({f'tgt_{k}': v for k,v in tgt_set.items()})\n",
    "        dataset.append(_data)\n",
    "    _datasets[split] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': array([  2, 854, 754, 869]),\n",
       " 'target': array([854, 754, 869,   3]),\n",
       " 'length': 4}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'valid'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_input': array([2, 4, 5, 6]),\n",
       " 'src_target': array([4, 5, 6, 3]),\n",
       " 'src_length': 4,\n",
       " 'tgt_input': array([ 2,  4,  1,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'tgt_target': array([ 4,  1,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,  3,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'tgt_length': 16}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ptb.PTB at 0x7fae397f4cf8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target_ptb = datasets[('train', 'tgt')]\n",
    "train_target_ptb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n",
      "TRAIN Batch 0000/960, Loss  174.7132, NLL-Loss  174.7128, KL-Loss    0.2473, KL-Weight  0.002\n",
      "TRAIN Batch 0050/960, Loss  131.1542, NLL-Loss  131.1520, KL-Loss    1.0226, KL-Weight  0.002\n",
      "TRAIN Batch 0100/960, Loss  111.2287, NLL-Loss  111.2169, KL-Loss    4.7605, KL-Weight  0.002\n",
      "TRAIN Batch 0150/960, Loss  100.2683, NLL-Loss  100.2468, KL-Loss    7.6762, KL-Weight  0.003\n",
      "TRAIN Batch 0200/960, Loss   97.2754, NLL-Loss   97.2433, KL-Loss   10.1182, KL-Weight  0.003\n",
      "TRAIN Batch 0250/960, Loss   88.8983, NLL-Loss   88.8567, KL-Loss   11.5986, KL-Weight  0.004\n",
      "TRAIN Batch 0300/960, Loss   82.9461, NLL-Loss   82.8949, KL-Loss   12.5753, KL-Weight  0.004\n",
      "TRAIN Batch 0350/960, Loss   99.1914, NLL-Loss   99.1268, KL-Loss   14.0035, KL-Weight  0.005\n",
      "TRAIN Batch 0400/960, Loss   96.7049, NLL-Loss   96.6283, KL-Loss   14.6729, KL-Weight  0.005\n",
      "TRAIN Batch 0450/960, Loss   84.5069, NLL-Loss   84.4179, KL-Loss   15.0522, KL-Weight  0.006\n",
      "TRAIN Batch 0500/960, Loss   67.2726, NLL-Loss   67.1732, KL-Loss   14.8473, KL-Weight  0.007\n",
      "TRAIN Batch 0550/960, Loss   79.2376, NLL-Loss   79.1198, KL-Loss   15.5392, KL-Weight  0.008\n",
      "TRAIN Batch 0600/960, Loss   63.2721, NLL-Loss   63.1360, KL-Loss   15.8698, KL-Weight  0.009\n",
      "TRAIN Batch 0650/960, Loss   71.2820, NLL-Loss   71.1253, KL-Loss   16.1470, KL-Weight  0.010\n",
      "TRAIN Batch 0700/960, Loss   79.8551, NLL-Loss   79.6710, KL-Loss   16.7641, KL-Weight  0.011\n",
      "TRAIN Batch 0750/960, Loss   69.5119, NLL-Loss   69.3109, KL-Loss   16.1704, KL-Weight  0.012\n",
      "TRAIN Batch 0800/960, Loss   70.1504, NLL-Loss   69.9260, KL-Loss   15.9511, KL-Weight  0.014\n",
      "TRAIN Batch 0850/960, Loss   81.4188, NLL-Loss   81.1529, KL-Loss   16.7113, KL-Weight  0.016\n",
      "TRAIN Batch 0900/960, Loss   82.6293, NLL-Loss   82.3402, KL-Loss   16.0733, KL-Weight  0.018\n",
      "TRAIN Batch 0950/960, Loss   62.4144, NLL-Loss   62.0982, KL-Loss   15.5519, KL-Weight  0.020\n",
      "TRAIN Batch 0960/960, Loss   65.4499, NLL-Loss   63.7287, KL-Loss   82.6034, KL-Weight  0.021\n",
      "TRAIN Epoch 00/10, Mean ELBO   84.8335\n",
      "Model saved at bin/2019-Dec-01-10:27:08/E0.pytorch\n",
      "VALID Batch 0000/240, Loss   62.5866, NLL-Loss   62.2738, KL-Loss   14.9741, KL-Weight  0.021\n",
      "VALID Batch 0050/240, Loss   62.1343, NLL-Loss   61.8180, KL-Loss   15.1404, KL-Weight  0.021\n",
      "VALID Batch 0100/240, Loss   60.9125, NLL-Loss   60.5987, KL-Loss   15.0217, KL-Weight  0.021\n",
      "VALID Batch 0150/240, Loss   67.5732, NLL-Loss   67.2510, KL-Loss   15.4226, KL-Weight  0.021\n",
      "VALID Batch 0200/240, Loss   60.4651, NLL-Loss   60.1435, KL-Loss   15.3942, KL-Weight  0.021\n",
      "VALID Batch 0240/240, Loss   34.5097, NLL-Loss   29.7987, KL-Loss  225.5413, KL-Weight  0.021\n",
      "VALID Epoch 00/10, Mean ELBO   63.1730\n",
      "TRAIN Batch 0000/960, Loss   56.3242, NLL-Loss   56.0041, KL-Loss   15.3256, KL-Weight  0.021\n",
      "TRAIN Batch 0050/960, Loss   53.6826, NLL-Loss   53.3123, KL-Loss   15.6920, KL-Weight  0.024\n",
      "TRAIN Batch 0100/960, Loss   57.5553, NLL-Loss   57.1411, KL-Loss   15.5331, KL-Weight  0.027\n",
      "TRAIN Batch 0150/960, Loss   63.7123, NLL-Loss   63.2535, KL-Loss   15.2414, KL-Weight  0.030\n",
      "TRAIN Batch 0200/960, Loss   51.5260, NLL-Loss   51.0317, KL-Loss   14.5476, KL-Weight  0.034\n",
      "TRAIN Batch 0250/960, Loss   52.3307, NLL-Loss   51.7960, KL-Loss   13.9510, KL-Weight  0.038\n",
      "TRAIN Batch 0300/960, Loss   46.8098, NLL-Loss   46.2130, KL-Loss   13.8120, KL-Weight  0.043\n",
      "TRAIN Batch 0350/960, Loss   46.7222, NLL-Loss   46.0631, KL-Loss   13.5377, KL-Weight  0.049\n",
      "TRAIN Batch 0400/960, Loss   55.8758, NLL-Loss   55.1473, KL-Loss   13.2913, KL-Weight  0.055\n",
      "TRAIN Batch 0450/960, Loss   52.4622, NLL-Loss   51.6343, KL-Loss   13.4267, KL-Weight  0.062\n",
      "TRAIN Batch 0500/960, Loss   56.4536, NLL-Loss   55.5333, KL-Loss   13.2804, KL-Weight  0.069\n",
      "TRAIN Batch 0550/960, Loss   48.1211, NLL-Loss   47.1855, KL-Loss   12.0239, KL-Weight  0.078\n",
      "TRAIN Batch 0600/960, Loss   51.1326, NLL-Loss   50.1025, KL-Loss   11.8047, KL-Weight  0.087\n",
      "TRAIN Batch 0650/960, Loss   53.9255, NLL-Loss   52.7915, KL-Loss   11.6009, KL-Weight  0.098\n",
      "TRAIN Batch 0700/960, Loss   44.0344, NLL-Loss   42.8043, KL-Loss   11.2498, KL-Weight  0.109\n",
      "TRAIN Batch 0750/960, Loss   49.5545, NLL-Loss   48.2242, KL-Loss   10.8936, KL-Weight  0.122\n",
      "TRAIN Batch 0800/960, Loss   51.4682, NLL-Loss   49.9793, KL-Loss   10.9344, KL-Weight  0.136\n",
      "TRAIN Batch 0850/960, Loss   49.9047, NLL-Loss   48.3506, KL-Loss   10.2547, KL-Weight  0.152\n",
      "TRAIN Batch 0900/960, Loss   57.2868, NLL-Loss   55.6158, KL-Loss    9.9268, KL-Weight  0.168\n",
      "TRAIN Batch 0950/960, Loss   50.1395, NLL-Loss   48.3302, KL-Loss    9.6984, KL-Weight  0.187\n",
      "TRAIN Batch 0960/960, Loss   56.8363, NLL-Loss   47.0799, KL-Loss   51.2452, KL-Weight  0.190\n",
      "TRAIN Epoch 01/10, Mean ELBO   54.9294\n",
      "Model saved at bin/2019-Dec-01-10:27:08/E1.pytorch\n",
      "VALID Batch 0000/240, Loss   55.2459, NLL-Loss   53.4698, KL-Loss    9.3099, KL-Weight  0.191\n",
      "VALID Batch 0050/240, Loss   55.3359, NLL-Loss   53.4939, KL-Loss    9.6554, KL-Weight  0.191\n",
      "VALID Batch 0100/240, Loss   54.2690, NLL-Loss   52.4662, KL-Loss    9.4503, KL-Weight  0.191\n",
      "VALID Batch 0150/240, Loss   59.6516, NLL-Loss   57.7900, KL-Loss    9.7581, KL-Weight  0.191\n",
      "VALID Batch 0200/240, Loss   52.1216, NLL-Loss   50.2866, KL-Loss    9.6191, KL-Weight  0.191\n",
      "VALID Batch 0240/240, Loss   50.3421, NLL-Loss   23.5699, KL-Loss  140.3361, KL-Weight  0.191\n",
      "VALID Epoch 01/10, Mean ELBO   55.6949\n",
      "TRAIN Batch 0000/960, Loss   31.9533, NLL-Loss   30.1951, KL-Loss    9.2163, KL-Weight  0.191\n",
      "TRAIN Batch 0050/960, Loss   43.6617, NLL-Loss   41.5371, KL-Loss   10.0781, KL-Weight  0.211\n",
      "TRAIN Batch 0100/960, Loss   58.8418, NLL-Loss   56.4826, KL-Loss   10.1530, KL-Weight  0.232\n",
      "TRAIN Batch 0150/960, Loss   44.4805, NLL-Loss   42.0319, KL-Loss    9.5875, KL-Weight  0.255\n",
      "TRAIN Batch 0200/960, Loss   41.5502, NLL-Loss   38.9870, KL-Loss    9.1577, KL-Weight  0.280\n",
      "TRAIN Batch 0250/960, Loss   34.2122, NLL-Loss   31.6707, KL-Loss    8.3120, KL-Weight  0.306\n",
      "TRAIN Batch 0300/960, Loss   43.1180, NLL-Loss   40.3764, KL-Loss    8.2349, KL-Weight  0.333\n",
      "TRAIN Batch 0350/960, Loss   46.6537, NLL-Loss   43.6423, KL-Loss    8.3364, KL-Weight  0.361\n",
      "TRAIN Batch 0400/960, Loss   45.9105, NLL-Loss   42.8280, KL-Loss    7.8927, KL-Weight  0.391\n",
      "TRAIN Batch 0450/960, Loss   41.0647, NLL-Loss   37.8727, KL-Loss    7.5879, KL-Weight  0.421\n",
      "TRAIN Batch 0500/960, Loss   48.9080, NLL-Loss   45.4386, KL-Loss    7.6858, KL-Weight  0.451\n",
      "TRAIN Batch 0550/960, Loss   46.1439, NLL-Loss   42.6928, KL-Loss    7.1525, KL-Weight  0.483\n",
      "TRAIN Batch 0600/960, Loss   49.7746, NLL-Loss   46.0738, KL-Loss    7.2036, KL-Weight  0.514\n",
      "TRAIN Batch 0650/960, Loss   39.7811, NLL-Loss   36.1412, KL-Loss    6.6803, KL-Weight  0.545\n",
      "TRAIN Batch 0700/960, Loss   43.2085, NLL-Loss   39.4501, KL-Loss    6.5287, KL-Weight  0.576\n",
      "TRAIN Batch 0750/960, Loss   41.9956, NLL-Loss   38.1038, KL-Loss    6.4234, KL-Weight  0.606\n",
      "TRAIN Batch 0800/960, Loss   46.6268, NLL-Loss   42.4702, KL-Loss    6.5429, KL-Weight  0.635\n",
      "TRAIN Batch 0850/960, Loss   42.6595, NLL-Loss   38.5325, KL-Loss    6.2178, KL-Weight  0.664\n",
      "TRAIN Batch 0900/960, Loss   50.6104, NLL-Loss   46.0022, KL-Loss    6.6684, KL-Weight  0.691\n",
      "TRAIN Batch 0950/960, Loss   46.6731, NLL-Loss   42.2824, KL-Loss    6.1231, KL-Weight  0.717\n",
      "TRAIN Batch 0960/960, Loss   69.1506, NLL-Loss   45.2690, KL-Loss   33.0716, KL-Weight  0.722\n",
      "TRAIN Epoch 02/10, Mean ELBO   44.5324\n",
      "Model saved at bin/2019-Dec-01-10:27:08/E2.pytorch\n",
      "VALID Batch 0000/240, Loss   58.2717, NLL-Loss   54.1350, KL-Loss    5.7246, KL-Weight  0.723\n",
      "VALID Batch 0050/240, Loss   55.9916, NLL-Loss   51.6692, KL-Loss    5.9815, KL-Weight  0.723\n",
      "VALID Batch 0100/240, Loss   55.5005, NLL-Loss   51.3670, KL-Loss    5.7202, KL-Weight  0.723\n",
      "VALID Batch 0150/240, Loss   60.8999, NLL-Loss   56.5239, KL-Loss    6.0557, KL-Weight  0.723\n",
      "VALID Batch 0200/240, Loss   51.9353, NLL-Loss   47.6840, KL-Loss    5.8832, KL-Weight  0.723\n",
      "VALID Batch 0240/240, Loss   81.7951, NLL-Loss   22.7904, KL-Loss   81.6538, KL-Weight  0.723\n",
      "VALID Epoch 02/10, Mean ELBO   57.0220\n",
      "TRAIN Batch 0000/960, Loss   33.0887, NLL-Loss   28.9306, KL-Loss    5.7542, KL-Weight  0.723\n",
      "TRAIN Batch 0050/960, Loss   45.3803, NLL-Loss   40.6483, KL-Loss    6.3349, KL-Weight  0.747\n",
      "TRAIN Batch 0100/960, Loss   30.4751, NLL-Loss   25.8925, KL-Loss    5.9525, KL-Weight  0.770\n",
      "TRAIN Batch 0150/960, Loss   37.5991, NLL-Loss   32.6566, KL-Loss    6.2464, KL-Weight  0.791\n",
      "TRAIN Batch 0200/960, Loss   34.2742, NLL-Loss   29.7521, KL-Loss    5.5749, KL-Weight  0.811\n",
      "TRAIN Batch 0250/960, Loss   31.2244, NLL-Loss   26.6234, KL-Loss    5.5463, KL-Weight  0.830\n",
      "TRAIN Batch 0300/960, Loss   35.2404, NLL-Loss   30.3303, KL-Loss    5.8003, KL-Weight  0.847\n",
      "TRAIN Batch 0350/960, Loss   38.3222, NLL-Loss   33.4409, KL-Loss    5.6624, KL-Weight  0.862\n",
      "TRAIN Batch 0400/960, Loss   42.2772, NLL-Loss   37.1693, KL-Loss    5.8292, KL-Weight  0.876\n",
      "TRAIN Batch 0450/960, Loss   37.0064, NLL-Loss   31.8280, KL-Loss    5.8237, KL-Weight  0.889\n",
      "TRAIN Batch 0500/960, Loss   31.9385, NLL-Loss   27.2589, KL-Loss    5.1943, KL-Weight  0.901\n",
      "TRAIN Batch 0550/960, Loss   40.4805, NLL-Loss   35.4474, KL-Loss    5.5215, KL-Weight  0.912\n",
      "TRAIN Batch 0600/960, Loss   48.0719, NLL-Loss   42.7555, KL-Loss    5.7717, KL-Weight  0.921\n",
      "TRAIN Batch 0650/960, Loss   35.3552, NLL-Loss   30.3286, KL-Loss    5.4065, KL-Weight  0.930\n",
      "TRAIN Batch 0700/960, Loss   36.5656, NLL-Loss   31.4503, KL-Loss    5.4565, KL-Weight  0.937\n",
      "TRAIN Batch 0750/960, Loss   40.8635, NLL-Loss   35.7618, KL-Loss    5.4020, KL-Weight  0.944\n",
      "TRAIN Batch 0800/960, Loss   40.9076, NLL-Loss   35.6281, KL-Loss    5.5538, KL-Weight  0.951\n",
      "TRAIN Batch 0850/960, Loss   45.6220, NLL-Loss   40.2662, KL-Loss    5.6014, KL-Weight  0.956\n",
      "TRAIN Batch 0900/960, Loss   35.8769, NLL-Loss   30.8136, KL-Loss    5.2681, KL-Weight  0.961\n",
      "TRAIN Batch 0950/960, Loss   45.6077, NLL-Loss   40.2683, KL-Loss    5.5300, KL-Weight  0.966\n",
      "TRAIN Batch 0960/960, Loss   69.4475, NLL-Loss   38.6141, KL-Loss   31.9071, KL-Weight  0.966\n",
      "TRAIN Epoch 03/10, Mean ELBO   39.3709\n",
      "Model saved at bin/2019-Dec-01-10:27:08/E3.pytorch\n",
      "VALID Batch 0000/240, Loss   56.5315, NLL-Loss   51.5005, KL-Loss    5.2058, KL-Weight  0.966\n",
      "VALID Batch 0050/240, Loss   57.6253, NLL-Loss   52.4971, KL-Loss    5.3063, KL-Weight  0.966\n",
      "VALID Batch 0100/240, Loss   56.3419, NLL-Loss   51.3504, KL-Loss    5.1649, KL-Weight  0.966\n",
      "VALID Batch 0150/240, Loss   61.8671, NLL-Loss   56.5504, KL-Loss    5.5013, KL-Weight  0.966\n",
      "VALID Batch 0200/240, Loss   53.7567, NLL-Loss   48.6285, KL-Loss    5.3063, KL-Weight  0.966\n",
      "VALID Batch 0240/240, Loss   88.5848, NLL-Loss   21.1262, KL-Loss   69.8018, KL-Weight  0.966\n",
      "VALID Epoch 03/10, Mean ELBO   57.8448\n",
      "TRAIN Batch 0000/960, Loss   31.8552, NLL-Loss   26.8193, KL-Loss    5.2109, KL-Weight  0.966\n",
      "TRAIN Batch 0050/960, Loss   31.5733, NLL-Loss   26.5261, KL-Loss    5.2018, KL-Weight  0.970\n",
      "TRAIN Batch 0100/960, Loss   32.9393, NLL-Loss   27.7840, KL-Loss    5.2948, KL-Weight  0.974\n",
      "TRAIN Batch 0150/960, Loss   32.0346, NLL-Loss   26.8858, KL-Loss    5.2717, KL-Weight  0.977\n",
      "TRAIN Batch 0200/960, Loss   31.2598, NLL-Loss   26.1917, KL-Loss    5.1748, KL-Weight  0.979\n",
      "TRAIN Batch 0250/960, Loss   28.8487, NLL-Loss   23.9736, KL-Loss    4.9658, KL-Weight  0.982\n",
      "TRAIN Batch 0300/960, Loss   30.3376, NLL-Loss   25.2525, KL-Loss    5.1686, KL-Weight  0.984\n",
      "TRAIN Batch 0350/960, Loss   30.0388, NLL-Loss   24.8610, KL-Loss    5.2529, KL-Weight  0.986\n",
      "TRAIN Batch 0400/960, Loss   34.5504, NLL-Loss   29.0756, KL-Loss    5.5447, KL-Weight  0.987\n",
      "TRAIN Batch 0450/960, Loss   36.4516, NLL-Loss   30.9452, KL-Loss    5.5684, KL-Weight  0.989\n",
      "TRAIN Batch 0500/960, Loss   39.5672, NLL-Loss   34.0687, KL-Loss    5.5531, KL-Weight  0.990\n",
      "TRAIN Batch 0550/960, Loss   39.7101, NLL-Loss   34.4045, KL-Loss    5.3522, KL-Weight  0.991\n",
      "TRAIN Batch 0600/960, Loss   45.0214, NLL-Loss   39.2394, KL-Loss    5.8268, KL-Weight  0.992\n",
      "TRAIN Batch 0650/960, Loss   30.9691, NLL-Loss   25.8215, KL-Loss    5.1828, KL-Weight  0.993\n",
      "TRAIN Batch 0700/960, Loss   29.8847, NLL-Loss   24.9708, KL-Loss    4.9436, KL-Weight  0.994\n",
      "TRAIN Batch 0750/960, Loss   33.0398, NLL-Loss   27.8636, KL-Loss    5.2037, KL-Weight  0.995\n",
      "TRAIN Batch 0800/960, Loss   34.0915, NLL-Loss   28.9942, KL-Loss    5.1213, KL-Weight  0.995\n",
      "TRAIN Batch 0850/960, Loss   29.2862, NLL-Loss   24.2860, KL-Loss    5.0210, KL-Weight  0.996\n",
      "TRAIN Batch 0900/960, Loss   31.4120, NLL-Loss   26.1469, KL-Loss    5.2844, KL-Weight  0.996\n",
      "TRAIN Batch 0950/960, Loss   38.6905, NLL-Loss   33.5579, KL-Loss    5.1493, KL-Weight  0.997\n",
      "TRAIN Batch 0960/960, Loss   56.4060, NLL-Loss   27.3180, KL-Loss   29.1797, KL-Weight  0.997\n",
      "TRAIN Epoch 04/10, Mean ELBO   35.0508\n",
      "Model saved at bin/2019-Dec-01-10:27:08/E4.pytorch\n",
      "VALID Batch 0000/240, Loss   56.7987, NLL-Loss   51.7075, KL-Loss    5.1072, KL-Weight  0.997\n",
      "VALID Batch 0050/240, Loss   57.5285, NLL-Loss   52.3641, KL-Loss    5.1807, KL-Weight  0.997\n",
      "VALID Batch 0100/240, Loss   57.7774, NLL-Loss   52.7000, KL-Loss    5.0933, KL-Weight  0.997\n",
      "VALID Batch 0150/240, Loss   60.7003, NLL-Loss   55.3639, KL-Loss    5.3532, KL-Weight  0.997\n",
      "VALID Batch 0200/240, Loss   53.6276, NLL-Loss   48.3818, KL-Loss    5.2623, KL-Weight  0.997\n",
      "VALID Batch 0240/240, Loss   94.5426, NLL-Loss   24.9887, KL-Loss   69.7725, KL-Weight  0.997\n",
      "VALID Epoch 04/10, Mean ELBO   57.8524\n",
      "TRAIN Batch 0000/960, Loss   26.5069, NLL-Loss   21.2218, KL-Loss    5.3017, KL-Weight  0.997\n",
      "TRAIN Batch 0050/960, Loss   32.9199, NLL-Loss   27.7916, KL-Loss    5.1425, KL-Weight  0.997\n",
      "TRAIN Batch 0100/960, Loss   33.2726, NLL-Loss   28.1406, KL-Loss    5.1446, KL-Weight  0.998\n",
      "TRAIN Batch 0150/960, Loss   23.2990, NLL-Loss   18.6170, KL-Loss    4.6921, KL-Weight  0.998\n",
      "TRAIN Batch 0200/960, Loss   25.7999, NLL-Loss   20.6726, KL-Loss    5.1371, KL-Weight  0.998\n",
      "TRAIN Batch 0250/960, Loss   41.9190, NLL-Loss   36.3358, KL-Loss    5.5926, KL-Weight  0.998\n",
      "TRAIN Batch 0300/960, Loss   31.4605, NLL-Loss   26.0846, KL-Loss    5.3839, KL-Weight  0.999\n",
      "TRAIN Batch 0350/960, Loss   31.6368, NLL-Loss   26.3444, KL-Loss    5.2993, KL-Weight  0.999\n",
      "TRAIN Batch 0400/960, Loss   33.5524, NLL-Loss   28.1399, KL-Loss    5.4188, KL-Weight  0.999\n",
      "TRAIN Batch 0450/960, Loss   26.4404, NLL-Loss   21.2220, KL-Loss    5.2237, KL-Weight  0.999\n",
      "TRAIN Batch 0500/960, Loss   24.4064, NLL-Loss   19.5341, KL-Loss    4.8766, KL-Weight  0.999\n",
      "TRAIN Batch 0550/960, Loss   25.1493, NLL-Loss   20.3175, KL-Loss    4.8357, KL-Weight  0.999\n",
      "TRAIN Batch 0600/960, Loss   28.3473, NLL-Loss   23.4143, KL-Loss    4.9365, KL-Weight  0.999\n",
      "TRAIN Batch 0650/960, Loss   35.3461, NLL-Loss   30.1172, KL-Loss    5.2322, KL-Weight  0.999\n",
      "TRAIN Batch 0700/960, Loss   28.5622, NLL-Loss   23.5827, KL-Loss    4.9823, KL-Weight  0.999\n",
      "TRAIN Batch 0750/960, Loss   30.9683, NLL-Loss   25.7369, KL-Loss    5.2339, KL-Weight  1.000\n",
      "TRAIN Batch 0800/960, Loss   29.6387, NLL-Loss   24.6190, KL-Loss    5.0218, KL-Weight  1.000\n",
      "TRAIN Batch 0850/960, Loss   35.9420, NLL-Loss   30.6571, KL-Loss    5.2869, KL-Weight  1.000\n",
      "TRAIN Batch 0900/960, Loss   27.4796, NLL-Loss   22.4108, KL-Loss    5.0705, KL-Weight  1.000\n",
      "TRAIN Batch 0950/960, Loss   28.7533, NLL-Loss   23.6489, KL-Loss    5.1059, KL-Weight  1.000\n",
      "TRAIN Batch 0960/960, Loss   41.4489, NLL-Loss   16.2081, KL-Loss   25.2480, KL-Weight  1.000\n",
      "TRAIN Epoch 05/10, Mean ELBO   31.5441\n",
      "Model saved at bin/2019-Dec-01-10:27:08/E5.pytorch\n",
      "VALID Batch 0000/240, Loss   57.3269, NLL-Loss   52.3584, KL-Loss    4.9699, KL-Weight  1.000\n",
      "VALID Batch 0050/240, Loss   55.9565, NLL-Loss   50.9641, KL-Loss    4.9938, KL-Weight  1.000\n",
      "VALID Batch 0100/240, Loss   56.6060, NLL-Loss   51.6513, KL-Loss    4.9562, KL-Weight  1.000\n",
      "VALID Batch 0150/240, Loss   61.6857, NLL-Loss   56.4838, KL-Loss    5.2033, KL-Weight  1.000\n",
      "VALID Batch 0200/240, Loss   53.3714, NLL-Loss   48.3927, KL-Loss    4.9802, KL-Weight  1.000\n",
      "VALID Batch 0240/240, Loss   85.9762, NLL-Loss   16.6226, KL-Loss   69.3733, KL-Weight  1.000\n",
      "VALID Epoch 05/10, Mean ELBO   57.6579\n",
      "TRAIN Batch 0000/960, Loss   28.7979, NLL-Loss   23.8708, KL-Loss    4.9285, KL-Weight  1.000\n",
      "TRAIN Batch 0050/960, Loss   27.0056, NLL-Loss   22.0089, KL-Loss    4.9979, KL-Weight  1.000\n",
      "TRAIN Batch 0100/960, Loss   27.4227, NLL-Loss   22.2201, KL-Loss    5.2037, KL-Weight  1.000\n",
      "TRAIN Batch 0150/960, Loss   29.8614, NLL-Loss   24.5053, KL-Loss    5.3571, KL-Weight  1.000\n",
      "TRAIN Batch 0200/960, Loss   27.6497, NLL-Loss   22.5154, KL-Loss    5.1351, KL-Weight  1.000\n",
      "TRAIN Batch 0250/960, Loss   27.6072, NLL-Loss   22.3685, KL-Loss    5.2396, KL-Weight  1.000\n",
      "TRAIN Batch 0300/960, Loss   27.7246, NLL-Loss   22.5020, KL-Loss    5.2233, KL-Weight  1.000\n",
      "TRAIN Batch 0350/960, Loss   21.5764, NLL-Loss   16.6898, KL-Loss    4.8871, KL-Weight  1.000\n",
      "TRAIN Batch 0400/960, Loss   27.1968, NLL-Loss   22.2053, KL-Loss    4.9920, KL-Weight  1.000\n",
      "TRAIN Batch 0450/960, Loss   27.2695, NLL-Loss   22.3909, KL-Loss    4.8791, KL-Weight  1.000\n",
      "TRAIN Batch 0500/960, Loss   32.0799, NLL-Loss   26.8204, KL-Loss    5.2599, KL-Weight  1.000\n",
      "TRAIN Batch 0550/960, Loss   34.3553, NLL-Loss   29.0355, KL-Loss    5.3202, KL-Weight  1.000\n",
      "TRAIN Batch 0600/960, Loss   37.8459, NLL-Loss   32.3246, KL-Loss    5.5217, KL-Weight  1.000\n",
      "TRAIN Batch 0650/960, Loss   29.9311, NLL-Loss   24.8735, KL-Loss    5.0579, KL-Weight  1.000\n",
      "TRAIN Batch 0700/960, Loss   32.0907, NLL-Loss   26.7745, KL-Loss    5.3165, KL-Weight  1.000\n",
      "TRAIN Batch 0750/960, Loss   29.1216, NLL-Loss   24.0896, KL-Loss    5.0322, KL-Weight  1.000\n",
      "TRAIN Batch 0800/960, Loss   26.0890, NLL-Loss   21.0534, KL-Loss    5.0357, KL-Weight  1.000\n",
      "TRAIN Batch 0850/960, Loss   28.8781, NLL-Loss   23.8761, KL-Loss    5.0022, KL-Weight  1.000\n",
      "TRAIN Batch 0900/960, Loss   36.5122, NLL-Loss   30.9001, KL-Loss    5.6123, KL-Weight  1.000\n",
      "TRAIN Batch 0950/960, Loss   34.4261, NLL-Loss   29.2601, KL-Loss    5.1662, KL-Weight  1.000\n",
      "TRAIN Batch 0960/960, Loss   66.4668, NLL-Loss   37.3215, KL-Loss   29.1460, KL-Weight  1.000\n",
      "TRAIN Epoch 06/10, Mean ELBO   28.7514\n",
      "Model saved at bin/2019-Dec-01-10:27:08/E6.pytorch\n",
      "VALID Batch 0000/240, Loss   56.7912, NLL-Loss   51.9267, KL-Loss    4.8646, KL-Weight  1.000\n",
      "VALID Batch 0050/240, Loss   57.5454, NLL-Loss   52.6369, KL-Loss    4.9086, KL-Weight  1.000\n",
      "VALID Batch 0100/240, Loss   55.8435, NLL-Loss   50.9894, KL-Loss    4.8543, KL-Weight  1.000\n",
      "VALID Batch 0150/240, Loss   62.1367, NLL-Loss   57.0831, KL-Loss    5.0537, KL-Weight  1.000\n",
      "VALID Batch 0200/240, Loss   52.8143, NLL-Loss   47.9399, KL-Loss    4.8746, KL-Weight  1.000\n",
      "VALID Batch 0240/240, Loss   81.5611, NLL-Loss   15.6974, KL-Loss   65.8654, KL-Weight  1.000\n",
      "VALID Epoch 06/10, Mean ELBO   57.6365\n",
      "TRAIN Batch 0000/960, Loss   29.4597, NLL-Loss   24.3228, KL-Loss    5.1369, KL-Weight  1.000\n",
      "TRAIN Batch 0050/960, Loss   24.3287, NLL-Loss   19.2696, KL-Loss    5.0592, KL-Weight  1.000\n",
      "TRAIN Batch 0100/960, Loss   28.7894, NLL-Loss   23.6585, KL-Loss    5.1310, KL-Weight  1.000\n",
      "TRAIN Batch 0150/960, Loss   30.1268, NLL-Loss   24.9009, KL-Loss    5.2260, KL-Weight  1.000\n",
      "TRAIN Batch 0200/960, Loss   27.9089, NLL-Loss   22.6457, KL-Loss    5.2633, KL-Weight  1.000\n",
      "TRAIN Batch 0250/960, Loss   30.6167, NLL-Loss   25.4404, KL-Loss    5.1763, KL-Weight  1.000\n",
      "TRAIN Batch 0300/960, Loss   31.4152, NLL-Loss   26.0182, KL-Loss    5.3971, KL-Weight  1.000\n",
      "TRAIN Batch 0350/960, Loss   30.0887, NLL-Loss   24.9416, KL-Loss    5.1471, KL-Weight  1.000\n",
      "TRAIN Batch 0400/960, Loss   23.5107, NLL-Loss   18.6643, KL-Loss    4.8465, KL-Weight  1.000\n",
      "TRAIN Batch 0450/960, Loss   32.0036, NLL-Loss   26.6904, KL-Loss    5.3133, KL-Weight  1.000\n",
      "TRAIN Batch 0500/960, Loss   26.3660, NLL-Loss   21.1307, KL-Loss    5.2353, KL-Weight  1.000\n",
      "TRAIN Batch 0550/960, Loss   19.6138, NLL-Loss   14.8468, KL-Loss    4.7671, KL-Weight  1.000\n",
      "TRAIN Batch 0600/960, Loss   26.9655, NLL-Loss   21.8246, KL-Loss    5.1410, KL-Weight  1.000\n",
      "TRAIN Batch 0650/960, Loss   25.4773, NLL-Loss   20.4392, KL-Loss    5.0381, KL-Weight  1.000\n",
      "TRAIN Batch 0700/960, Loss   30.0043, NLL-Loss   24.6783, KL-Loss    5.3260, KL-Weight  1.000\n",
      "TRAIN Batch 0750/960, Loss   23.3087, NLL-Loss   18.4832, KL-Loss    4.8255, KL-Weight  1.000\n",
      "TRAIN Batch 0800/960, Loss   30.1642, NLL-Loss   24.6037, KL-Loss    5.5605, KL-Weight  1.000\n",
      "TRAIN Batch 0850/960, Loss   29.7086, NLL-Loss   24.5252, KL-Loss    5.1834, KL-Weight  1.000\n",
      "TRAIN Batch 0900/960, Loss   23.8137, NLL-Loss   18.9577, KL-Loss    4.8560, KL-Weight  1.000\n",
      "TRAIN Batch 0950/960, Loss   28.9170, NLL-Loss   23.8210, KL-Loss    5.0960, KL-Weight  1.000\n",
      "TRAIN Batch 0960/960, Loss   44.7435, NLL-Loss   19.5963, KL-Loss   25.1472, KL-Weight  1.000\n",
      "TRAIN Epoch 07/10, Mean ELBO   26.5876\n",
      "Model saved at bin/2019-Dec-01-10:27:08/E7.pytorch\n",
      "VALID Batch 0000/240, Loss   58.8319, NLL-Loss   53.8630, KL-Loss    4.9689, KL-Weight  1.000\n",
      "VALID Batch 0050/240, Loss   57.9251, NLL-Loss   52.9246, KL-Loss    5.0006, KL-Weight  1.000\n",
      "VALID Batch 0100/240, Loss   57.2672, NLL-Loss   52.3438, KL-Loss    4.9234, KL-Weight  1.000\n",
      "VALID Batch 0150/240, Loss   62.3583, NLL-Loss   57.1873, KL-Loss    5.1710, KL-Weight  1.000\n",
      "VALID Batch 0200/240, Loss   52.1796, NLL-Loss   47.1827, KL-Loss    4.9969, KL-Weight  1.000\n",
      "VALID Batch 0240/240, Loss   84.1128, NLL-Loss   17.7196, KL-Loss   66.3933, KL-Weight  1.000\n",
      "VALID Epoch 07/10, Mean ELBO   58.4040\n",
      "TRAIN Batch 0000/960, Loss   24.7953, NLL-Loss   19.5203, KL-Loss    5.2749, KL-Weight  1.000\n",
      "TRAIN Batch 0050/960, Loss   24.6570, NLL-Loss   19.5011, KL-Loss    5.1560, KL-Weight  1.000\n",
      "TRAIN Batch 0100/960, Loss   22.3581, NLL-Loss   17.6271, KL-Loss    4.7310, KL-Weight  1.000\n",
      "TRAIN Batch 0150/960, Loss   25.8040, NLL-Loss   20.7144, KL-Loss    5.0895, KL-Weight  1.000\n",
      "TRAIN Batch 0200/960, Loss   22.9586, NLL-Loss   17.8419, KL-Loss    5.1166, KL-Weight  1.000\n",
      "TRAIN Batch 0250/960, Loss   29.1976, NLL-Loss   23.9882, KL-Loss    5.2094, KL-Weight  1.000\n",
      "TRAIN Batch 0300/960, Loss   25.8549, NLL-Loss   20.6431, KL-Loss    5.2118, KL-Weight  1.000\n",
      "TRAIN Batch 0350/960, Loss   26.0871, NLL-Loss   21.0513, KL-Loss    5.0358, KL-Weight  1.000\n",
      "TRAIN Batch 0400/960, Loss   27.4118, NLL-Loss   22.3465, KL-Loss    5.0653, KL-Weight  1.000\n",
      "TRAIN Batch 0450/960, Loss   20.9825, NLL-Loss   15.8752, KL-Loss    5.1072, KL-Weight  1.000\n",
      "TRAIN Batch 0500/960, Loss   23.3729, NLL-Loss   18.3492, KL-Loss    5.0237, KL-Weight  1.000\n",
      "TRAIN Batch 0550/960, Loss   20.2787, NLL-Loss   15.4877, KL-Loss    4.7909, KL-Weight  1.000\n",
      "TRAIN Batch 0600/960, Loss   26.5522, NLL-Loss   21.4889, KL-Loss    5.0633, KL-Weight  1.000\n",
      "TRAIN Batch 0650/960, Loss   24.4482, NLL-Loss   19.4738, KL-Loss    4.9744, KL-Weight  1.000\n",
      "TRAIN Batch 0700/960, Loss   30.9558, NLL-Loss   25.6819, KL-Loss    5.2739, KL-Weight  1.000\n",
      "TRAIN Batch 0750/960, Loss   30.1387, NLL-Loss   24.8260, KL-Loss    5.3127, KL-Weight  1.000\n",
      "TRAIN Batch 0800/960, Loss   31.9642, NLL-Loss   26.4890, KL-Loss    5.4752, KL-Weight  1.000\n",
      "TRAIN Batch 0850/960, Loss   22.5013, NLL-Loss   17.4793, KL-Loss    5.0220, KL-Weight  1.000\n",
      "TRAIN Batch 0900/960, Loss   25.0312, NLL-Loss   19.8601, KL-Loss    5.1711, KL-Weight  1.000\n",
      "TRAIN Batch 0950/960, Loss   29.3942, NLL-Loss   24.1017, KL-Loss    5.2925, KL-Weight  1.000\n",
      "TRAIN Batch 0960/960, Loss   54.3290, NLL-Loss   23.5024, KL-Loss   30.8265, KL-Weight  1.000\n",
      "TRAIN Epoch 08/10, Mean ELBO   24.8832\n",
      "Model saved at bin/2019-Dec-01-10:27:08/E8.pytorch\n",
      "VALID Batch 0000/240, Loss   59.9681, NLL-Loss   55.0986, KL-Loss    4.8696, KL-Weight  1.000\n",
      "VALID Batch 0050/240, Loss   56.8076, NLL-Loss   51.9566, KL-Loss    4.8510, KL-Weight  1.000\n",
      "VALID Batch 0100/240, Loss   58.3123, NLL-Loss   53.4166, KL-Loss    4.8957, KL-Weight  1.000\n",
      "VALID Batch 0150/240, Loss   60.8778, NLL-Loss   55.8398, KL-Loss    5.0380, KL-Weight  1.000\n",
      "VALID Batch 0200/240, Loss   53.4522, NLL-Loss   48.5766, KL-Loss    4.8756, KL-Weight  1.000\n",
      "VALID Batch 0240/240, Loss   90.0474, NLL-Loss   21.6843, KL-Loss   68.3632, KL-Weight  1.000\n",
      "VALID Epoch 08/10, Mean ELBO   58.7472\n",
      "TRAIN Batch 0000/960, Loss   21.2377, NLL-Loss   16.3518, KL-Loss    4.8859, KL-Weight  1.000\n",
      "TRAIN Batch 0050/960, Loss   21.4243, NLL-Loss   16.5376, KL-Loss    4.8867, KL-Weight  1.000\n",
      "TRAIN Batch 0100/960, Loss   17.1783, NLL-Loss   12.6470, KL-Loss    4.5313, KL-Weight  1.000\n",
      "TRAIN Batch 0150/960, Loss   27.3061, NLL-Loss   21.9772, KL-Loss    5.3289, KL-Weight  1.000\n",
      "TRAIN Batch 0200/960, Loss   22.7541, NLL-Loss   17.6539, KL-Loss    5.1002, KL-Weight  1.000\n",
      "TRAIN Batch 0250/960, Loss   20.3938, NLL-Loss   15.5952, KL-Loss    4.7986, KL-Weight  1.000\n",
      "TRAIN Batch 0300/960, Loss   24.2596, NLL-Loss   19.3221, KL-Loss    4.9375, KL-Weight  1.000\n",
      "TRAIN Batch 0350/960, Loss   24.4310, NLL-Loss   19.5652, KL-Loss    4.8659, KL-Weight  1.000\n",
      "TRAIN Batch 0400/960, Loss   23.1032, NLL-Loss   18.1469, KL-Loss    4.9563, KL-Weight  1.000\n",
      "TRAIN Batch 0450/960, Loss   24.0333, NLL-Loss   18.8920, KL-Loss    5.1413, KL-Weight  1.000\n",
      "TRAIN Batch 0500/960, Loss   27.5549, NLL-Loss   22.3388, KL-Loss    5.2161, KL-Weight  1.000\n",
      "TRAIN Batch 0550/960, Loss   28.9097, NLL-Loss   23.5865, KL-Loss    5.3232, KL-Weight  1.000\n",
      "TRAIN Batch 0600/960, Loss   22.6079, NLL-Loss   17.7857, KL-Loss    4.8222, KL-Weight  1.000\n",
      "TRAIN Batch 0650/960, Loss   19.7879, NLL-Loss   14.9390, KL-Loss    4.8489, KL-Weight  1.000\n",
      "TRAIN Batch 0700/960, Loss   28.2365, NLL-Loss   22.9523, KL-Loss    5.2842, KL-Weight  1.000\n",
      "TRAIN Batch 0750/960, Loss   24.2665, NLL-Loss   19.3938, KL-Loss    4.8727, KL-Weight  1.000\n",
      "TRAIN Batch 0800/960, Loss   25.4269, NLL-Loss   20.4753, KL-Loss    4.9516, KL-Weight  1.000\n",
      "TRAIN Batch 0850/960, Loss   17.7419, NLL-Loss   12.8317, KL-Loss    4.9101, KL-Weight  1.000\n",
      "TRAIN Batch 0900/960, Loss   28.7937, NLL-Loss   23.4206, KL-Loss    5.3731, KL-Weight  1.000\n",
      "TRAIN Batch 0950/960, Loss   28.1849, NLL-Loss   22.9654, KL-Loss    5.2195, KL-Weight  1.000\n",
      "TRAIN Batch 0960/960, Loss   47.7196, NLL-Loss   20.9354, KL-Loss   26.7842, KL-Weight  1.000\n",
      "TRAIN Epoch 09/10, Mean ELBO   23.4533\n",
      "Model saved at bin/2019-Dec-01-10:27:08/E9.pytorch\n",
      "VALID Batch 0000/240, Loss   60.7339, NLL-Loss   55.8501, KL-Loss    4.8838, KL-Weight  1.000\n",
      "VALID Batch 0050/240, Loss   58.2946, NLL-Loss   53.3800, KL-Loss    4.9146, KL-Weight  1.000\n",
      "VALID Batch 0100/240, Loss   58.2978, NLL-Loss   53.4243, KL-Loss    4.8735, KL-Weight  1.000\n",
      "VALID Batch 0150/240, Loss   64.1985, NLL-Loss   59.1556, KL-Loss    5.0429, KL-Weight  1.000\n",
      "VALID Batch 0200/240, Loss   53.7576, NLL-Loss   48.8116, KL-Loss    4.9460, KL-Weight  1.000\n",
      "VALID Batch 0240/240, Loss   86.3090, NLL-Loss   17.2988, KL-Loss   69.0102, KL-Weight  1.000\n",
      "VALID Epoch 09/10, Mean ELBO   59.5821\n"
     ]
    }
   ],
   "source": [
    "%pdb on\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    for split in splits:\n",
    "\n",
    "        data_loader = DataLoader(\n",
    "            dataset=_datasets[split],\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=split=='train',\n",
    "            num_workers=cpu_count(),\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "\n",
    "        tracker = defaultdict(tensor)\n",
    "\n",
    "        # Enable/Disable Dropout\n",
    "        if split == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        for iteration, batch in enumerate(data_loader):\n",
    "            \n",
    "            batch_size = batch['src_input'].size(0)\n",
    "            \n",
    "            for k, v in batch.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    batch[k] = to_var(v)\n",
    "            \n",
    "            # loss calculation | additional Condition ！!!\n",
    "            cal_dict = model(batch['tgt_input'], batch['tgt_length'], cond_sequence=batch['src_input'], cond_length=batch['src_length'])\n",
    "            logp, mean, logv, z = cal_dict['logp'], cal_dict['mean'], cal_dict['logv'], cal_dict['z']\n",
    "            \n",
    "            loss_dict = model.loss(logp, batch['tgt_target'], batch['tgt_length'], mean, logv, args.anneal_function, step, args.k, args.x0, \n",
    "                                   cond_mean=cal_dict['cond_mean'], cond_logv=cal_dict['cond_logv'], bow_input=cal_dict['dec_input'])\n",
    "            loss, NLL_loss, KL_weight, KL_loss, avg_bow_loss = loss_dict['loss'], loss_dict['NLL_loss'], loss_dict['KL_weight'], loss_dict['KL_loss'], loss_dict.get('avg_bow_loss')\n",
    "\n",
    "            # backward + optimization\n",
    "            if split == 'train':\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                step += 1\n",
    "\n",
    "            # bookkeepeing\n",
    "            tracker['ELBO'] = torch.cat((tracker['ELBO'], loss.data.view(1)))\n",
    "\n",
    "            if args.tensorboard_logging:\n",
    "                writer.add_scalar(\"%s/ELBO\"%split.upper(), loss.data.item(), epoch*len(data_loader) + iteration)\n",
    "                writer.add_scalar(\"%s/NLL Loss\"%split.upper(), NLL_loss.data.item()/batch_size, epoch*len(data_loader) + iteration)\n",
    "                writer.add_scalar(\"%s/KL Loss\"%split.upper(), KL_loss.data.item()/batch_size, epoch*len(data_loader) + iteration)\n",
    "                writer.add_scalar(\"%s/KL Weight\"%split.upper(), KL_weight, epoch*len(data_loader) + iteration)\n",
    "                if avg_bow_loss is not None:\n",
    "                    writer.add_scalar(\"%s/BOW Loss\"%split.upper(), avg_bow_loss, epoch*len(data_loader) + iteration)\n",
    "\n",
    "            if iteration % args.print_every == 0 or iteration+1 == len(data_loader):\n",
    "                print_text = \"%s Batch %04d/%i, Loss %9.4f, NLL-Loss %9.4f, KL-Loss %9.4f, KL-Weight %6.3f\"%(split.upper(), iteration, len(data_loader)-1, loss.data.item(), NLL_loss.data.item()/batch_size, KL_loss.data.item()/batch_size, KL_weight)\n",
    "                if avg_bow_loss is not None:\n",
    "                    print_text += ', BOW Loss %9.4f,'%(avg_bow_loss)\n",
    "                print(print_text)\n",
    "\n",
    "            if split == 'valid':\n",
    "                if 'target_sents' not in tracker:\n",
    "                    tracker['target_sents'] = list()\n",
    "                tracker['target_sents'] += idx2word(batch['tgt_target'].data, i2w=train_target_ptb.get_i2w(), pad_idx=PAD_INDEX)\n",
    "                tracker['z'] = torch.cat((tracker['z'], z.data), dim=0)\n",
    "\n",
    "        print(\"%s Epoch %02d/%i, Mean ELBO %9.4f\"%(split.upper(), epoch, args.epochs, torch.mean(tracker['ELBO'])))\n",
    "\n",
    "        if args.tensorboard_logging:\n",
    "            writer.add_scalar(\"%s-Epoch/ELBO\"%split.upper(), torch.mean(tracker['ELBO']), epoch)\n",
    "\n",
    "        # save a dump of all sentences and the encoded latent space\n",
    "        if split == 'valid':\n",
    "            dump = {'target_sents':tracker['target_sents'], 'z':tracker['z'].tolist()}\n",
    "            if not os.path.exists(os.path.join('dumps', ts)):\n",
    "                os.makedirs('dumps/'+ts)\n",
    "            with open(os.path.join('dumps/'+ts+'/valid_E%i.json'%epoch), 'w') as dump_file:\n",
    "                json.dump(dump,dump_file)\n",
    "\n",
    "        # save checkpoint\n",
    "        if split == 'train':\n",
    "            checkpoint_path = os.path.join(save_model_path, \"E%i.pytorch\"%(epoch))\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(\"Model saved at %s\"%checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
