{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "path = os.path.join(os.path.abspath(os.curdir), '../src')\n",
    "sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from multiprocessing import cpu_count\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "from ptb import PTB\n",
    "from utils import idx2word, experiment_name, AttributeDict\n",
    "from models.model_kwbase import SentenceVAE\n",
    "from models.model_utils import to_var, sample_z\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/root/user/work/src/Sentence-VAE', '/root/user/work/src/Sentence-VAE/runs')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_dir = os.path.abspath('..')\n",
    "runs_dir = f'{top_dir}/runs'\n",
    "top_dir, runs_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/user/work/src/Sentence-VAE/data/eccos_v2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_base_dir = f'{top_dir}/data'\n",
    "data_name = 'eccos_v2'\n",
    "data_dir = f'{data_base_dir}/{data_name}'\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/root/user/work/src/Sentence-VAE/runs',\n",
       " '/root/user/work/src/Sentence-VAE/runs')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir, save_model_path = runs_dir, runs_dir\n",
    "# log_dir = f'{runs_dir}/logs'\n",
    "# save_model_path = f'{runs_dir}/models'\n",
    "log_dir, save_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src max: 51, tgt max: 51\n"
     ]
    }
   ],
   "source": [
    "def readlines(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return [s.replace('\\n', '') for s in f.readlines()]\n",
    "\n",
    "def cal_max_file_lines(path):\n",
    "    lines = readlines(path)\n",
    "    line_lengths = [len(line.split(' ')) for line in lines]\n",
    "    return max(line_lengths)\n",
    "    \n",
    "src_max_length = cal_max_file_lines(f'{data_dir}/src/ptb.train.txt')\n",
    "tgt_max_length = cal_max_file_lines(f'{data_dir}/tgt/ptb.train.txt')\n",
    "print(f'src max: {src_max_length}, tgt max: {tgt_max_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AttrDict{'data_dir': '/root/user/work/src/Sentence-VAE/data/eccos_v2', 'create_data': False, 'max_sequence_length': 51, 'max_sequence_length_src': 51, 'min_occ': 1, 'test': False, 'epochs': 10, 'batch_size': 32, 'learning_rate': 0.001, 'embedding_size': 300, 'rnn_type': 'gru', 'hidden_size': 256, 'num_layers': 1, 'bidirectional': False, 'latent_size': 16, 'word_dropout': 0, 'embedding_dropout': 0.5, 'anneal_function': 'logistic', 'k': 0.0025, 'x0': 2500, 'print_every': 50, 'tensorboard_logging': True, 'logdir': '/root/user/work/src/Sentence-VAE/runs', 'save_model_path': '/root/user/work/src/Sentence-VAE/runs', 'experiment_name': 'vae_eccos_v2', 'debug': False}>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = {\n",
    "    'data_dir': data_dir,\n",
    "    'create_data': False,\n",
    "    'max_sequence_length': tgt_max_length,\n",
    "    'max_sequence_length_src': src_max_length,\n",
    "    'min_occ': 1,\n",
    "    'test': False,\n",
    "\n",
    "    'epochs': 10,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "\n",
    "    'embedding_size': 300,\n",
    "    'rnn_type': 'gru',\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 1,\n",
    "    'bidirectional': False,\n",
    "    'latent_size': 16,\n",
    "    'word_dropout': 0,\n",
    "    'embedding_dropout': 0.5,\n",
    "\n",
    "    'anneal_function': 'logistic',\n",
    "    'k': 0.0025,\n",
    "    'x0': 2500,\n",
    "\n",
    "    'print_every': 50,\n",
    "    'tensorboard_logging': True,\n",
    "    'logdir': log_dir,\n",
    "    'save_model_path': save_model_path,\n",
    "    'experiment_name': f'vae_{data_name}',\n",
    "    \n",
    "    'debug': False,\n",
    "}\n",
    "\n",
    "args = AttributeDict(args)\n",
    "\n",
    "args.rnn_type = args.rnn_type.lower()\n",
    "args.anneal_function = args.anneal_function.lower()\n",
    "\n",
    "assert args.rnn_type in ['rnn', 'lstm', 'gru']\n",
    "assert args.anneal_function in ['logistic', 'linear']\n",
    "assert 0 <= args.word_dropout <= 1\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /root/user/work/src/Sentence-VAE/data/eccos_v2\n",
      "('train', 'src')\n",
      "vocab: 10293, records: 44596\n",
      "('train', 'tgt')\n",
      "vocab: 10293, records: 44596\n",
      "('valid', 'src')\n",
      "vocab: 10293, records: 2477\n",
      "('valid', 'tgt')\n",
      "vocab: 10293, records: 2477\n",
      "CPU times: user 2.09 s, sys: 117 ms, total: 2.21 s\n",
      "Wall time: 2.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import itertools\n",
    "data_types = ['src', 'tgt']\n",
    "splits = ['train', 'valid'] + (['test'] if args.test else [])\n",
    "datasets = OrderedDict()\n",
    "print(f'loading {args.data_dir}')\n",
    "for split, src_tgt in itertools.product(splits, data_types):\n",
    "    key = (split, src_tgt)\n",
    "    print(key)\n",
    "    datasets[key] = PTB(\n",
    "        data_dir=f'{args.data_dir}/{src_tgt}',\n",
    "        split=split,\n",
    "        create_data=args.create_data,\n",
    "        max_sequence_length=args.max_sequence_length if src_tgt == 'tgt' else args.max_sequence_length_src,\n",
    "        min_occ=args.min_occ\n",
    "    )\n",
    "    print(f'vocab: {datasets[key].vocab_size}, records: {len(datasets[key].data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "■ src-input \n",
      "<sos> ベタベタ ・ <unk> ! <sep> 透明 & 毛穴 カバー 耐久 パウダー <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "■ tgt-target\n",
      "ベタベタ ・ <unk> ! <sep> 透明 & 毛穴 カバー 耐久 パウダー <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "# 実際のデータ確認\n",
    "def ids2text(id_list, ptb):\n",
    "    return ' '.join([ptb.i2w[f'{i}'] for i in id_list])\n",
    "\n",
    "_ptb_src = datasets[('train', 'src')]\n",
    "_ptb_tgt = datasets[('train', 'tgt')]\n",
    "index = str(101)\n",
    "_sample_src, _sample_tgt = _ptb_src.data[index], _ptb_tgt[index]\n",
    "print(f'■ src-input \\n{ids2text(_sample_src[\"input\"], _ptb_src)}')\n",
    "# print(f'■ src-target \\n{ids2text(_sample_src[\"target\"], _ptb_src)}')\n",
    "# print(f'■ tgt-input\\n{ids2text(_sample_tgt[\"input\"], _ptb_tgt)}')\n",
    "print(f'■ tgt-target\\n{ids2text(_sample_tgt[\"target\"], _ptb_tgt)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptb import SOS_INDEX, EOS_INDEX, PAD_INDEX, UNK_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceVAE(\n",
    "    vocab_size=datasets[('train', 'src')].vocab_size,\n",
    "    sos_idx=SOS_INDEX,\n",
    "    eos_idx=EOS_INDEX,\n",
    "    pad_idx=PAD_INDEX,\n",
    "    unk_idx=UNK_INDEX,\n",
    "    max_sequence_length=args.max_sequence_length,\n",
    "    embedding_size=args.embedding_size,\n",
    "    rnn_type=args.rnn_type,\n",
    "    hidden_size=args.hidden_size,\n",
    "    word_dropout=args.word_dropout,\n",
    "    embedding_dropout=args.embedding_dropout,\n",
    "    latent_size=args.latent_size,\n",
    "    num_layers=args.num_layers,\n",
    "    bidirectional=args.bidirectional,\n",
    "    \n",
    "    # bow loss\n",
    "    # bow_hidden_size=256,\n",
    "    use_bow_loss=False,\n",
    "    \n",
    "    # kw base\n",
    "    # out_vocab_size=datasets[('train', 'tgt')].vocab_size,\n",
    "    )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceVAE(\n",
       "  (embedding): Embedding(10293, 300)\n",
       "  (embedding_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (decoder_embedding): Embedding(10293, 300)\n",
       "  (encoder_rnn): GRU(300, 256, batch_first=True)\n",
       "  (decoder_rnn): GRU(300, 256, batch_first=True)\n",
       "  (hidden2mean): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (hidden2logv): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (latent2hidden): Linear(in_features=16, out_features=256, bias=True)\n",
       "  (outputs2vocab): Linear(in_features=256, out_features=10293, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cstr(obj):\n",
    "    return f'```{obj}```'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_dict(_dict):\n",
    "    return '  \\n'.join([f'{k}: {v}' for k,v in _dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_model_dict(model, args):\n",
    "    meta_dict = {k:v for k, v in model.__dict__.items() if not k[0] == '_'}\n",
    "    meta_dict.update(args.obj)\n",
    "    return meta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard logging: True\n",
      "▼tensorboard logging\n",
      "/root/user/work/src/Sentence-VAE/runs/vae_eccos_v2_TS=2020-01-10-061630_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500\n",
      "▼ model save\n",
      "/root/user/work/src/Sentence-VAE/runs/vae_eccos_v2_TS=2020-01-10-061630_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models\n"
     ]
    }
   ],
   "source": [
    "print(f'tensorboard logging: {args.tensorboard_logging}')\n",
    "ts = time.strftime('%Y-%m-%d-%H%M%S', time.localtime())\n",
    "exp_name = experiment_name(args,ts)\n",
    "\n",
    "if args.tensorboard_logging:\n",
    "    writer_path = os.path.join(args.logdir, exp_name)\n",
    "    writer = SummaryWriter(writer_path)\n",
    "    writer.add_text(\"model\", cstr(model.__repr__().replace('\\n', '  \\n')))\n",
    "    writer.add_text(\"args\", cstr(str_dict(args.obj)))\n",
    "    writer.add_text(\"ts\", ts)\n",
    "    print(f'▼tensorboard logging\\n{writer_path}')\n",
    "    \n",
    "save_model_path = os.path.join(args.save_model_path, exp_name, 'models')\n",
    "os.makedirs(save_model_path, exist_ok=True)\n",
    "print(f'▼ model save\\n{save_model_path}')\n",
    "\n",
    "# メタパラメータ保存\n",
    "with open(os.path.join(save_model_path, 'model_meta.json'), 'w') as f:\n",
    "    meta_dict = get_meta_model_dict(model, args)\n",
    "    meta_dict.pop('tensor')\n",
    "    json.dump(meta_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "LongTensor = torch.cuda.LongTensor if torch.cuda.is_available() else torch.LongTensor\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys([('train', 'src'), ('train', 'tgt'), ('valid', 'src'), ('valid', 'tgt')])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> 水 なし ! <sep> 美容 成分 しか 入っ て ない ! <sep> セラミド <num> 倍 ジェル が やばい 笑 <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "水 なし ! <sep> 美容 成分 しか 入っ て ない ! <sep> セラミド <num> 倍 ジェル が やばい 笑 <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "ae_datasets = {split: dataset for (split, src_tgt), dataset in datasets.items() if src_tgt == 'tgt'}\n",
    "print(ids2text(ae_datasets['train'][0]['input'], ae_datasets['train']))\n",
    "print(ids2text(ae_datasets['train'][0]['target'], ae_datasets['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 44596\n",
      "valid: 2477\n"
     ]
    }
   ],
   "source": [
    "# ループ内で扱う用に変形\n",
    "_datasets = {}\n",
    "for split in splits:\n",
    "    dataset = []\n",
    "    print(f\"{split}: {len(datasets[(split, 'src')])}\")\n",
    "    for i in range(len(datasets[(split, 'src')])):\n",
    "        _data = {}\n",
    "        for data_type in data_types:\n",
    "            d = datasets[(split, data_type)][f'{i}']\n",
    "            _data.update({f'{data_type}_{k}': v for k, v in d.items()})\n",
    "        dataset.append(_data)\n",
    "    _datasets[split] = dataset\n",
    "    if args.debug:\n",
    "        _data_limit = 300\n",
    "        _datasets[split] = dataset[:_data_limit]\n",
    "        print(f'debug → {_data_limit}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'valid'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_input': array([ 2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,  6,  7, 14, 15, 16, 17,\n",
       "        18, 19, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'src_target': array([ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13,  6,  7, 14, 15, 16, 17, 18,\n",
       "        19, 20,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'src_length': 20,\n",
       " 'tgt_input': array([ 2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,  6,  7, 14, 15, 16, 17,\n",
       "        18, 19, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'tgt_target': array([ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13,  6,  7, 14, 15, 16, 17, 18,\n",
       "        19, 20,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'tgt_length': 20}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ptb.PTB at 0x7efc8f92dc90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target_ptb = datasets[('train', 'tgt')]\n",
    "train_target_ptb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ids2text as ids2ptext\n",
    "from metric import write_tensorboard_valid_metric, remove_pad_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0000/1393, Loss  173.1861, NLL-Loss  173.1854, KL-Loss    0.3519, KL-Weight  0.002\n",
      "TRAIN Batch 0050/1393, Loss   95.2253, NLL-Loss   95.1984, KL-Loss   12.3638, KL-Weight  0.002\n",
      "TRAIN Batch 0100/1393, Loss   90.9645, NLL-Loss   90.8953, KL-Loss   27.9826, KL-Weight  0.002\n",
      "TRAIN Batch 0150/1393, Loss   97.8059, NLL-Loss   97.7140, KL-Loss   32.8032, KL-Weight  0.003\n",
      "TRAIN Batch 0200/1393, Loss   78.7274, NLL-Loss   78.6018, KL-Loss   39.6056, KL-Weight  0.003\n",
      "TRAIN Batch 0250/1393, Loss   76.9029, NLL-Loss   76.7050, KL-Loss   55.0716, KL-Weight  0.004\n",
      "TRAIN Batch 0300/1393, Loss   75.2121, NLL-Loss   74.9897, KL-Loss   54.6367, KL-Weight  0.004\n",
      "TRAIN Batch 0350/1393, Loss   70.3116, NLL-Loss   70.0524, KL-Loss   56.2361, KL-Weight  0.005\n",
      "TRAIN Batch 0400/1393, Loss   69.8588, NLL-Loss   69.5403, KL-Loss   61.0130, KL-Weight  0.005\n",
      "TRAIN Batch 0450/1393, Loss   72.0345, NLL-Loss   71.6848, KL-Loss   59.1659, KL-Weight  0.006\n",
      "TRAIN Batch 0500/1393, Loss   79.7716, NLL-Loss   79.3153, KL-Loss   68.1774, KL-Weight  0.007\n",
      "TRAIN Batch 0550/1393, Loss   70.2912, NLL-Loss   69.7850, KL-Loss   66.8068, KL-Weight  0.008\n",
      "TRAIN Batch 0600/1393, Loss   79.3452, NLL-Loss   78.7916, KL-Loss   64.5428, KL-Weight  0.009\n",
      "TRAIN Batch 0650/1393, Loss   65.4033, NLL-Loss   64.8037, KL-Loss   61.7578, KL-Weight  0.010\n",
      "TRAIN Batch 0700/1393, Loss   69.5390, NLL-Loss   68.8368, KL-Loss   63.9094, KL-Weight  0.011\n",
      "TRAIN Batch 0750/1393, Loss   60.0669, NLL-Loss   59.2776, KL-Loss   63.4907, KL-Weight  0.012\n",
      "TRAIN Batch 0800/1393, Loss   65.1571, NLL-Loss   64.2649, KL-Loss   63.4405, KL-Weight  0.014\n",
      "TRAIN Batch 0850/1393, Loss   67.5917, NLL-Loss   66.6422, KL-Loss   59.6893, KL-Weight  0.016\n",
      "TRAIN Batch 0900/1393, Loss   60.4752, NLL-Loss   59.4227, KL-Loss   58.5209, KL-Weight  0.018\n",
      "TRAIN Batch 0950/1393, Loss   70.9072, NLL-Loss   69.7586, KL-Loss   56.4909, KL-Weight  0.020\n",
      "TRAIN Batch 1000/1393, Loss   62.9655, NLL-Loss   61.6028, KL-Loss   59.3035, KL-Weight  0.023\n",
      "TRAIN Batch 1050/1393, Loss   66.9030, NLL-Loss   65.4548, KL-Loss   55.7882, KL-Weight  0.026\n",
      "TRAIN Batch 1100/1393, Loss   61.4400, NLL-Loss   59.8368, KL-Loss   54.6954, KL-Weight  0.029\n",
      "TRAIN Batch 1150/1393, Loss   67.0419, NLL-Loss   65.3345, KL-Loss   51.6056, KL-Weight  0.033\n",
      "TRAIN Batch 1200/1393, Loss   56.8680, NLL-Loss   55.0443, KL-Loss   48.8575, KL-Weight  0.037\n",
      "TRAIN Batch 1250/1393, Loss   60.9558, NLL-Loss   58.8812, KL-Loss   49.2912, KL-Weight  0.042\n",
      "TRAIN Batch 1300/1393, Loss   50.6160, NLL-Loss   48.4644, KL-Loss   45.3678, KL-Weight  0.047\n",
      "TRAIN Batch 1350/1393, Loss   70.1042, NLL-Loss   67.6485, KL-Loss   45.9846, KL-Weight  0.053\n",
      "TRAIN Batch 1393/1393, Loss   56.7861, NLL-Loss   54.2902, KL-Loss   42.2284, KL-Weight  0.059\n",
      "TRAIN Epoch 00/10, Mean Loss   73.0516\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/vae_eccos_v2_TS=2020-01-10-061630_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E0.pytorch\n",
      "VALID Batch 0000/77, Loss   56.2304, NLL-Loss   53.7654, KL-Loss   41.6072, KL-Weight  0.059\n",
      "VALID Batch 0050/77, Loss   64.9765, NLL-Loss   62.3471, KL-Loss   44.3820, KL-Weight  0.059\n",
      "VALID Batch 0077/77, Loss   54.6609, NLL-Loss   52.2148, KL-Loss   41.2882, KL-Weight  0.059\n",
      "VALID Epoch 00/10, Mean Loss   57.9468\n",
      "TRAIN Batch 0000/1393, Loss   60.2593, NLL-Loss   57.7048, KL-Loss   43.1184, KL-Weight  0.059\n",
      "TRAIN Batch 0050/1393, Loss   55.3184, NLL-Loss   52.5823, KL-Loss   41.0766, KL-Weight  0.067\n",
      "TRAIN Batch 0100/1393, Loss   52.0684, NLL-Loss   48.9402, KL-Loss   41.8127, KL-Weight  0.075\n",
      "TRAIN Batch 0150/1393, Loss   59.8109, NLL-Loss   56.5497, KL-Loss   38.8523, KL-Weight  0.084\n",
      "TRAIN Batch 0200/1393, Loss   67.9589, NLL-Loss   64.3718, KL-Loss   38.1343, KL-Weight  0.094\n",
      "TRAIN Batch 0250/1393, Loss   54.9983, NLL-Loss   51.4245, KL-Loss   33.9493, KL-Weight  0.105\n",
      "TRAIN Batch 0300/1393, Loss   53.0866, NLL-Loss   49.1157, KL-Loss   33.7559, KL-Weight  0.118\n",
      "TRAIN Batch 0350/1393, Loss   62.8950, NLL-Loss   58.4786, KL-Loss   33.6506, KL-Weight  0.131\n",
      "TRAIN Batch 0400/1393, Loss   58.9047, NLL-Loss   54.4041, KL-Loss   30.7907, KL-Weight  0.146\n",
      "TRAIN Batch 0450/1393, Loss   61.6047, NLL-Loss   56.8989, KL-Loss   28.9650, KL-Weight  0.162\n",
      "TRAIN Batch 0500/1393, Loss   68.2987, NLL-Loss   63.2249, KL-Loss   28.1564, KL-Weight  0.180\n",
      "TRAIN Batch 0550/1393, Loss   63.4486, NLL-Loss   58.2944, KL-Loss   25.8474, KL-Weight  0.199\n",
      "TRAIN Batch 0600/1393, Loss   65.3273, NLL-Loss   59.5768, KL-Loss   26.1252, KL-Weight  0.220\n",
      "TRAIN Batch 0650/1393, Loss   57.7520, NLL-Loss   52.0538, KL-Loss   23.5154, KL-Weight  0.242\n",
      "TRAIN Batch 0700/1393, Loss   68.3751, NLL-Loss   62.2191, KL-Loss   23.1428, KL-Weight  0.266\n",
      "TRAIN Batch 0750/1393, Loss   59.1613, NLL-Loss   52.9716, KL-Loss   21.2625, KL-Weight  0.291\n",
      "TRAIN Batch 0800/1393, Loss   61.2856, NLL-Loss   55.2409, KL-Loss   19.0350, KL-Weight  0.318\n",
      "TRAIN Batch 0850/1393, Loss   70.7895, NLL-Loss   64.0142, KL-Loss   19.6248, KL-Weight  0.345\n",
      "TRAIN Batch 0900/1393, Loss   65.7552, NLL-Loss   58.9167, KL-Loss   18.2837, KL-Weight  0.374\n",
      "TRAIN Batch 0950/1393, Loss   57.4210, NLL-Loss   49.8712, KL-Loss   18.7009, KL-Weight  0.404\n",
      "TRAIN Batch 1000/1393, Loss   60.9181, NLL-Loss   53.8697, KL-Loss   16.2355, KL-Weight  0.434\n",
      "TRAIN Batch 1050/1393, Loss   56.4375, NLL-Loss   49.4925, KL-Loss   14.9335, KL-Weight  0.465\n",
      "TRAIN Batch 1100/1393, Loss   61.2445, NLL-Loss   53.9474, KL-Loss   14.7044, KL-Weight  0.496\n",
      "TRAIN Batch 1150/1393, Loss   67.3997, NLL-Loss   59.2366, KL-Loss   15.4759, KL-Weight  0.527\n",
      "TRAIN Batch 1200/1393, Loss   66.3834, NLL-Loss   58.7154, KL-Loss   13.7301, KL-Weight  0.558\n",
      "TRAIN Batch 1250/1393, Loss   63.3132, NLL-Loss   54.4183, KL-Loss   15.1007, KL-Weight  0.589\n",
      "TRAIN Batch 1300/1393, Loss   62.8167, NLL-Loss   54.1415, KL-Loss   14.0165, KL-Weight  0.619\n",
      "TRAIN Batch 1350/1393, Loss   58.4838, NLL-Loss   50.6551, KL-Loss   12.0824, KL-Weight  0.648\n",
      "TRAIN Batch 1393/1393, Loss   72.2181, NLL-Loss   63.4742, KL-Loss   13.0107, KL-Weight  0.672\n",
      "TRAIN Epoch 01/10, Mean Loss   61.4537\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/vae_eccos_v2_TS=2020-01-10-061630_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E1.pytorch\n",
      "VALID Batch 0000/77, Loss   61.9584, NLL-Loss   53.9030, KL-Loss   11.9763, KL-Weight  0.673\n",
      "VALID Batch 0050/77, Loss   70.1581, NLL-Loss   61.5748, KL-Loss   12.7612, KL-Weight  0.673\n",
      "VALID Batch 0077/77, Loss   56.5954, NLL-Loss   48.5644, KL-Loss   11.9402, KL-Weight  0.673\n",
      "VALID Epoch 01/10, Mean Loss   63.2102\n",
      "TRAIN Batch 0000/1393, Loss   58.7001, NLL-Loss   50.3297, KL-Loss   12.4448, KL-Weight  0.673\n",
      "TRAIN Batch 0050/1393, Loss   60.2741, NLL-Loss   51.4949, KL-Loss   12.5503, KL-Weight  0.700\n",
      "TRAIN Batch 0100/1393, Loss   62.3933, NLL-Loss   53.7299, KL-Loss   11.9476, KL-Weight  0.725\n",
      "TRAIN Batch 0150/1393, Loss   61.0380, NLL-Loss   52.2519, KL-Loss   11.7255, KL-Weight  0.749\n",
      "TRAIN Batch 0200/1393, Loss   73.1337, NLL-Loss   63.4909, KL-Loss   12.4896, KL-Weight  0.772\n",
      "TRAIN Batch 0250/1393, Loss   67.3537, NLL-Loss   58.1911, KL-Loss   11.5498, KL-Weight  0.793\n",
      "TRAIN Batch 0300/1393, Loss   55.3018, NLL-Loss   46.4051, KL-Loss   10.9424, KL-Weight  0.813\n",
      "TRAIN Batch 0350/1393, Loss   70.8680, NLL-Loss   61.3623, KL-Loss   11.4345, KL-Weight  0.831\n",
      "TRAIN Batch 0400/1393, Loss   60.4142, NLL-Loss   51.9144, KL-Loss   10.0218, KL-Weight  0.848\n",
      "TRAIN Batch 0450/1393, Loss   65.1335, NLL-Loss   56.4445, KL-Loss   10.0620, KL-Weight  0.864\n",
      "TRAIN Batch 0500/1393, Loss   62.3911, NLL-Loss   54.5221, KL-Loss    8.9664, KL-Weight  0.878\n",
      "TRAIN Batch 0550/1393, Loss   62.9232, NLL-Loss   54.1636, KL-Loss    9.8376, KL-Weight  0.890\n",
      "TRAIN Batch 0600/1393, Loss   67.0411, NLL-Loss   59.0721, KL-Loss    8.8345, KL-Weight  0.902\n",
      "TRAIN Batch 0650/1393, Loss   63.0186, NLL-Loss   54.7038, KL-Loss    9.1117, KL-Weight  0.913\n",
      "TRAIN Batch 0700/1393, Loss   64.2573, NLL-Loss   56.2059, KL-Loss    8.7325, KL-Weight  0.922\n",
      "TRAIN Batch 0750/1393, Loss   73.3912, NLL-Loss   64.7450, KL-Loss    9.2917, KL-Weight  0.931\n",
      "TRAIN Batch 0800/1393, Loss   61.1862, NLL-Loss   52.1985, KL-Loss    9.5798, KL-Weight  0.938\n",
      "TRAIN Batch 0850/1393, Loss   62.2594, NLL-Loss   54.4291, KL-Loss    8.2855, KL-Weight  0.945\n",
      "TRAIN Batch 0900/1393, Loss   64.0969, NLL-Loss   55.3001, KL-Loss    9.2481, KL-Weight  0.951\n",
      "TRAIN Batch 0950/1393, Loss   61.4986, NLL-Loss   53.3356, KL-Loss    8.5326, KL-Weight  0.957\n",
      "TRAIN Batch 1000/1393, Loss   71.4835, NLL-Loss   63.0631, KL-Loss    8.7568, KL-Weight  0.962\n",
      "TRAIN Batch 1050/1393, Loss   64.0936, NLL-Loss   55.8087, KL-Loss    8.5771, KL-Weight  0.966\n",
      "TRAIN Batch 1100/1393, Loss   61.3989, NLL-Loss   53.1184, KL-Loss    8.5382, KL-Weight  0.970\n",
      "TRAIN Batch 1150/1393, Loss   60.2158, NLL-Loss   52.2289, KL-Loss    8.2062, KL-Weight  0.973\n",
      "TRAIN Batch 1200/1393, Loss   70.0677, NLL-Loss   62.1396, KL-Loss    8.1202, KL-Weight  0.976\n",
      "TRAIN Batch 1250/1393, Loss   73.0877, NLL-Loss   65.1286, KL-Loss    8.1293, KL-Weight  0.979\n",
      "TRAIN Batch 1300/1393, Loss   68.9697, NLL-Loss   61.6112, KL-Loss    7.4974, KL-Weight  0.981\n",
      "TRAIN Batch 1350/1393, Loss   64.7456, NLL-Loss   56.7763, KL-Loss    8.1020, KL-Weight  0.984\n",
      "TRAIN Batch 1393/1393, Loss   69.7449, NLL-Loss   61.7468, KL-Loss    8.1178, KL-Weight  0.985\n",
      "TRAIN Epoch 02/10, Mean Loss   64.6073\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/vae_eccos_v2_TS=2020-01-10-061630_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E2.pytorch\n",
      "VALID Batch 0000/77, Loss   62.7456, NLL-Loss   55.3936, KL-Loss    7.4617, KL-Weight  0.985\n",
      "VALID Batch 0050/77, Loss   72.0136, NLL-Loss   64.1692, KL-Loss    7.9614, KL-Weight  0.985\n",
      "VALID Batch 0077/77, Loss   58.5877, NLL-Loss   50.9082, KL-Loss    7.7941, KL-Weight  0.985\n",
      "VALID Epoch 02/10, Mean Loss   63.5810\n",
      "TRAIN Batch 0000/1393, Loss   56.0278, NLL-Loss   48.7747, KL-Loss    7.3613, KL-Weight  0.985\n",
      "TRAIN Batch 0050/1393, Loss   77.8766, NLL-Loss   69.4537, KL-Loss    8.5337, KL-Weight  0.987\n",
      "TRAIN Batch 0100/1393, Loss   72.4746, NLL-Loss   63.8504, KL-Loss    8.7244, KL-Weight  0.989\n",
      "TRAIN Batch 0150/1393, Loss   65.2456, NLL-Loss   57.0727, KL-Loss    8.2567, KL-Weight  0.990\n",
      "TRAIN Batch 0200/1393, Loss   58.1140, NLL-Loss   50.7272, KL-Loss    7.4537, KL-Weight  0.991\n",
      "TRAIN Batch 0250/1393, Loss   61.4518, NLL-Loss   54.1149, KL-Loss    7.3955, KL-Weight  0.992\n",
      "TRAIN Batch 0300/1393, Loss   60.3488, NLL-Loss   52.3488, KL-Loss    8.0564, KL-Weight  0.993\n",
      "TRAIN Batch 0350/1393, Loss   70.7821, NLL-Loss   62.4736, KL-Loss    8.3602, KL-Weight  0.994\n",
      "TRAIN Batch 0400/1393, Loss   58.9349, NLL-Loss   51.8566, KL-Loss    7.1171, KL-Weight  0.995\n",
      "TRAIN Batch 0450/1393, Loss   53.4659, NLL-Loss   46.6353, KL-Loss    6.8637, KL-Weight  0.995\n",
      "TRAIN Batch 0500/1393, Loss   57.9534, NLL-Loss   50.0218, KL-Loss    7.9654, KL-Weight  0.996\n",
      "TRAIN Batch 0550/1393, Loss   64.0680, NLL-Loss   56.4465, KL-Loss    7.6502, KL-Weight  0.996\n",
      "TRAIN Batch 0600/1393, Loss   61.9825, NLL-Loss   54.2828, KL-Loss    7.7253, KL-Weight  0.997\n",
      "TRAIN Batch 0650/1393, Loss   62.3843, NLL-Loss   54.4581, KL-Loss    7.9495, KL-Weight  0.997\n",
      "TRAIN Batch 0700/1393, Loss   65.2730, NLL-Loss   58.2171, KL-Loss    7.0743, KL-Weight  0.997\n",
      "TRAIN Batch 0750/1393, Loss   57.9967, NLL-Loss   50.7174, KL-Loss    7.2959, KL-Weight  0.998\n",
      "TRAIN Batch 0800/1393, Loss   66.2040, NLL-Loss   58.3361, KL-Loss    7.8838, KL-Weight  0.998\n",
      "TRAIN Batch 0850/1393, Loss   60.9966, NLL-Loss   53.3618, KL-Loss    7.6484, KL-Weight  0.998\n",
      "TRAIN Batch 0900/1393, Loss   69.3102, NLL-Loss   61.4904, KL-Loss    7.8320, KL-Weight  0.998\n",
      "TRAIN Batch 0950/1393, Loss   57.8126, NLL-Loss   50.6079, KL-Loss    7.2147, KL-Weight  0.999\n",
      "TRAIN Batch 1000/1393, Loss   60.7304, NLL-Loss   53.3296, KL-Loss    7.4098, KL-Weight  0.999\n",
      "TRAIN Batch 1050/1393, Loss   56.9875, NLL-Loss   49.2857, KL-Loss    7.7102, KL-Weight  0.999\n",
      "TRAIN Batch 1100/1393, Loss   62.6038, NLL-Loss   55.2675, KL-Loss    7.3433, KL-Weight  0.999\n",
      "TRAIN Batch 1150/1393, Loss   63.5423, NLL-Loss   56.0010, KL-Loss    7.5476, KL-Weight  0.999\n",
      "TRAIN Batch 1200/1393, Loss   63.3474, NLL-Loss   56.6555, KL-Loss    6.6968, KL-Weight  0.999\n",
      "TRAIN Batch 1250/1393, Loss   62.9496, NLL-Loss   55.1622, KL-Loss    7.7925, KL-Weight  0.999\n",
      "TRAIN Batch 1300/1393, Loss   70.7031, NLL-Loss   63.5786, KL-Loss    7.1287, KL-Weight  0.999\n",
      "TRAIN Batch 1350/1393, Loss   61.7468, NLL-Loss   54.3340, KL-Loss    7.4165, KL-Weight  0.999\n",
      "TRAIN Batch 1393/1393, Loss   66.5217, NLL-Loss   58.8726, KL-Loss    7.6525, KL-Weight  1.000\n",
      "TRAIN Epoch 03/10, Mean Loss   62.5164\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/vae_eccos_v2_TS=2020-01-10-061630_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E3.pytorch\n",
      "VALID Batch 0000/77, Loss   60.4398, NLL-Loss   53.7049, KL-Loss    6.7379, KL-Weight  1.000\n",
      "VALID Batch 0050/77, Loss   70.0616, NLL-Loss   62.6827, KL-Loss    7.3823, KL-Weight  1.000\n",
      "VALID Batch 0077/77, Loss   57.3108, NLL-Loss   50.0933, KL-Loss    7.2208, KL-Weight  1.000\n",
      "VALID Epoch 03/10, Mean Loss   62.0844\n",
      "TRAIN Batch 0000/1393, Loss   58.4958, NLL-Loss   51.2930, KL-Loss    7.2061, KL-Weight  1.000\n",
      "TRAIN Batch 0050/1393, Loss   60.7740, NLL-Loss   52.7934, KL-Loss    7.9839, KL-Weight  1.000\n",
      "TRAIN Batch 0100/1393, Loss   76.4557, NLL-Loss   67.9803, KL-Loss    8.4784, KL-Weight  1.000\n",
      "TRAIN Batch 0150/1393, Loss   57.5145, NLL-Loss   50.0564, KL-Loss    7.4604, KL-Weight  1.000\n",
      "TRAIN Batch 0200/1393, Loss   58.8529, NLL-Loss   51.0488, KL-Loss    7.8063, KL-Weight  1.000\n",
      "TRAIN Batch 0250/1393, Loss   56.7942, NLL-Loss   49.4843, KL-Loss    7.3117, KL-Weight  1.000\n",
      "TRAIN Batch 0300/1393, Loss   54.4819, NLL-Loss   47.7028, KL-Loss    6.7806, KL-Weight  1.000\n",
      "TRAIN Batch 0350/1393, Loss   70.6036, NLL-Loss   63.1017, KL-Loss    7.5033, KL-Weight  1.000\n",
      "TRAIN Batch 0400/1393, Loss   59.4712, NLL-Loss   51.4822, KL-Loss    7.9904, KL-Weight  1.000\n",
      "TRAIN Batch 0450/1393, Loss   55.9369, NLL-Loss   48.2717, KL-Loss    7.6663, KL-Weight  1.000\n",
      "TRAIN Batch 0500/1393, Loss   67.0267, NLL-Loss   59.6114, KL-Loss    7.4163, KL-Weight  1.000\n",
      "TRAIN Batch 0550/1393, Loss   64.5782, NLL-Loss   56.6822, KL-Loss    7.8969, KL-Weight  1.000\n",
      "TRAIN Batch 0600/1393, Loss   61.2024, NLL-Loss   53.7033, KL-Loss    7.4998, KL-Weight  1.000\n",
      "TRAIN Batch 0650/1393, Loss   57.8191, NLL-Loss   50.3473, KL-Loss    7.4724, KL-Weight  1.000\n",
      "TRAIN Batch 0700/1393, Loss   64.5776, NLL-Loss   57.3469, KL-Loss    7.2313, KL-Weight  1.000\n",
      "TRAIN Batch 0750/1393, Loss   55.9609, NLL-Loss   48.6792, KL-Loss    7.2822, KL-Weight  1.000\n",
      "TRAIN Batch 0800/1393, Loss   57.4624, NLL-Loss   50.0366, KL-Loss    7.4263, KL-Weight  1.000\n",
      "TRAIN Batch 0850/1393, Loss   60.5176, NLL-Loss   53.4774, KL-Loss    7.0406, KL-Weight  1.000\n",
      "TRAIN Batch 0900/1393, Loss   52.5074, NLL-Loss   45.3939, KL-Loss    7.1138, KL-Weight  1.000\n",
      "TRAIN Batch 0950/1393, Loss   58.5234, NLL-Loss   51.8837, KL-Loss    6.6399, KL-Weight  1.000\n",
      "TRAIN Batch 1000/1393, Loss   62.2684, NLL-Loss   55.4603, KL-Loss    6.8084, KL-Weight  1.000\n",
      "TRAIN Batch 1050/1393, Loss   50.7365, NLL-Loss   43.5612, KL-Loss    7.1755, KL-Weight  1.000\n",
      "TRAIN Batch 1100/1393, Loss   58.2675, NLL-Loss   51.5737, KL-Loss    6.6940, KL-Weight  1.000\n",
      "TRAIN Batch 1150/1393, Loss   63.7281, NLL-Loss   56.6424, KL-Loss    7.0858, KL-Weight  1.000\n",
      "TRAIN Batch 1200/1393, Loss   72.1727, NLL-Loss   64.8143, KL-Loss    7.3586, KL-Weight  1.000\n",
      "TRAIN Batch 1250/1393, Loss   64.9836, NLL-Loss   57.2480, KL-Loss    7.7358, KL-Weight  1.000\n",
      "TRAIN Batch 1300/1393, Loss   63.8542, NLL-Loss   56.5434, KL-Loss    7.3110, KL-Weight  1.000\n",
      "TRAIN Batch 1350/1393, Loss   70.9117, NLL-Loss   63.3593, KL-Loss    7.5525, KL-Weight  1.000\n",
      "TRAIN Batch 1393/1393, Loss   71.1160, NLL-Loss   63.8168, KL-Loss    7.2994, KL-Weight  1.000\n",
      "TRAIN Epoch 04/10, Mean Loss   60.4121\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/vae_eccos_v2_TS=2020-01-10-061630_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E4.pytorch\n",
      "VALID Batch 0000/77, Loss   60.2957, NLL-Loss   53.4066, KL-Loss    6.8892, KL-Weight  1.000\n",
      "VALID Batch 0050/77, Loss   70.0559, NLL-Loss   62.6213, KL-Loss    7.4347, KL-Weight  1.000\n",
      "VALID Batch 0077/77, Loss   57.5290, NLL-Loss   50.6258, KL-Loss    6.9033, KL-Weight  1.000\n",
      "VALID Epoch 04/10, Mean Loss   61.3294\n",
      "TRAIN Batch 0000/1393, Loss   56.3048, NLL-Loss   49.6650, KL-Loss    6.6399, KL-Weight  1.000\n",
      "TRAIN Batch 0050/1393, Loss   59.1063, NLL-Loss   51.8108, KL-Loss    7.2956, KL-Weight  1.000\n",
      "TRAIN Batch 0100/1393, Loss   56.0008, NLL-Loss   49.1479, KL-Loss    6.8529, KL-Weight  1.000\n",
      "TRAIN Batch 0150/1393, Loss   57.2116, NLL-Loss   50.0022, KL-Loss    7.2096, KL-Weight  1.000\n",
      "TRAIN Batch 0200/1393, Loss   53.1341, NLL-Loss   45.7177, KL-Loss    7.4165, KL-Weight  1.000\n",
      "TRAIN Batch 0250/1393, Loss   52.9764, NLL-Loss   46.6692, KL-Loss    6.3072, KL-Weight  1.000\n",
      "TRAIN Batch 0300/1393, Loss   60.9612, NLL-Loss   53.8935, KL-Loss    7.0678, KL-Weight  1.000\n",
      "TRAIN Batch 0350/1393, Loss   55.3657, NLL-Loss   47.9530, KL-Loss    7.4128, KL-Weight  1.000\n",
      "TRAIN Batch 0400/1393, Loss   55.2033, NLL-Loss   48.0029, KL-Loss    7.2004, KL-Weight  1.000\n",
      "TRAIN Batch 0450/1393, Loss   59.1850, NLL-Loss   52.0604, KL-Loss    7.1247, KL-Weight  1.000\n",
      "TRAIN Batch 0500/1393, Loss   49.8655, NLL-Loss   43.0956, KL-Loss    6.7699, KL-Weight  1.000\n",
      "TRAIN Batch 0550/1393, Loss   59.4896, NLL-Loss   52.6725, KL-Loss    6.8171, KL-Weight  1.000\n",
      "TRAIN Batch 0600/1393, Loss   61.4205, NLL-Loss   54.3969, KL-Loss    7.0236, KL-Weight  1.000\n",
      "TRAIN Batch 0650/1393, Loss   57.1596, NLL-Loss   50.1649, KL-Loss    6.9948, KL-Weight  1.000\n",
      "TRAIN Batch 0700/1393, Loss   61.5285, NLL-Loss   54.3647, KL-Loss    7.1639, KL-Weight  1.000\n",
      "TRAIN Batch 0750/1393, Loss   62.6607, NLL-Loss   55.0631, KL-Loss    7.5976, KL-Weight  1.000\n",
      "TRAIN Batch 0800/1393, Loss   59.1202, NLL-Loss   51.8206, KL-Loss    7.2996, KL-Weight  1.000\n",
      "TRAIN Batch 0850/1393, Loss   56.1867, NLL-Loss   49.4177, KL-Loss    6.7690, KL-Weight  1.000\n",
      "TRAIN Batch 0900/1393, Loss   67.2557, NLL-Loss   60.1960, KL-Loss    7.0597, KL-Weight  1.000\n",
      "TRAIN Batch 0950/1393, Loss   52.7804, NLL-Loss   45.9045, KL-Loss    6.8759, KL-Weight  1.000\n",
      "TRAIN Batch 1000/1393, Loss   51.4744, NLL-Loss   44.4087, KL-Loss    7.0657, KL-Weight  1.000\n",
      "TRAIN Batch 1050/1393, Loss   54.1052, NLL-Loss   47.6345, KL-Loss    6.4707, KL-Weight  1.000\n",
      "TRAIN Batch 1100/1393, Loss   62.0547, NLL-Loss   54.8639, KL-Loss    7.1908, KL-Weight  1.000\n",
      "TRAIN Batch 1150/1393, Loss   61.0355, NLL-Loss   54.0724, KL-Loss    6.9632, KL-Weight  1.000\n",
      "TRAIN Batch 1200/1393, Loss   63.4520, NLL-Loss   56.8670, KL-Loss    6.5850, KL-Weight  1.000\n",
      "TRAIN Batch 1250/1393, Loss   58.5634, NLL-Loss   51.6324, KL-Loss    6.9310, KL-Weight  1.000\n",
      "TRAIN Batch 1300/1393, Loss   61.0481, NLL-Loss   54.4956, KL-Loss    6.5525, KL-Weight  1.000\n",
      "TRAIN Batch 1350/1393, Loss   59.5615, NLL-Loss   52.6900, KL-Loss    6.8715, KL-Weight  1.000\n",
      "TRAIN Batch 1393/1393, Loss   61.0055, NLL-Loss   53.6868, KL-Loss    7.3188, KL-Weight  1.000\n",
      "TRAIN Epoch 05/10, Mean Loss   58.8377\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/vae_eccos_v2_TS=2020-01-10-061630_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E5.pytorch\n",
      "VALID Batch 0000/77, Loss   59.8765, NLL-Loss   53.1599, KL-Loss    6.7167, KL-Weight  1.000\n",
      "VALID Batch 0050/77, Loss   71.2093, NLL-Loss   63.8022, KL-Loss    7.4071, KL-Weight  1.000\n",
      "VALID Batch 0077/77, Loss   57.1747, NLL-Loss   50.3428, KL-Loss    6.8319, KL-Weight  1.000\n",
      "VALID Epoch 05/10, Mean Loss   60.5503\n",
      "TRAIN Batch 0000/1393, Loss   59.4080, NLL-Loss   52.2149, KL-Loss    7.1931, KL-Weight  1.000\n",
      "TRAIN Batch 0050/1393, Loss   60.8958, NLL-Loss   53.5189, KL-Loss    7.3770, KL-Weight  1.000\n",
      "TRAIN Batch 0100/1393, Loss   55.2826, NLL-Loss   47.9081, KL-Loss    7.3745, KL-Weight  1.000\n",
      "TRAIN Batch 0150/1393, Loss   52.6466, NLL-Loss   45.2383, KL-Loss    7.4083, KL-Weight  1.000\n",
      "TRAIN Batch 0200/1393, Loss   53.5165, NLL-Loss   46.7955, KL-Loss    6.7211, KL-Weight  1.000\n",
      "TRAIN Batch 0250/1393, Loss   51.2208, NLL-Loss   44.7413, KL-Loss    6.4795, KL-Weight  1.000\n",
      "TRAIN Batch 0300/1393, Loss   54.7576, NLL-Loss   47.8374, KL-Loss    6.9202, KL-Weight  1.000\n",
      "TRAIN Batch 0350/1393, Loss   57.6430, NLL-Loss   50.6570, KL-Loss    6.9859, KL-Weight  1.000\n",
      "TRAIN Batch 0400/1393, Loss   61.8423, NLL-Loss   55.1127, KL-Loss    6.7296, KL-Weight  1.000\n",
      "TRAIN Batch 0450/1393, Loss   57.3516, NLL-Loss   50.2547, KL-Loss    7.0969, KL-Weight  1.000\n",
      "TRAIN Batch 0500/1393, Loss   56.7838, NLL-Loss   50.0519, KL-Loss    6.7320, KL-Weight  1.000\n",
      "TRAIN Batch 0550/1393, Loss   54.2010, NLL-Loss   46.8855, KL-Loss    7.3156, KL-Weight  1.000\n",
      "TRAIN Batch 0600/1393, Loss   57.5943, NLL-Loss   50.6453, KL-Loss    6.9490, KL-Weight  1.000\n",
      "TRAIN Batch 0650/1393, Loss   64.8432, NLL-Loss   57.8597, KL-Loss    6.9834, KL-Weight  1.000\n",
      "TRAIN Batch 0700/1393, Loss   60.0498, NLL-Loss   53.2849, KL-Loss    6.7649, KL-Weight  1.000\n",
      "TRAIN Batch 0750/1393, Loss   59.0304, NLL-Loss   52.0321, KL-Loss    6.9983, KL-Weight  1.000\n",
      "TRAIN Batch 0800/1393, Loss   59.0267, NLL-Loss   52.2038, KL-Loss    6.8228, KL-Weight  1.000\n",
      "TRAIN Batch 0850/1393, Loss   56.6181, NLL-Loss   49.4345, KL-Loss    7.1836, KL-Weight  1.000\n",
      "TRAIN Batch 0900/1393, Loss   54.9045, NLL-Loss   47.6908, KL-Loss    7.2137, KL-Weight  1.000\n",
      "TRAIN Batch 0950/1393, Loss   58.3338, NLL-Loss   51.1923, KL-Loss    7.1415, KL-Weight  1.000\n",
      "TRAIN Batch 1000/1393, Loss   51.5943, NLL-Loss   45.2034, KL-Loss    6.3909, KL-Weight  1.000\n",
      "TRAIN Batch 1050/1393, Loss   52.8887, NLL-Loss   46.2243, KL-Loss    6.6644, KL-Weight  1.000\n",
      "TRAIN Batch 1100/1393, Loss   61.1969, NLL-Loss   54.2196, KL-Loss    6.9774, KL-Weight  1.000\n",
      "TRAIN Batch 1150/1393, Loss   54.7865, NLL-Loss   48.2514, KL-Loss    6.5352, KL-Weight  1.000\n",
      "TRAIN Batch 1200/1393, Loss   54.4220, NLL-Loss   47.8196, KL-Loss    6.6024, KL-Weight  1.000\n",
      "TRAIN Batch 1250/1393, Loss   60.6705, NLL-Loss   53.0816, KL-Loss    7.5889, KL-Weight  1.000\n",
      "TRAIN Batch 1300/1393, Loss   59.7321, NLL-Loss   53.0228, KL-Loss    6.7093, KL-Weight  1.000\n",
      "TRAIN Batch 1350/1393, Loss   56.2048, NLL-Loss   48.6434, KL-Loss    7.5614, KL-Weight  1.000\n",
      "TRAIN Batch 1393/1393, Loss   53.5108, NLL-Loss   47.2060, KL-Loss    6.3049, KL-Weight  1.000\n",
      "TRAIN Epoch 06/10, Mean Loss   57.5083\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/vae_eccos_v2_TS=2020-01-10-061630_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E6.pytorch\n",
      "VALID Batch 0000/77, Loss   59.4111, NLL-Loss   53.0870, KL-Loss    6.3241, KL-Weight  1.000\n",
      "VALID Batch 0050/77, Loss   70.5990, NLL-Loss   63.7981, KL-Loss    6.8009, KL-Weight  1.000\n",
      "VALID Batch 0077/77, Loss   55.3365, NLL-Loss   48.6307, KL-Loss    6.7057, KL-Weight  1.000\n",
      "VALID Epoch 06/10, Mean Loss   60.0610\n",
      "TRAIN Batch 0000/1393, Loss   56.7895, NLL-Loss   50.4362, KL-Loss    6.3533, KL-Weight  1.000\n",
      "TRAIN Batch 0050/1393, Loss   55.0410, NLL-Loss   47.9562, KL-Loss    7.0849, KL-Weight  1.000\n",
      "TRAIN Batch 0100/1393, Loss   57.0171, NLL-Loss   49.6599, KL-Loss    7.3572, KL-Weight  1.000\n",
      "TRAIN Batch 0150/1393, Loss   54.0970, NLL-Loss   47.0980, KL-Loss    6.9990, KL-Weight  1.000\n",
      "TRAIN Batch 0200/1393, Loss   54.7541, NLL-Loss   47.2829, KL-Loss    7.4712, KL-Weight  1.000\n",
      "TRAIN Batch 0250/1393, Loss   56.2804, NLL-Loss   49.0856, KL-Loss    7.1948, KL-Weight  1.000\n",
      "TRAIN Batch 0300/1393, Loss   60.4295, NLL-Loss   52.9345, KL-Loss    7.4951, KL-Weight  1.000\n",
      "TRAIN Batch 0350/1393, Loss   47.6863, NLL-Loss   40.5731, KL-Loss    7.1132, KL-Weight  1.000\n",
      "TRAIN Batch 0400/1393, Loss   53.2969, NLL-Loss   46.5582, KL-Loss    6.7387, KL-Weight  1.000\n",
      "TRAIN Batch 0450/1393, Loss   51.8094, NLL-Loss   45.0973, KL-Loss    6.7121, KL-Weight  1.000\n",
      "TRAIN Batch 0500/1393, Loss   59.2778, NLL-Loss   52.1229, KL-Loss    7.1550, KL-Weight  1.000\n",
      "TRAIN Batch 0550/1393, Loss   52.9177, NLL-Loss   46.5424, KL-Loss    6.3753, KL-Weight  1.000\n",
      "TRAIN Batch 0600/1393, Loss   61.8676, NLL-Loss   54.7206, KL-Loss    7.1470, KL-Weight  1.000\n",
      "TRAIN Batch 0650/1393, Loss   55.0838, NLL-Loss   48.1923, KL-Loss    6.8915, KL-Weight  1.000\n",
      "TRAIN Batch 0700/1393, Loss   57.0223, NLL-Loss   49.4520, KL-Loss    7.5703, KL-Weight  1.000\n",
      "TRAIN Batch 0750/1393, Loss   49.8192, NLL-Loss   43.0489, KL-Loss    6.7703, KL-Weight  1.000\n",
      "TRAIN Batch 0800/1393, Loss   56.4598, NLL-Loss   49.4890, KL-Loss    6.9708, KL-Weight  1.000\n",
      "TRAIN Batch 0850/1393, Loss   54.6592, NLL-Loss   47.6102, KL-Loss    7.0490, KL-Weight  1.000\n",
      "TRAIN Batch 0900/1393, Loss   61.2347, NLL-Loss   54.3856, KL-Loss    6.8490, KL-Weight  1.000\n",
      "TRAIN Batch 0950/1393, Loss   53.8348, NLL-Loss   46.9661, KL-Loss    6.8686, KL-Weight  1.000\n",
      "TRAIN Batch 1000/1393, Loss   59.1309, NLL-Loss   52.4992, KL-Loss    6.6317, KL-Weight  1.000\n",
      "TRAIN Batch 1050/1393, Loss   56.2419, NLL-Loss   49.2239, KL-Loss    7.0180, KL-Weight  1.000\n",
      "TRAIN Batch 1100/1393, Loss   55.8889, NLL-Loss   49.4884, KL-Loss    6.4005, KL-Weight  1.000\n",
      "TRAIN Batch 1150/1393, Loss   53.6572, NLL-Loss   46.6425, KL-Loss    7.0148, KL-Weight  1.000\n",
      "TRAIN Batch 1200/1393, Loss   53.5654, NLL-Loss   46.6338, KL-Loss    6.9315, KL-Weight  1.000\n",
      "TRAIN Batch 1250/1393, Loss   56.5484, NLL-Loss   49.8982, KL-Loss    6.6502, KL-Weight  1.000\n",
      "TRAIN Batch 1300/1393, Loss   47.4789, NLL-Loss   40.9838, KL-Loss    6.4950, KL-Weight  1.000\n",
      "TRAIN Batch 1350/1393, Loss   55.4831, NLL-Loss   48.7417, KL-Loss    6.7413, KL-Weight  1.000\n",
      "TRAIN Batch 1393/1393, Loss   60.7376, NLL-Loss   53.9861, KL-Loss    6.7515, KL-Weight  1.000\n",
      "TRAIN Epoch 07/10, Mean Loss   56.4285\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/vae_eccos_v2_TS=2020-01-10-061630_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E7.pytorch\n",
      "VALID Batch 0000/77, Loss   59.9657, NLL-Loss   53.5006, KL-Loss    6.4651, KL-Weight  1.000\n",
      "VALID Batch 0050/77, Loss   70.2009, NLL-Loss   63.1819, KL-Loss    7.0190, KL-Weight  1.000\n",
      "VALID Batch 0077/77, Loss   55.3839, NLL-Loss   48.9023, KL-Loss    6.4817, KL-Weight  1.000\n",
      "VALID Epoch 07/10, Mean Loss   59.8817\n",
      "TRAIN Batch 0000/1393, Loss   49.4191, NLL-Loss   42.5023, KL-Loss    6.9168, KL-Weight  1.000\n",
      "TRAIN Batch 0050/1393, Loss   54.8633, NLL-Loss   48.1714, KL-Loss    6.6919, KL-Weight  1.000\n",
      "TRAIN Batch 0100/1393, Loss   52.4497, NLL-Loss   45.4700, KL-Loss    6.9797, KL-Weight  1.000\n",
      "TRAIN Batch 0150/1393, Loss   54.6961, NLL-Loss   47.3475, KL-Loss    7.3486, KL-Weight  1.000\n",
      "TRAIN Batch 0200/1393, Loss   58.2557, NLL-Loss   51.3817, KL-Loss    6.8740, KL-Weight  1.000\n",
      "TRAIN Batch 0250/1393, Loss   58.0476, NLL-Loss   51.3991, KL-Loss    6.6485, KL-Weight  1.000\n",
      "TRAIN Batch 0300/1393, Loss   51.4897, NLL-Loss   44.6409, KL-Loss    6.8488, KL-Weight  1.000\n",
      "TRAIN Batch 0350/1393, Loss   57.2510, NLL-Loss   50.2458, KL-Loss    7.0052, KL-Weight  1.000\n",
      "TRAIN Batch 0400/1393, Loss   51.6809, NLL-Loss   44.6401, KL-Loss    7.0408, KL-Weight  1.000\n",
      "TRAIN Batch 0450/1393, Loss   56.0997, NLL-Loss   49.9142, KL-Loss    6.1855, KL-Weight  1.000\n",
      "TRAIN Batch 0500/1393, Loss   54.2876, NLL-Loss   47.2181, KL-Loss    7.0695, KL-Weight  1.000\n",
      "TRAIN Batch 0550/1393, Loss   54.6077, NLL-Loss   47.8529, KL-Loss    6.7548, KL-Weight  1.000\n",
      "TRAIN Batch 0600/1393, Loss   53.8330, NLL-Loss   46.8530, KL-Loss    6.9800, KL-Weight  1.000\n",
      "TRAIN Batch 0650/1393, Loss   50.1380, NLL-Loss   43.6816, KL-Loss    6.4564, KL-Weight  1.000\n",
      "TRAIN Batch 0700/1393, Loss   51.0060, NLL-Loss   44.2805, KL-Loss    6.7255, KL-Weight  1.000\n",
      "TRAIN Batch 0750/1393, Loss   53.0301, NLL-Loss   46.0565, KL-Loss    6.9736, KL-Weight  1.000\n",
      "TRAIN Batch 0800/1393, Loss   53.5574, NLL-Loss   46.7659, KL-Loss    6.7915, KL-Weight  1.000\n",
      "TRAIN Batch 0850/1393, Loss   54.6539, NLL-Loss   47.4218, KL-Loss    7.2321, KL-Weight  1.000\n",
      "TRAIN Batch 0900/1393, Loss   53.0130, NLL-Loss   45.9443, KL-Loss    7.0688, KL-Weight  1.000\n",
      "TRAIN Batch 0950/1393, Loss   47.8778, NLL-Loss   40.8185, KL-Loss    7.0593, KL-Weight  1.000\n",
      "TRAIN Batch 1000/1393, Loss   56.3299, NLL-Loss   49.2890, KL-Loss    7.0409, KL-Weight  1.000\n",
      "TRAIN Batch 1050/1393, Loss   57.3554, NLL-Loss   49.8791, KL-Loss    7.4763, KL-Weight  1.000\n",
      "TRAIN Batch 1100/1393, Loss   60.0573, NLL-Loss   53.1634, KL-Loss    6.8940, KL-Weight  1.000\n",
      "TRAIN Batch 1150/1393, Loss   52.3071, NLL-Loss   45.5286, KL-Loss    6.7785, KL-Weight  1.000\n",
      "TRAIN Batch 1200/1393, Loss   63.6416, NLL-Loss   55.6968, KL-Loss    7.9448, KL-Weight  1.000\n",
      "TRAIN Batch 1250/1393, Loss   56.7685, NLL-Loss   50.0048, KL-Loss    6.7637, KL-Weight  1.000\n",
      "TRAIN Batch 1300/1393, Loss   54.8060, NLL-Loss   47.5985, KL-Loss    7.2075, KL-Weight  1.000\n",
      "TRAIN Batch 1350/1393, Loss   62.8517, NLL-Loss   55.8730, KL-Loss    6.9787, KL-Weight  1.000\n",
      "TRAIN Batch 1393/1393, Loss   62.2551, NLL-Loss   55.0559, KL-Loss    7.1992, KL-Weight  1.000\n",
      "TRAIN Epoch 08/10, Mean Loss   55.5227\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/vae_eccos_v2_TS=2020-01-10-061630_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E8.pytorch\n",
      "VALID Batch 0000/77, Loss   59.3861, NLL-Loss   53.0788, KL-Loss    6.3073, KL-Weight  1.000\n",
      "VALID Batch 0050/77, Loss   69.7651, NLL-Loss   62.8700, KL-Loss    6.8951, KL-Weight  1.000\n",
      "VALID Batch 0077/77, Loss   53.6811, NLL-Loss   47.2830, KL-Loss    6.3980, KL-Weight  1.000\n",
      "VALID Epoch 08/10, Mean Loss   59.5029\n",
      "TRAIN Batch 0000/1393, Loss   59.6977, NLL-Loss   52.3640, KL-Loss    7.3337, KL-Weight  1.000\n",
      "TRAIN Batch 0050/1393, Loss   49.1130, NLL-Loss   42.8269, KL-Loss    6.2861, KL-Weight  1.000\n",
      "TRAIN Batch 0100/1393, Loss   53.7155, NLL-Loss   47.1749, KL-Loss    6.5406, KL-Weight  1.000\n",
      "TRAIN Batch 0150/1393, Loss   51.2189, NLL-Loss   44.1107, KL-Loss    7.1083, KL-Weight  1.000\n",
      "TRAIN Batch 0200/1393, Loss   59.0796, NLL-Loss   51.7738, KL-Loss    7.3058, KL-Weight  1.000\n",
      "TRAIN Batch 0250/1393, Loss   58.8977, NLL-Loss   51.6406, KL-Loss    7.2570, KL-Weight  1.000\n",
      "TRAIN Batch 0300/1393, Loss   49.7446, NLL-Loss   43.0774, KL-Loss    6.6671, KL-Weight  1.000\n",
      "TRAIN Batch 0350/1393, Loss   49.0258, NLL-Loss   41.9370, KL-Loss    7.0888, KL-Weight  1.000\n",
      "TRAIN Batch 0400/1393, Loss   55.8455, NLL-Loss   49.3119, KL-Loss    6.5336, KL-Weight  1.000\n",
      "TRAIN Batch 0450/1393, Loss   53.9532, NLL-Loss   46.8593, KL-Loss    7.0939, KL-Weight  1.000\n",
      "TRAIN Batch 0500/1393, Loss   57.6244, NLL-Loss   50.5967, KL-Loss    7.0276, KL-Weight  1.000\n",
      "TRAIN Batch 0550/1393, Loss   54.1829, NLL-Loss   47.1526, KL-Loss    7.0303, KL-Weight  1.000\n",
      "TRAIN Batch 0600/1393, Loss   52.8838, NLL-Loss   46.2814, KL-Loss    6.6024, KL-Weight  1.000\n",
      "TRAIN Batch 0650/1393, Loss   48.0360, NLL-Loss   40.6429, KL-Loss    7.3931, KL-Weight  1.000\n",
      "TRAIN Batch 0700/1393, Loss   62.1656, NLL-Loss   55.3865, KL-Loss    6.7790, KL-Weight  1.000\n",
      "TRAIN Batch 0750/1393, Loss   54.3117, NLL-Loss   47.4682, KL-Loss    6.8436, KL-Weight  1.000\n",
      "TRAIN Batch 0800/1393, Loss   46.7079, NLL-Loss   39.9156, KL-Loss    6.7923, KL-Weight  1.000\n",
      "TRAIN Batch 0850/1393, Loss   55.2682, NLL-Loss   47.8844, KL-Loss    7.3838, KL-Weight  1.000\n",
      "TRAIN Batch 0900/1393, Loss   50.8891, NLL-Loss   44.0696, KL-Loss    6.8194, KL-Weight  1.000\n",
      "TRAIN Batch 0950/1393, Loss   63.5870, NLL-Loss   56.2694, KL-Loss    7.3177, KL-Weight  1.000\n",
      "TRAIN Batch 1000/1393, Loss   54.1250, NLL-Loss   46.8980, KL-Loss    7.2270, KL-Weight  1.000\n",
      "TRAIN Batch 1050/1393, Loss   54.9426, NLL-Loss   47.7232, KL-Loss    7.2193, KL-Weight  1.000\n",
      "TRAIN Batch 1100/1393, Loss   58.0513, NLL-Loss   50.6720, KL-Loss    7.3793, KL-Weight  1.000\n",
      "TRAIN Batch 1150/1393, Loss   56.6108, NLL-Loss   49.9284, KL-Loss    6.6824, KL-Weight  1.000\n",
      "TRAIN Batch 1200/1393, Loss   54.0060, NLL-Loss   46.6619, KL-Loss    7.3441, KL-Weight  1.000\n",
      "TRAIN Batch 1250/1393, Loss   54.7521, NLL-Loss   47.6753, KL-Loss    7.0769, KL-Weight  1.000\n",
      "TRAIN Batch 1300/1393, Loss   60.0837, NLL-Loss   52.9218, KL-Loss    7.1618, KL-Weight  1.000\n",
      "TRAIN Batch 1350/1393, Loss   56.0639, NLL-Loss   48.9367, KL-Loss    7.1272, KL-Weight  1.000\n",
      "TRAIN Batch 1393/1393, Loss   62.9138, NLL-Loss   55.5439, KL-Loss    7.3699, KL-Weight  1.000\n",
      "TRAIN Epoch 09/10, Mean Loss   54.6903\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/vae_eccos_v2_TS=2020-01-10-061630_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E9.pytorch\n",
      "VALID Batch 0000/77, Loss   58.2215, NLL-Loss   51.7192, KL-Loss    6.5023, KL-Weight  1.000\n",
      "VALID Batch 0050/77, Loss   68.9251, NLL-Loss   61.9540, KL-Loss    6.9711, KL-Weight  1.000\n",
      "VALID Batch 0077/77, Loss   54.9699, NLL-Loss   48.5427, KL-Loss    6.4271, KL-Weight  1.000\n",
      "VALID Epoch 09/10, Mean Loss   59.3169\n"
     ]
    }
   ],
   "source": [
    "# %pdb on\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    for split in splits:\n",
    "        \n",
    "        data_loader = DataLoader(\n",
    "            dataset=_datasets[split],\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=split=='train',\n",
    "            num_workers=cpu_count(),\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "\n",
    "        tracker = defaultdict(Tensor)\n",
    "\n",
    "        # Enable/Disable Dropout\n",
    "        if split == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        for iteration, batch in enumerate(data_loader):\n",
    "            \n",
    "            batch_size = batch['src_input'].size(0)\n",
    "            \n",
    "            for k, v in batch.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    batch[k] = to_var(v)\n",
    "            \n",
    "            # loss calculation\n",
    "            # kw → copy の場合\n",
    "            # cal_dict = model(batch['src_input'], batch['src_length'], batch['tgt_input'], batch['tgt_length'])\n",
    "            cal_dict = model(batch['src_input'], batch['src_length'])\n",
    "            logp, mean, logv, z = cal_dict['logp'], cal_dict['mean'], cal_dict['logv'], cal_dict['z']\n",
    "            \n",
    "            loss_dict = model.loss(logp, batch['tgt_target'], batch['tgt_length'], mean, logv, args.anneal_function, step, args.k, args.x0, bow_input=z)\n",
    "            loss, NLL_loss, KL_weight, KL_loss, avg_bow_loss = loss_dict['loss'], loss_dict['NLL_loss'], loss_dict['KL_weight'], loss_dict['KL_loss'], loss_dict.get('avg_bow_loss')\n",
    "\n",
    "            # backward + optimization\n",
    "            if split == 'train':\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                step += 1\n",
    "\n",
    "            # bookkeepeing\n",
    "            tracker['Loss'] = torch.cat((tracker['Loss'], loss.detach().view(1)))\n",
    "\n",
    "            if args.tensorboard_logging:\n",
    "                writer.add_scalar(\"%s/Loss\"%split.upper(), loss.detach().item(), epoch*len(data_loader) + iteration)\n",
    "                writer.add_scalar(\"%s/TEXT_NLL_Loss\"%split.upper(), NLL_loss.detach().item()/batch_size, epoch*len(data_loader) + iteration)\n",
    "                writer.add_scalar(\"%s/KL Loss\"%split.upper(), KL_loss.detach().item()/batch_size, epoch*len(data_loader) + iteration)\n",
    "                writer.add_scalar(\"%s/KL Weight\"%split.upper(), KL_weight, epoch*len(data_loader) + iteration)\n",
    "                if avg_bow_loss is not None:\n",
    "                    writer.add_scalar(\"%s/BOW Loss\"%split.upper(), avg_bow_loss, epoch*len(data_loader) + iteration)\n",
    "\n",
    "            if iteration % args.print_every == 0 or iteration+1 == len(data_loader):\n",
    "                print_text = \"%s Batch %04d/%i, Loss %9.4f, NLL-Loss %9.4f, KL-Loss %9.4f, KL-Weight %6.3f\"%(split.upper(), iteration, len(data_loader)-1, loss.detach().item(), NLL_loss.detach().item()/batch_size, KL_loss.detach().item()/batch_size, KL_weight)\n",
    "                if avg_bow_loss is not None:\n",
    "                    print_text += ', BOW Loss %9.4f,'%(avg_bow_loss)\n",
    "                print(print_text)\n",
    "\n",
    "            if split == 'valid':\n",
    "                tracker['target_ids'] = torch.cat((tracker.get('target_ids', LongTensor()), batch['tgt_target'].detach()), dim=0)\n",
    "                tracker['target_sents'] = tracker.get('target_sents', []) + [ids2ptext(text_ids, train_target_ptb.i2w) for text_ids in batch['tgt_target'].data]\n",
    "                tracker['z'] = torch.cat((tracker['z'], z.detach()), dim=0)\n",
    "                with torch.no_grad():\n",
    "                    # Single Inference\n",
    "                    decoded_ids, _ = model.inference(z=z)\n",
    "                    tracker['decoded_ids'] = torch.cat((tracker.get('decoded_ids', LongTensor()), decoded_ids.detach()), dim=0)\n",
    "                    tracker['decoded_sents'] = tracker.get('decoded_sents', []) + [ids2ptext(text_ids, train_target_ptb.i2w) for text_ids in decoded_ids]\n",
    "                    \n",
    "                    # Multiple Inference\n",
    "                    z_list = sample_z(mean, logv, n=10)\n",
    "                    z_list = z_list.permute(1, 0, 2) # バッチ数, サンプル数, 次元数\n",
    "                    text_decoded_ids = [model.inference(z=_z)[0].tolist() for _z in z_list]\n",
    "                    tracker['multi_text_decoded_ids'] = tracker.get('multi_text_decoded_ids', []) + text_decoded_ids\n",
    "                    \n",
    "\n",
    "        print(\"%s Epoch %02d/%i, Mean Loss %9.4f\"%(split.upper(), epoch, args.epochs, torch.mean(tracker['Loss'])))\n",
    "\n",
    "        if args.tensorboard_logging:\n",
    "            writer.add_scalar(\"%s-Epoch/Loss\"%split.upper(), torch.mean(tracker['Loss']), epoch)\n",
    "        \n",
    "        if split == 'valid':\n",
    "            decoded_id_list = remove_pad_index(tracker['decoded_ids'])\n",
    "            multi_decoded_id_list = remove_pad_index(tracker['multi_text_decoded_ids'])\n",
    "            valid_tgt_id_list = remove_pad_index(tracker['target_ids'])\n",
    "            train_tgt_id_list = remove_pad_index([d['tgt_target'] for d in _datasets['train']]) # コピー率用\n",
    "            write_tensorboard_valid_metric(writer, valid_tgt_id_list, decoded_id_list, multi_decoded_id_list, train_tgt_id_list, train_target_ptb.i2w, split, epoch)\n",
    "\n",
    "        # save checkpoint\n",
    "        if split == 'train':\n",
    "            checkpoint_path = os.path.join(save_model_path, f\"model_E{epoch}.pytorch\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(\"Model saved at %s\"%checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
