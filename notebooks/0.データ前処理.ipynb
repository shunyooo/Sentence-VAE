{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# コピー文 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", 500)\n",
    "pd.set_option(\"display.max_rows\", 101)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://wakimoto-ramiel/experiments/exp_20191220_kw2title/table/cosme.tsv...\n",
      "- [1 files][ 67.4 MiB/ 67.4 MiB]                                                \n",
      "Operation completed over 1 objects/67.4 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "gs_table_url = 'gs://wakimoto-ramiel/experiments/exp_20191220_kw2title/table/cosme.tsv'\n",
    "data_path = './data/tmp/cosme.tsv'\n",
    "! gsutil cp $gs_table_url $data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単体テスト用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "def unit_test(test_case_list):\n",
    "    # 単体テスト. in/outが1変数のやつしか対応していない\n",
    "    def _unit_test(func):\n",
    "        for test_case in test_case_list:\n",
    "            _in, _out, _comment = None, None, None\n",
    "            if len(test_case) == 2:\n",
    "                _in, _out = test_case\n",
    "            elif len(test_case) == 3:\n",
    "                _in, _out, _comment = test_case\n",
    "            else:\n",
    "                raise Exception(f'{test_case}の長さは2か3であるべきです. テストケースを書き直してください.')\n",
    "            res = func(_in)\n",
    "            assert res == _out, f'''\n",
    "Error :{_comment}\n",
    "Input :{_in}\n",
    "Label :{_out}\n",
    "Output:{res}\n",
    "'''\n",
    "        logging.info(f'【unit_test】 {func.__name__} は{len(test_case_list)}件のテストに通りました.')\n",
    "        return func\n",
    "    return _unit_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mecab 前処理\n",
    "- URL除去\n",
    "- ハッシュタグ除去\n",
    "- コロン削除： 30,000等の形態素解析でノイズなので\n",
    "- 文末, 文頭の空白削除\n",
    "- 文中の全角空白は「。」にする\n",
    "- カタカナ記号は全角で統一. 数字,記号の正規化\n",
    "- *1 とかの注釈マークを除去\n",
    "- TODO: 顔文字をトークンとしてみなす"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_uq_text[df_uq_text.text.str.contains('　')].sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:【unit_test】 remove_notes_mark は4件のテストに通りました.\n",
      "INFO:root:【unit_test】 normalize は4件のテストに通りました.\n",
      "INFO:root:【unit_test】 replace_ja_period_sep は2件のテストに通りました.\n",
      "INFO:root:【unit_test】 replace_fwspace_ja_period は3件のテストに通りました.\n",
      "INFO:root:【unit_test】 remove_coron は1件のテストに通りました.\n",
      "INFO:root:【unit_test】 remove_head_tail_space は6件のテストに通りました.\n",
      "INFO:root:【unit_test】 remove_url は3件のテストに通りました.\n",
      "INFO:root:【unit_test】 remove_hash は1件のテストに通りました.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import jaconv\n",
    "SEP_TAG = '<sep>'\n",
    "JA_PERIOD = '。'\n",
    "unit_test_success_log = {}\n",
    "\n",
    "def is_valid_sentence(text):\n",
    "    return bool(text)\n",
    "\n",
    "def before_tagger_processing(text):\n",
    "    # 形態素解析にかける前に行う前処理\n",
    "    # 空白除去など\n",
    "    \n",
    "    # URL削除\n",
    "    text = remove_url(text)\n",
    "    \n",
    "    # ハッシュタグ削除\n",
    "    text = remove_hash(text)\n",
    "    \n",
    "    # コロン削除\n",
    "    # - 本当は数詞内のみ削除, 他は<sep>にしたい\n",
    "    text = remove_coron(text)\n",
    "    \n",
    "    # 文末, 文頭の空白削除\n",
    "    text = remove_head_tail_space(text)\n",
    "    \n",
    "    # 文中の全角空白は「。」にする\n",
    "    text = replace_fwspace_ja_period(text)\n",
    "    \n",
    "    # ----- カタカナ記号は全角で統一. 数字,記号の正規化 ---------\n",
    "    text = normalize(text)\n",
    "    text = re.sub('\\s', '', text)\n",
    "    \n",
    "    # 「。!?」は<sep>をつける：これはmecab後のやつ\n",
    "    # text = replace_ja_period_sep(text)\n",
    "    \n",
    "    # *1 とかの注釈マークを除去\n",
    "    text = remove_notes_mark(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "@unit_test([\n",
    "    ('7日間のエイジングケア(*1)で集中アプローチ。高濃度イモーテル(*2)の力で素肌にハリ*1。', '7日間のエイジングケアで集中アプローチ。高濃度イモーテルの力で素肌にハリ。'),\n",
    "    ('ああ*1これ*2ああ(*2)でも(*)だけど(*3)', 'ああこれああでもだけど'),\n",
    "    ('ああ※1これ※2ああ(※2)でも(※)だけど(※3)', 'ああこれああでもだけど'),\n",
    "    ('こういうのは微妙(*hoge), データ(*hoge)にないからいい', 'こういうのは微妙(hoge), データ(hoge)にないからいい'),\n",
    "])\n",
    "def remove_notes_mark(text):\n",
    "    # *, *1, (*2), (*)といった注釈マーク消す. ※も.\n",
    "    text = re.sub(r'\\([\\*|※]\\d*\\)', '', text)\n",
    "    text = re.sub(r'[\\*|※]\\d*', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "@unit_test([\n",
    "    ('ｼﾘｺﾝﾊﾟﾌから､ｸｯｼｮﾝﾌｧﾝﾃﾞ専用､新登場｡', 'シリコンパフから、クッションファンデ専用、新登場。', 'カタカナは全角に'),\n",
    "    ('１０００, 1000', '1000, 1000', '数字は半角に'),\n",
    "    ('!!!???***, ！！！？？？＊＊＊', '!!!???***, !!!???***', '記号は半角に'),\n",
    "    ('\"これはそのまま\"', '\"これはそのまま\"', '「\"」はそのままにする'),\n",
    "])\n",
    "def normalize(text):\n",
    "    # カタカナは全角に\n",
    "    # ~,_などの正規化, 数字, 記号は半角にしてくれる\n",
    "    # \"hoge\" を ``hoge\"にするのは期待に沿わないので, \"hoge\"に直す\n",
    "    text = jaconv.normalize(text, 'NFKC')\n",
    "    text = re.sub(r'``', '\"', text)\n",
    "    return text\n",
    "\n",
    "@unit_test([\n",
    "    ('いつでもどこでも、すうっと一息。アロマのマッサージ。', 'いつでもどこでも、すうっと一息。<sep>アロマのマッサージ。', '<sep>を入れる. 文末にはいれない'),\n",
    "    ('ああ!!??なに?これ???', 'ああ!!??<sep>なに?<sep>これ???', '記号が混ざっても複数<sep>を入れない'),\n",
    "])\n",
    "def replace_ja_period_sep(text):\n",
    "    # 。や!の後に<sep>を挿入. 。とか!は効果や表現に意味を持ちそうなので今回除去しない\n",
    "    text = re.sub(r'([！|？|!|?|。]+)', r'\\1<sep>', text)\n",
    "    text = re.sub(r'<sep>$', '', text)\n",
    "    return text\n",
    "\n",
    "@unit_test([\n",
    "    ('うねりやすい髪が気になる方に　すんなりまとまる髪へ', f'うねりやすい髪が気になる方に{JA_PERIOD}すんなりまとまる髪へ'),\n",
    "    ('うねりやすい髪が気になる方に　　すんなりまとまる髪へ', f'うねりやすい髪が気になる方に{JA_PERIOD}すんなりまとまる髪へ', '複数あったらまとめて置換'),\n",
    "    ('いつでもどこでも うるおいケア', f'いつでもどこでも うるおいケア', '半角は対象としない'),\n",
    "])\n",
    "def replace_fwspace_ja_period(text):\n",
    "    # 文中の全角空白は「。」に（文末, 文頭の空白は除去されている前提）\n",
    "    return re.sub(r'　+', JA_PERIOD, text)\n",
    "\n",
    "@unit_test([\n",
    "    ('肌をなめらかにほぐす,柔らか化粧水', '肌をなめらかにほぐす柔らか化粧水'),\n",
    "])\n",
    "def remove_coron(text):\n",
    "    # コロン削除\n",
    "    return re.sub(r',', '', text)\n",
    "\n",
    "@unit_test([\n",
    "    ('  aaa','aaa', '文頭の空白は消す'),\n",
    "    ('aaa   ','aaa', '文末の空白は消す'),\n",
    "    ('  aaa　　','aaa', '文頭,文末の空白は消す. 半角全角は区別しない.'),\n",
    "    ('　　aaa','aaa', '半角全角は区別しない'),\n",
    "    ('　　a   aa　　　','a   aa', '文中の空白は消さない'),\n",
    "    ('a  aa','a  aa', '文中の空白は消さない'),\n",
    "])\n",
    "def remove_head_tail_space(text):\n",
    "    # 文頭, 文末の空白削除\n",
    "    text = re.sub(r'\\s+$', '', text)\n",
    "    text = re.sub(r'^\\s+', '', text)\n",
    "    return text\n",
    "\n",
    "@unit_test([\n",
    "    ('https://hoge.com',''),\n",
    "    ('hogehttps://hoge.com だね','hoge だね'),\n",
    "    ('↓少しでも気になる方はまずクリック↓ https://hoge.com','↓少しでも気になる方はまずクリック↓ '),\n",
    "])\n",
    "def remove_url(text):\n",
    "    return re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+', '', text)\n",
    "\n",
    "@unit_test([\n",
    "    ('おすすめです!#HOGE #SAIKO だよ','おすすめです!  だよ'),\n",
    "])\n",
    "def remove_hash(text):\n",
    "    return re.sub(r'[#＃][Ａ-Ｚａ-ｚA-Za-z一-鿆0-9０-９ぁ-ヶｦ-ﾟー]+', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mecab：分かち書き"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "from functools import lru_cache\n",
    "m = MeCab.Tagger(\"-Ochasen -d /usr/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "\n",
    "@lru_cache(maxsize = None)\n",
    "def mecab_parse(text):\n",
    "    \"\"\"\n",
    "    textをmecabでパースする\n",
    "    input: \n",
    "    \"\"\"\n",
    "    parsed_list = []\n",
    "    text_parsed = m.parse(text)\n",
    "    for word_parsed in text_parsed.split('\\n')[:-2]: # EOF省略\n",
    "        wp_list = word_parsed.split('\\t')\n",
    "        origin, kana, stem, pos_list, _, _ = wp_list\n",
    "        parsed_list.append({\n",
    "            'origin': origin,\n",
    "            'kana': kana,\n",
    "            'stem': stem,\n",
    "            'pos_list': pos_list,\n",
    "        })\n",
    "    return parsed_list\n",
    "\n",
    "def wakati(text):\n",
    "    if type(text) is str:\n",
    "        tag_list = parse_text(text)\n",
    "    elif type(text) is list and text and 'origin' in text[0].keys():\n",
    "        tag_list = text\n",
    "    text = ' '.join([tag['origin'] for tag in tag_list])\n",
    "    return text\n",
    "\n",
    "def contains_only_noun(pos_list):\n",
    "    # 名詞しか使ってなかったら非文 : 368件しかなかったのでやめとく\n",
    "    return sum(['名詞' in pos for pos in pos_list]) == len(pos_list)\n",
    "\n",
    "def is_valid_sentence(tag_list):\n",
    "    origin = ''.join([tag['origin'] for tag in tag_list])\n",
    "    pos_list = [tag['pos_list'] for tag in tag_list]\n",
    "    # if contains_only_noun(pos_list): return False\n",
    "    # ひらがな, カタカナ以外のみで文が構成されているか\n",
    "    is_only_contains_not_kana = bool(re.match(r'^[^ぁ-ん^ァ-ン]*$', origin))\n",
    "    return not is_only_contains_not_kana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mecab_parse('ピコ太郎さんはカナブンに角をつけてカブトムシとして売るバイトをしている')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mecab後処理\n",
    "\n",
    "特別な数字トークン以外の数字を<num>にする  \n",
    "。！？ の後に<sep>を入れる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:【unit_test】 mask_general_num_token_wakatied は1件のテストに通りました.\n",
      "INFO:root:【unit_test】 replace_ja_period_sep_wakatied は2件のテストに通りました.\n"
     ]
    }
   ],
   "source": [
    "# 他の数字とは明らかに文表現が異なる単語リスト\n",
    "num_special_words = ['1本', '1個', '100%', '1日', '1品', '365日', '1つ', '1枚', '24時間', '3D', 'No.1', '99%', '1度']\n",
    "\n",
    "def after_tagger_processing(text):\n",
    "    # <sep>の挿入\n",
    "    text = replace_ja_period_sep_wakatied(text)\n",
    "    # 数詞トークンをマスク\n",
    "    text = mask_general_num_token_wakatied(text)\n",
    "    return text\n",
    "\n",
    "@unit_test([\n",
    "    ('お 得 な 3つ で 2 役 約120g 。 100% です 。 2.0 立法 。', 'お 得 な <num> つ で <num> 役 約 <num> g 。 100% です 。 <num> 立法 。', 'マスクする'),\n",
    "    ('1 2ml 入ってます', '<num> ml 入ってます', '<num>を連続させない'),\n",
    "])\n",
    "def mask_general_num_token_wakatied(text):\n",
    "    # 一般的な数詞トークンをマスク, 分割\n",
    "    word_list = text.split(' ')\n",
    "    masked_word_list = []\n",
    "    for word in word_list:\n",
    "        # 特殊数字ならそのまま\n",
    "        if word in num_special_words:\n",
    "            masked_word_list.append(word)\n",
    "            continue\n",
    "            \n",
    "        # それ以外の普遍的な数字はマスクし前後に空白を挟む\n",
    "        word = re.sub(r'(?:\\d+\\.?\\d*|\\.\\d+)', r' <num> ', word)\n",
    "        splited_word_list = [w for w in word.split(' ') if w]\n",
    "        masked_word_list += splited_word_list\n",
    "    masked_word_list = unite_duplicate(masked_word_list, '<num>')\n",
    "    masked_word_list = unite_duplicate(masked_word_list, '<sep>')\n",
    "    return ' '.join(masked_word_list)\n",
    "\n",
    "@unit_test([\n",
    "    ('これ だ ! それ だ 。 どれ だ ？ これ だ ！', 'これ だ ! <sep> それ だ 。 <sep> どれ だ ？ <sep> これ だ ！', '<sep>を入れる. 文末にはいれない'),\n",
    "    ('これ だ !!!! それ だ !!???。。 これ だ !', 'これ だ !!!! <sep> それ だ !!???。。 <sep> これ だ !', '記号が混ざっても複数<sep>を入れない'),\n",
    "])\n",
    "def replace_ja_period_sep_wakatied(text):\n",
    "    # 。や!の後に<sep>を挿入. 。とか!は効果や表現に意味を持ちそうなので今回除去しない\n",
    "    # mecab後のやつ. 空白で挟む\n",
    "    text = re.sub(r'([！|？|!|?|。]+)', r'\\1 <sep>', text)\n",
    "    text = re.sub(r' *<sep>$', '', text)\n",
    "    return text\n",
    "\n",
    "def unite_duplicate(arr, unit):\n",
    "    # 連続するunit要素をまとめて1個にする\n",
    "    new_arr = []\n",
    "    pre_item = None\n",
    "    for item in arr:\n",
    "        if item == pre_item and unit == item:\n",
    "            pre_item = item\n",
    "            continue\n",
    "        else:\n",
    "            new_arr.append(item)\n",
    "            pre_item = item\n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 2, 2, 1, 2, 2]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unite_duplicate([0, 1, 1, 0, 0, 2, 2, 1, 2, 2], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['text', 'media', 'parsed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text', 'media', 'parsed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan除去：192042件\n"
     ]
    }
   ],
   "source": [
    "df = df[~df.text.isna()].copy()\n",
    "print(f'Nan除去：{df.shape[0]}件')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "重複除去：50639件\n"
     ]
    }
   ],
   "source": [
    "df_uq_text = df.drop_duplicates('text').copy()\n",
    "print(f'重複除去：{df_uq_text.shape[0]}件')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.26 s, sys: 4 ms, total: 1.26 s\n",
      "Wall time: 1.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# わかちがき前処理\n",
    "# - コロン削除 ： 30,000とかのため\n",
    "# - 文末, 文頭の空白削除\n",
    "# - 文中の全角空白は「。」にする\n",
    "# - カタカナ記号は全角で統一. 数字,記号の正規化\n",
    "# - *1 とかの注釈マークを除去\n",
    "# - URL除去\n",
    "# - ハッシュタグ除去\n",
    "df_uq_text['normalized_text'] = df_uq_text.text.apply(before_tagger_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36 ms, sys: 0 ns, total: 36 ms\n",
      "Wall time: 36.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# タグづけ\n",
    "df_uq_text['mecab_tagged_text'] = df_uq_text.normalized_text.apply(mecab_parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.28 s, sys: 72 ms, total: 1.35 s\n",
      "Wall time: 1.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# POSタグ抽出 ： 一旦愚直に並べてみる\n",
    "df_uq_text['pos_all'] = df_uq_text.mecab_tagged_text.apply(lambda tags: [t['pos_list'] for t in tags])\n",
    "df_uq_text['pos_large'] = df_uq_text.mecab_tagged_text.apply(lambda tags: [t['pos_list'].split('-')[0] for t in tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 156 ms, sys: 4 ms, total: 160 ms\n",
      "Wall time: 163 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 分かち書き\n",
    "df_uq_text['wakatied_text'] = df_uq_text.mecab_tagged_text.apply(wakati)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.33 s, sys: 12 ms, total: 2.34 s\n",
      "Wall time: 2.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# わかちがき後処理\n",
    "df_uq_text['complete_processed_text'] = df_uq_text.wakatied_text.apply(after_tagger_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全レコード: 50639件\n",
      "非文除去後: 50362件\n",
      "CPU times: user 288 ms, sys: 4 ms, total: 292 ms\n",
      "Wall time: 297 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 非文除去\n",
    "df_uq_text['is_valid'] = df_uq_text.mecab_tagged_text.apply(is_valid_sentence)\n",
    "print(f'全レコード: {df_uq_text.shape[0]}件')\n",
    "df_uq_valid_text = df_uq_text[df_uq_text.is_valid].copy()\n",
    "print(f'非文除去後: {df_uq_valid_text.shape[0]}件')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POSチェック\n",
    "# df_uq_valid_text['pos_large_str'] = df_uq_valid_text.pos_large.apply(lambda pos_list: str(pos_list))\n",
    "# print('<br/>'.join(df_uq_valid_text[df_uq_valid_text.pos_large_str == \"['名詞', '助詞', '名詞', '記号', '名詞', '助詞', '動詞', '名詞', '助詞']\"].text.tolist()[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整形後の重複除去\n",
    "df_uq_valid_text = df_uq_valid_text.drop_duplicates('complete_processed_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uq_valid_text = df_uq_valid_text[['text', 'media', 'normalized_text', 'complete_processed_text', 'mecab_tagged_text', 'pos_all', 'pos_large']].copy()\n",
    "df_uq_valid_text.to_pickle('./data/tmp/df_cosme_parsed.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習用データ分割,吐き出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TYPES = ['train', 'valid', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df(df, train_rate=0.8):\n",
    "    # データシャッフル\n",
    "    df = df.sample(frac=1, random_state=199).reset_index(drop=True)\n",
    "    df_size = df.shape[0]\n",
    "    train_index = int(df_size * 0.9)\n",
    "    valid_index = int(train_index + df_size * 0.05)\n",
    "    df['data_type'] = None\n",
    "    df['data_type'][:train_index] = 'train'\n",
    "    df['data_type'][train_index:valid_index] = 'valid'\n",
    "    df['data_type'][valid_index:] = 'test'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = split_df(df_uq_valid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>data_type</th>\n",
       "      <th>train</th>\n",
       "      <th>valid</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>text</td>\n",
       "      <td>44596</td>\n",
       "      <td>2477</td>\n",
       "      <td>2479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "data_type  train  valid  test\n",
       "text       44596   2477  2479"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('data_type').count()[['text']].T[DATA_TYPES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_lines(lines, path):\n",
    "    with open(path, 'w') as f:\n",
    "        f.write('\\n'.join(lines))\n",
    "        print(f'save → {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pos_str'] = df.pos_large.apply(lambda l: ' '.join(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_df_train(df):\n",
    "    for data_type in DATA_TYPES:\n",
    "        _df = df[df.data_type == data_type]\n",
    "        save_lines(_df.complete_processed_text.tolist(), f'./data/tmp/{data_type}_lines.txt')\n",
    "        save_lines(_df.pos_str.tolist(), f'./data/tmp/{data_type}_pos.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save → ./data/tmp/train_lines.txt\n",
      "save → ./data/tmp/train_pos.txt\n",
      "save → ./data/tmp/valid_lines.txt\n",
      "save → ./data/tmp/valid_pos.txt\n",
      "save → ./data/tmp/test_lines.txt\n",
      "save → ./data/tmp/test_pos.txt\n"
     ]
    }
   ],
   "source": [
    "save_data_df_train(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kao_reg = r'\\([^あ-ん\\u30A1-\\u30F4\\u2E80-\\u2FDF\\u3005-\\u3007\\u3400-\\u4DBF\\u4E00-\\u9FFF\\uF900-\\uFAFF\\U00020000-\\U0002EBEF]+?\\)'\n",
    "# kao_reg = r'\\(.*\\)'\n",
    "# df[df.normalized_text.str.match(kao_reg)][['text', 'complete_processed_text']]\n",
    "# df[df.text.str.contains('\\(')][['text', 'complete_processed_text']]\n",
    "# df[df.text.str.contains('\\(')].shape[0], df[df.text.str.contains('\\（')].shape[0]\n",
    "# df[df.normalized_text.str.contains('\\(')][['text', 'complete_processed_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データアップロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building synchronization state...\n",
      "Starting synchronization...\n",
      "Copying file://./data/tmp/cosme.tsv [Content-Type=text/tab-separated-values]...\n",
      "Copying file://./data/tmp/df_cosme_parsed.pickle [Content-Type=application/octet-stream]...\n",
      "Copying file://./data/tmp/test_lines.txt [Content-Type=text/plain]...\n",
      "Copying file://./data/tmp/test_pos.txt [Content-Type=text/plain]...             \n",
      "Copying file://./data/tmp/train_lines.txt [Content-Type=text/plain]...          \n",
      "Copying file://./data/tmp/train_pos.txt [Content-Type=text/plain]...            \n",
      "Copying file://./data/tmp/valid_lines.txt [Content-Type=text/plain]...          \n",
      "Copying file://./data/tmp/valid_pos.txt [Content-Type=text/plain]...            \n",
      "/ [8/8 files][150.8 MiB/150.8 MiB] 100% Done                                    \n",
      "Operation completed over 8 objects/150.8 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "! gsutil -m rsync ./data/tmp gs://kawamoto-ramiel/experiments_v3_pos_20200104/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
