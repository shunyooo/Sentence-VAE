{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POSベースモデル\n",
    "<div align=\"center\">\n",
    "<img src='https://user-images.githubusercontent.com/17490886/71782891-a09ebb80-3022-11ea-9558-ad6618f7171b.png' width=1000>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil -m rsync -d -r gs://kawamoto-ramiel/experiments_v3_pos_20200104/data ../data/eccos_v2/\n",
    "# ! gsutil -m rsync -d -r ../data/eccos_v2/ gs://kawamoto-ramiel/experiments_v3_pos_20200104/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from multiprocessing import cpu_count\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "from ptb import PTB\n",
    "from utils import idx2word, experiment_name, AttributeDict\n",
    "from models.model_pos import POSVAE\n",
    "from models.model_utils import to_var, sample_z\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_dir: /Users/s07309/gdrive/src/ca_dev/Sentence-VAE\n",
      "runs_dir: /Users/s07309/gdrive/src/ca_dev/Sentence-VAE/runs\n"
     ]
    }
   ],
   "source": [
    "top_dir = os.path.abspath('..')\n",
    "runs_dir = f'{top_dir}/runs'\n",
    "print(f'top_dir: {top_dir}\\nruns_dir: {runs_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/s07309/gdrive/src/ca_dev/Sentence-VAE/data/eccos_v2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_base_dir = f'{top_dir}/data'\n",
    "data_name = 'eccos_v2'\n",
    "data_dir = f'{data_base_dir}/{data_name}'\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/s07309/gdrive/src/ca_dev/Sentence-VAE/runs',\n",
       " '/Users/s07309/gdrive/src/ca_dev/Sentence-VAE/runs')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir, save_model_path = runs_dir, runs_dir\n",
    "log_dir, save_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src max: 51, tgt max: 51, pos max: 48\n"
     ]
    }
   ],
   "source": [
    "def readlines(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return [s.replace('\\n', '') for s in f.readlines()]\n",
    "\n",
    "def cal_max_file_lines(path):\n",
    "    lines = readlines(path)\n",
    "    line_lengths = [len(line.split(' ')) for line in lines]\n",
    "    return max(line_lengths)\n",
    "    \n",
    "src_max_length = cal_max_file_lines(f'{data_dir}/src/ptb.train.txt')\n",
    "tgt_max_length = cal_max_file_lines(f'{data_dir}/tgt/ptb.train.txt')\n",
    "pos_max_length = cal_max_file_lines(f'{data_dir}/pos/ptb.train.txt')\n",
    "print(f'src max: {src_max_length}, tgt max: {tgt_max_length}, pos max: {pos_max_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AttrDict{'data_dir': '/Users/s07309/gdrive/src/ca_dev/Sentence-VAE/data/eccos_v2', 'create_data': False, 'src_max_sequence_length': 51, 'tgt_max_sequence_length': 51, 'pos_max_sequence_length': 48, 'min_occ': 1, 'test': False, 'epochs': 10, 'batch_size': 32, 'learning_rate': 0.001, 'embedding_size': 300, 'pos_embedding_size': 20, 'rnn_type': 'gru', 'hidden_size': 256, 'num_layers': 1, 'bidirectional': False, 'latent_size': 16, 'word_dropout': 0, 'embedding_dropout': 0.5, 'anneal_function': 'logistic', 'k': 0.0025, 'x0': 2500, 'print_every': 50, 'tensorboard_logging': True, 'logdir': '/Users/s07309/gdrive/src/ca_dev/Sentence-VAE/runs', 'save_model_path': '/Users/s07309/gdrive/src/ca_dev/Sentence-VAE/runs', 'experiment_name': 'posvae_eccos_v2', 'debug': True}>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = {\n",
    "    'data_dir': data_dir,\n",
    "    'create_data': False,\n",
    "    'src_max_sequence_length': src_max_length,\n",
    "    'tgt_max_sequence_length': tgt_max_length,\n",
    "    'pos_max_sequence_length': pos_max_length,\n",
    "    \n",
    "    'min_occ': 1,\n",
    "    'test': False,\n",
    "\n",
    "    'epochs': 10,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "    \n",
    "    'embedding_size': 300,\n",
    "    'pos_embedding_size': 20,\n",
    "    'rnn_type': 'gru',\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 1,\n",
    "    'bidirectional': False,\n",
    "    'latent_size': 16,\n",
    "    'word_dropout': 0,\n",
    "    'embedding_dropout': 0.5,\n",
    "\n",
    "    'anneal_function': 'logistic',\n",
    "    'k': 0.0025,\n",
    "    'x0': 2500,\n",
    "\n",
    "    'print_every': 50,\n",
    "    'tensorboard_logging': True,\n",
    "    'logdir': log_dir,\n",
    "    'save_model_path': save_model_path,\n",
    "    'experiment_name': f'posvae_{data_name}',\n",
    "    \n",
    "    'debug': False,\n",
    "}\n",
    "\n",
    "args = AttributeDict(args)\n",
    "\n",
    "args.rnn_type = args.rnn_type.lower()\n",
    "args.anneal_function = args.anneal_function.lower()\n",
    "\n",
    "assert args.rnn_type in ['rnn', 'lstm', 'gru']\n",
    "assert args.anneal_function in ['logistic', 'linear']\n",
    "assert 0 <= args.word_dropout <= 1\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /Users/s07309/gdrive/src/ca_dev/Sentence-VAE/data/eccos_v2\n",
      "('train', 'src')\n",
      "vocab: 10293, records: 44596\n",
      "('train', 'tgt')\n",
      "vocab: 10293, records: 44596\n",
      "('train', 'pos')\n",
      "vocab: 16, records: 44596\n",
      "('valid', 'src')\n",
      "vocab: 10293, records: 2477\n",
      "('valid', 'tgt')\n",
      "vocab: 10293, records: 2477\n",
      "('valid', 'pos')\n",
      "vocab: 16, records: 2477\n",
      "CPU times: user 1.58 s, sys: 99.5 ms, total: 1.68 s\n",
      "Wall time: 2.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import itertools\n",
    "splits = ['train', 'valid'] + (['test'] if args.test else [])\n",
    "data_types = ['src', 'tgt', 'pos']\n",
    "datasets = OrderedDict()\n",
    "print(f'loading {args.data_dir}')\n",
    "for split, src_tgt in itertools.product(splits, data_types):\n",
    "    key = (split, src_tgt)\n",
    "    print(key)\n",
    "    datasets[key] = PTB(\n",
    "        data_dir=f'{args.data_dir}/{src_tgt}',\n",
    "        split=split,\n",
    "        create_data=args.create_data,\n",
    "        max_sequence_length=args.obj[f'{src_tgt}_max_sequence_length'],\n",
    "        min_occ=args.min_occ\n",
    "    )\n",
    "    print(f'vocab: {datasets[key].vocab_size}, records: {len(datasets[key].data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- src --------\n",
      "■ src-input \n",
      "<sos> 特別 な ケア を ルルルン で ! <sep> web 限定 の セット で しっかり お 顔 の 隅々 に まで アプローチ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "■ src-target \n",
      "特別 な ケア を ルルルン で ! <sep> web 限定 の セット で しっかり お 顔 の 隅々 に まで アプローチ <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------- pos --------\n",
      "■ pos-input \n",
      "<sos> 名詞 助動詞 名詞 助詞 名詞 助詞 記号 名詞 名詞 助詞 名詞 助詞 副詞 接頭詞 名詞 助詞 名詞 助詞 助詞 名詞 <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "■ pos-target \n",
      "名詞 助動詞 名詞 助詞 名詞 助詞 記号 名詞 名詞 助詞 名詞 助詞 副詞 接頭詞 名詞 助詞 名詞 助詞 助詞 名詞 <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------- tgt --------\n",
      "■ tgt-input\n",
      "<sos> 特別 な ケア を ルルルン で ! <sep> web 限定 の セット で しっかり お 顔 の 隅々 に まで アプローチ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "■ tgt-target\n",
      "特別 な ケア を ルルルン で ! <sep> web 限定 の セット で しっかり お 顔 の 隅々 に まで アプローチ <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "# 実際のデータ確認\n",
    "def ids2text(id_list, ptb):\n",
    "    return ' '.join([ptb.i2w[f'{i}'] for i in id_list])\n",
    "\n",
    "_ptb_src = datasets[('train', 'src')]\n",
    "_ptb_pos = datasets[('train', 'pos')]\n",
    "_ptb_tgt = datasets[('train', 'tgt')]\n",
    "index = str(100)\n",
    "_sample_src, _sample_tgt, _sample_pos = _ptb_src[index], _ptb_tgt[index], _ptb_pos[index]\n",
    "print(f'------- src --------')\n",
    "print(f'■ src-input \\n{ids2text(_sample_src[\"input\"], _ptb_src)}')\n",
    "print(f'■ src-target \\n{ids2text(_sample_src[\"target\"], _ptb_src)}')\n",
    "print(f'------- pos --------')\n",
    "print(f'■ pos-input \\n{ids2text(_sample_pos[\"input\"], _ptb_pos)}')\n",
    "print(f'■ pos-target \\n{ids2text(_sample_pos[\"target\"], _ptb_pos)}')\n",
    "print(f'------- tgt --------')\n",
    "print(f'■ tgt-input\\n{ids2text(_sample_tgt[\"input\"], _ptb_tgt)}')\n",
    "print(f'■ tgt-target\\n{ids2text(_sample_tgt[\"target\"], _ptb_tgt)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptb import SOS_INDEX, EOS_INDEX, PAD_INDEX, UNK_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10293, 10293)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets[('train', 'src')].w2i), len(datasets[('valid', 'src')].w2i) #, len(datasets[('test', 'src')].w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10293, 10293)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets[('train', 'tgt')].w2i), len(datasets[('valid', 'tgt')].w2i) #, len(datasets[('test', 'tgt')].w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets[('train', 'pos')].w2i), len(datasets[('valid', 'pos')].w2i) #, len(datasets[('test', 'pos')].w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    'text': {'w2i': datasets[('train', 'src')].w2i, }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AttrDict{'data_dir': '/Users/s07309/gdrive/src/ca_dev/Sentence-VAE/data/eccos_v2', 'create_data': False, 'src_max_sequence_length': 51, 'tgt_max_sequence_length': 51, 'pos_max_sequence_length': 48, 'min_occ': 1, 'test': False, 'epochs': 10, 'batch_size': 32, 'learning_rate': 0.001, 'embedding_size': 300, 'pos_embedding_size': 20, 'rnn_type': 'gru', 'hidden_size': 256, 'num_layers': 1, 'bidirectional': False, 'latent_size': 16, 'word_dropout': 0, 'embedding_dropout': 0.5, 'anneal_function': 'logistic', 'k': 0.0025, 'x0': 2500, 'print_every': 50, 'tensorboard_logging': True, 'logdir': '/Users/s07309/gdrive/src/ca_dev/Sentence-VAE/runs', 'save_model_path': '/Users/s07309/gdrive/src/ca_dev/Sentence-VAE/runs', 'experiment_name': 'posvae_eccos_v2', 'debug': True}>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = POSVAE(\n",
    "    vocab_size=datasets[('train', 'src')].vocab_size,\n",
    "    pos_vocab_size=datasets[('train', 'pos')].vocab_size,\n",
    "    embedding_size=args.embedding_size,\n",
    "    pos_embedding_size=args.pos_embedding_size,\n",
    "    \n",
    "    rnn_type=args.rnn_type,\n",
    "    hidden_size=args.hidden_size,\n",
    "    word_dropout=args.word_dropout,\n",
    "    embedding_dropout=args.embedding_dropout,\n",
    "    latent_size=args.latent_size,\n",
    "    num_layers=args.num_layers,\n",
    "    bidirectional=args.bidirectional,\n",
    "    \n",
    "    tgt_max_sequence_length=args.tgt_max_sequence_length,\n",
    "    pos_max_sequence_length=args.pos_max_sequence_length,\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POSVAE(\n",
       "  (embedding): Embedding(10293, 300)\n",
       "  (embedding_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (pos_embedding): Embedding(16, 20)\n",
       "  (encoder_rnn): GRU(300, 256, batch_first=True)\n",
       "  (pos_decoder_rnn): GRU(20, 256, batch_first=True)\n",
       "  (hidden2mean): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (hidden2logv): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (latent2pos_decoder_hidden): Linear(in_features=16, out_features=256, bias=True)\n",
       "  (outputs2pos): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (pos_encoder_rnn): GRU(20, 256, batch_first=True)\n",
       "  (text_decoder_rnn): GRU(300, 256, batch_first=True)\n",
       "  (latent2pos_encoder_hidden): Linear(in_features=16, out_features=256, bias=True)\n",
       "  (outputs2vocab): Linear(in_features=256, out_features=10293, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cstr(obj):\n",
    "    return f'```{obj}```'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_dict(_dict):\n",
    "    return '  \\n'.join([f'{k}: {v}' for k,v in _dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_model_dict(model, args):\n",
    "    meta_dict = {k:v for k, v in model.__dict__.items() if not k[0] == '_'}\n",
    "    meta_dict.update(args.obj)\n",
    "    return meta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard logging: True\n",
      "▼tensorboard logging\n",
      "/Users/s07309/gdrive/src/ca_dev/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-09-151949_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500\n",
      "▼ model save\n",
      "/Users/s07309/gdrive/src/ca_dev/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-09-151949_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models\n"
     ]
    }
   ],
   "source": [
    "print(f'tensorboard logging: {args.tensorboard_logging}')\n",
    "ts = time.strftime('%Y-%m-%d-%H%M%S', time.localtime())\n",
    "exp_name = experiment_name(args,ts)\n",
    "\n",
    "if args.tensorboard_logging:\n",
    "    writer_path = os.path.join(args.logdir, exp_name)\n",
    "    writer = SummaryWriter(writer_path)\n",
    "    writer.add_text(\"model\", cstr(model.__repr__().replace('\\n', '  \\n')))\n",
    "    writer.add_text(\"args\", cstr(str_dict(args.obj)))\n",
    "    writer.add_text(\"ts\", ts)\n",
    "    print(f'▼tensorboard logging\\n{writer_path}')\n",
    "    \n",
    "save_model_path = os.path.join(args.save_model_path, exp_name, 'models')\n",
    "os.makedirs(save_model_path, exist_ok=True)\n",
    "print(f'▼ model save\\n{save_model_path}')\n",
    "\n",
    "# メタパラメータ保存\n",
    "with open(os.path.join(save_model_path, 'model_meta.json'), 'w') as f:\n",
    "    meta_dict = get_meta_model_dict(model, args)\n",
    "    json.dump(meta_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "LongTensor = torch.cuda.LongTensor if torch.cuda.is_available() else torch.LongTensor\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys([('train', 'src'), ('train', 'tgt'), ('train', 'pos'), ('valid', 'src'), ('valid', 'tgt'), ('valid', 'pos')])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> 水 なし ! <sep> 美容 成分 しか 入っ て ない ! <sep> セラミド <num> 倍 ジェル が やばい 笑 <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "水 なし ! <sep> 美容 成分 しか 入っ て ない ! <sep> セラミド <num> 倍 ジェル が やばい 笑 <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "ae_datasets = {split: dataset for (split, src_tgt), dataset in datasets.items() if src_tgt == 'tgt'}\n",
    "print(ids2text(ae_datasets['train'][0]['input'], ae_datasets['train']))\n",
    "print(ids2text(ae_datasets['train'][0]['target'], ae_datasets['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['src', 'tgt', 'pos']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets[('train', 'src')].data['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': array([ 2, 21, 22, 23, 24, 25,  9, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "        36,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'target': array([21, 22, 23, 24, 25,  9, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "         3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'length': 18}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[('train', 'src')]['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 44596\n",
      "debug → 300\n",
      "valid: 2477\n",
      "debug → 300\n"
     ]
    }
   ],
   "source": [
    "# ループ内で扱う用に変形\n",
    "_datasets = {}\n",
    "for split in splits:\n",
    "    dataset = []\n",
    "    print(f\"{split}: {len(datasets[(split, 'src')])}\")\n",
    "    for i in range(len(datasets[(split, 'src')])):\n",
    "        _data = {}\n",
    "        for data_type in data_types:\n",
    "            d = datasets[(split, data_type)][f'{i}']\n",
    "            _data.update({f'{data_type}_{k}': v for k, v in d.items()})\n",
    "            _data.update({f'{data_type}_raw': d['input'][1:]})\n",
    "            _data.update({f'{data_type}_raw_length': d['length'] - 1})\n",
    "            \n",
    "        dataset.append(_data)\n",
    "    _datasets[split] = dataset\n",
    "    if args.debug:\n",
    "        _data_limit = 300\n",
    "        _datasets[split] = dataset[:_data_limit]\n",
    "        print(f'debug → {_data_limit}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ptb.PTB at 0x1a380eef60>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target_ptb = datasets[('train', 'tgt')]\n",
    "train_target_ptb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ids2text as ids2ptext\n",
    "from metric import write_tensorboard_valid_metric, remove_pad_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0/9, Loss  191.7890, POS_NLL_Loss   41.6805, TEXT_NLL_Loss  150.1078, KL_Weight    0.0019, KL_Loss    0.3700\n",
      "TRAIN Batch 9/9, Loss  170.3569, POS_NLL_Loss   28.4954, TEXT_NLL_Loss  141.8470, KL_Weight    0.0020, KL_Loss    7.3727\n",
      "TRAIN Epoch 00/10, Mean Loss  192.2332\n",
      "Model saved at /Users/s07309/gdrive/src/ca_dev/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-09-151949_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E0.pytorch\n",
      "VALID Batch 0/9, Loss  175.4671, POS_NLL_Loss   29.9181, TEXT_NLL_Loss  145.5279, KL_Weight    0.0020, KL_Loss   10.6328\n",
      "VALID Batch 9/9, Loss  183.2882, POS_NLL_Loss   31.9829, TEXT_NLL_Loss  151.2693, KL_Weight    0.0020, KL_Loss   18.2386\n",
      "VALID Epoch 00/10, Mean Loss  182.0981\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "\n",
    "    for split in splits:\n",
    "        \n",
    "        data_loader = DataLoader(\n",
    "            dataset=_datasets[split],\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=split=='train',\n",
    "            num_workers=cpu_count(),\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "\n",
    "        tracker = defaultdict(Tensor)\n",
    "\n",
    "        # Enable/Disable Dropout\n",
    "        if split == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        for iteration, batch in enumerate(data_loader):\n",
    "            batch_size = batch['src_input'].size(0)\n",
    "            \n",
    "            for k, v in batch.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    batch[k] = to_var(v)\n",
    "            \n",
    "            # model output\n",
    "            b = label_dict = batch\n",
    "            out_dict = model(b['src_input'], b['src_length'], b['pos_input'], b['pos_length'])\n",
    "            mean, logv, z = out_dict['mean'], out_dict['logv'], out_dict['z']\n",
    "            # loss calculation\n",
    "            loss_dict = model.loss(out_dict, label_dict, step, args)\n",
    "            loss = loss_dict['Loss']\n",
    "\n",
    "            # backward + optimization\n",
    "            if split == 'train':\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                step += 1\n",
    "\n",
    "            # for log\n",
    "            loss_value_dict = {}\n",
    "            for _name, _loss in loss_dict.items():\n",
    "                loss_value = _loss.detach().item() if 'detach' in dir(_loss) else _loss\n",
    "                if _name not in ['Loss', 'KL_Weight']:\n",
    "                    loss_value /= batch_size\n",
    "                loss_value_dict[_name] = loss_value\n",
    "                \n",
    "            # bookkeepeing\n",
    "            tracker['Loss'] = torch.cat((tracker['Loss'], loss.detach().view(1)))\n",
    "\n",
    "            if args.tensorboard_logging:\n",
    "                _ = [writer.add_scalar(f'{split.upper()}/{name}', value, epoch*len(data_loader) + iteration) for name, value in loss_value_dict.items()]\n",
    "\n",
    "            if iteration % args.print_every == 0 or iteration+1 == len(data_loader):\n",
    "                print_text = f'{split.upper()} Batch {iteration}/{len(data_loader)-1}'\n",
    "                for k, v in loss_value_dict.items():\n",
    "                    print_text += f', {k} {v:9.4f}'\n",
    "                print(print_text)\n",
    "\n",
    "            if split == 'valid':\n",
    "                def add_tokens_tracker(id_key, token_key, ids, dataset_key, sep=''):\n",
    "                    tracker[id_key] = torch.cat((tracker.get(id_key, LongTensor()), ids.detach()), dim=0)\n",
    "                    tracker[token_key] = tracker.get(token_key, []) + [ids2ptext(text_ids, datasets[dataset_key].i2w, sep=sep) for text_ids in ids]\n",
    "                \n",
    "                add_tokens_tracker('target_text_ids', 'target_texts', batch['tgt_target'].data, ('train', 'tgt'))\n",
    "                add_tokens_tracker('target_pos_ids', 'target_poses', batch['pos_target'].data, ('train', 'pos'), sep=' ')\n",
    "                tracker['z'] = torch.cat((tracker['z'], z.detach()), dim=0)\n",
    "                with torch.no_grad():\n",
    "                    # Single Inference\n",
    "                    pos_decoded_ids = model.pos_inference(z=z) # z → POS\n",
    "                    pos_decoded_input, pos_decoded_length = model.target2input(pos_decoded_ids)  # POS → POS Input\n",
    "                    text_decoded_ids = model.text_inference(pos_decoded_input, pos_decoded_length, z) # POS Input → Text\n",
    "                    add_tokens_tracker('decoded_text_ids', 'decoded_texts',  text_decoded_ids, ('train', 'tgt'))\n",
    "                    add_tokens_tracker('decoded_pos_ids', 'decoded_poses',  pos_decoded_ids, ('train', 'pos'), sep=' ')\n",
    "                    \n",
    "                    # Multiple Inference\n",
    "                    z_list = sample_z(mean, logv, n=10)\n",
    "                    z_list = z_list.permute(1, 0, 2) # バッチ数, サンプル数, 次元数\n",
    "                    text_decoded_ids = [model.pos_text_inference(z=_z)['text_decoded_ids'].tolist() for _z in z_list]\n",
    "                    tracker['multi_text_decoded_ids'] = tracker.get('multi_text_decoded_ids', []) + text_decoded_ids\n",
    "\n",
    "                    \n",
    "        print(\"%s Epoch %02d/%i, Mean Loss %9.4f\"%(split.upper(), epoch, args.epochs, torch.mean(tracker['Loss'])))\n",
    "\n",
    "        if args.tensorboard_logging:\n",
    "            writer.add_scalar(\"%s-Epoch/Loss\"%split.upper(), torch.mean(tracker['Loss']), epoch)\n",
    "        \n",
    "        if split == 'valid':\n",
    "            decoded_id_list = remove_pad_index(tracker['decoded_text_ids'])\n",
    "            multi_decoded_id_list = remove_pad_index(tracker['multi_text_decoded_ids'])\n",
    "            valid_tgt_id_list = remove_pad_index(tracker['target_text_ids'])\n",
    "            train_tgt_id_list = remove_pad_index([d['tgt_target'] for d in _datasets['train']]) # コピー率用\n",
    "            write_tensorboard_valid_metric(writer, valid_tgt_id_list, decoded_id_list, multi_decoded_id_list, train_tgt_id_list, datasets[('train', 'tgt')].i2w, split, epoch)\n",
    "\n",
    "        # save checkpoint\n",
    "        if split == 'train':\n",
    "            checkpoint_path = os.path.join(save_model_path, f\"model_E{epoch}.pytorch\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(\"Model saved at %s\"%checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
