{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POSベースモデル\n",
    "<div align=\"center\">\n",
    "<img src='https://user-images.githubusercontent.com/17490886/71782891-a09ebb80-3022-11ea-9558-ad6618f7171b.png' width=1000>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil -m rsync -d -r gs://kawamoto-ramiel/experiments_v3_pos_20200104/data ../data/eccos_v2/\n",
    "# ! gsutil -m rsync -d -r ../data/eccos_v2/ gs://kawamoto-ramiel/experiments_v3_pos_20200104/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from multiprocessing import cpu_count\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "from ptb import PTB\n",
    "from utils import idx2word, experiment_name, AttributeDict\n",
    "from models.model_pos import POSVAE\n",
    "from models.model_utils import to_var, sample_z\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_dir: /root/user/work/src/Sentence-VAE\n",
      "runs_dir: /root/user/work/src/Sentence-VAE/runs\n"
     ]
    }
   ],
   "source": [
    "top_dir = os.path.abspath('..')\n",
    "runs_dir = f'{top_dir}/runs'\n",
    "print(f'top_dir: {top_dir}\\nruns_dir: {runs_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/user/work/src/Sentence-VAE/data/eccos_v2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_base_dir = f'{top_dir}/data'\n",
    "data_name = 'eccos_v2'\n",
    "data_dir = f'{data_base_dir}/{data_name}'\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/root/user/work/src/Sentence-VAE/runs',\n",
       " '/root/user/work/src/Sentence-VAE/runs')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir, save_model_path = runs_dir, runs_dir\n",
    "log_dir, save_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src max: 51, tgt max: 51, pos max: 48\n"
     ]
    }
   ],
   "source": [
    "def readlines(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return [s.replace('\\n', '') for s in f.readlines()]\n",
    "\n",
    "def cal_max_file_lines(path):\n",
    "    lines = readlines(path)\n",
    "    line_lengths = [len(line.split(' ')) for line in lines]\n",
    "    return max(line_lengths)\n",
    "    \n",
    "src_max_length = cal_max_file_lines(f'{data_dir}/src/ptb.train.txt')\n",
    "tgt_max_length = cal_max_file_lines(f'{data_dir}/tgt/ptb.train.txt')\n",
    "pos_max_length = cal_max_file_lines(f'{data_dir}/pos/ptb.train.txt')\n",
    "print(f'src max: {src_max_length}, tgt max: {tgt_max_length}, pos max: {pos_max_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AttrDict{'data_dir': '/root/user/work/src/Sentence-VAE/data/eccos_v2', 'create_data': False, 'src_max_sequence_length': 51, 'tgt_max_sequence_length': 51, 'pos_max_sequence_length': 48, 'min_occ': 1, 'test': False, 'epochs': 10, 'batch_size': 32, 'learning_rate': 0.001, 'embedding_size': 300, 'pos_embedding_size': 20, 'rnn_type': 'gru', 'hidden_size': 256, 'num_layers': 1, 'bidirectional': False, 'latent_size': 16, 'word_dropout': 0, 'embedding_dropout': 0.5, 'anneal_function': 'logistic', 'k': 0.0025, 'x0': 2500, 'print_every': 50, 'tensorboard_logging': True, 'logdir': '/root/user/work/src/Sentence-VAE/runs', 'save_model_path': '/root/user/work/src/Sentence-VAE/runs', 'experiment_name': 'posvae_eccos_v2', 'debug': False}>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = {\n",
    "    'data_dir': data_dir,\n",
    "    'create_data': False,\n",
    "    'src_max_sequence_length': src_max_length,\n",
    "    'tgt_max_sequence_length': tgt_max_length,\n",
    "    'pos_max_sequence_length': pos_max_length,\n",
    "    \n",
    "    'min_occ': 1,\n",
    "    'test': False,\n",
    "\n",
    "    'epochs': 10,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "    \n",
    "    'embedding_size': 300,\n",
    "    'pos_embedding_size': 20,\n",
    "    'rnn_type': 'gru',\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 1,\n",
    "    'bidirectional': False,\n",
    "    'latent_size': 16,\n",
    "    'word_dropout': 0,\n",
    "    'embedding_dropout': 0.5,\n",
    "\n",
    "    'anneal_function': 'logistic',\n",
    "    'k': 0.0025,\n",
    "    'x0': 2500,\n",
    "\n",
    "    'print_every': 50,\n",
    "    'tensorboard_logging': True,\n",
    "    'logdir': log_dir,\n",
    "    'save_model_path': save_model_path,\n",
    "    'experiment_name': f'posvae_{data_name}',\n",
    "    \n",
    "    'debug': False,\n",
    "}\n",
    "\n",
    "args = AttributeDict(args)\n",
    "\n",
    "args.rnn_type = args.rnn_type.lower()\n",
    "args.anneal_function = args.anneal_function.lower()\n",
    "\n",
    "assert args.rnn_type in ['rnn', 'lstm', 'gru']\n",
    "assert args.anneal_function in ['logistic', 'linear']\n",
    "assert 0 <= args.word_dropout <= 1\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /root/user/work/src/Sentence-VAE/data/eccos_v2\n",
      "('train', 'src')\n",
      "vocab: 10293, records: 44596\n",
      "('train', 'tgt')\n",
      "vocab: 10293, records: 44596\n",
      "('train', 'pos')\n",
      "vocab: 16, records: 44596\n",
      "('valid', 'src')\n",
      "vocab: 10293, records: 2477\n",
      "('valid', 'tgt')\n",
      "vocab: 10293, records: 2477\n",
      "('valid', 'pos')\n",
      "vocab: 16, records: 2477\n",
      "CPU times: user 2.99 s, sys: 193 ms, total: 3.18 s\n",
      "Wall time: 3.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import itertools\n",
    "splits = ['train', 'valid'] + (['test'] if args.test else [])\n",
    "data_types = ['src', 'tgt', 'pos']\n",
    "datasets = OrderedDict()\n",
    "print(f'loading {args.data_dir}')\n",
    "for split, src_tgt in itertools.product(splits, data_types):\n",
    "    key = (split, src_tgt)\n",
    "    print(key)\n",
    "    datasets[key] = PTB(\n",
    "        data_dir=f'{args.data_dir}/{src_tgt}',\n",
    "        split=split,\n",
    "        create_data=args.create_data,\n",
    "        max_sequence_length=args.obj[f'{src_tgt}_max_sequence_length'],\n",
    "        min_occ=args.min_occ\n",
    "    )\n",
    "    print(f'vocab: {datasets[key].vocab_size}, records: {len(datasets[key].data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- src --------\n",
      "■ src-input \n",
      "<sos> 特別 な ケア を ルルルン で ! <sep> web 限定 の セット で しっかり お 顔 の 隅々 に まで アプローチ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "■ src-target \n",
      "特別 な ケア を ルルルン で ! <sep> web 限定 の セット で しっかり お 顔 の 隅々 に まで アプローチ <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------- pos --------\n",
      "■ pos-input \n",
      "<sos> 名詞 助動詞 名詞 助詞 名詞 助詞 記号 名詞 名詞 助詞 名詞 助詞 副詞 接頭詞 名詞 助詞 名詞 助詞 助詞 名詞 <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "■ pos-target \n",
      "名詞 助動詞 名詞 助詞 名詞 助詞 記号 名詞 名詞 助詞 名詞 助詞 副詞 接頭詞 名詞 助詞 名詞 助詞 助詞 名詞 <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------- tgt --------\n",
      "■ tgt-input\n",
      "<sos> 特別 な ケア を ルルルン で ! <sep> web 限定 の セット で しっかり お 顔 の 隅々 に まで アプローチ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "■ tgt-target\n",
      "特別 な ケア を ルルルン で ! <sep> web 限定 の セット で しっかり お 顔 の 隅々 に まで アプローチ <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "# 実際のデータ確認\n",
    "def ids2text(id_list, ptb):\n",
    "    return ' '.join([ptb.i2w[f'{i}'] for i in id_list])\n",
    "\n",
    "_ptb_src = datasets[('train', 'src')]\n",
    "_ptb_pos = datasets[('train', 'pos')]\n",
    "_ptb_tgt = datasets[('train', 'tgt')]\n",
    "index = str(100)\n",
    "_sample_src, _sample_tgt, _sample_pos = _ptb_src[index], _ptb_tgt[index], _ptb_pos[index]\n",
    "print(f'------- src --------')\n",
    "print(f'■ src-input \\n{ids2text(_sample_src[\"input\"], _ptb_src)}')\n",
    "print(f'■ src-target \\n{ids2text(_sample_src[\"target\"], _ptb_src)}')\n",
    "print(f'------- pos --------')\n",
    "print(f'■ pos-input \\n{ids2text(_sample_pos[\"input\"], _ptb_pos)}')\n",
    "print(f'■ pos-target \\n{ids2text(_sample_pos[\"target\"], _ptb_pos)}')\n",
    "print(f'------- tgt --------')\n",
    "print(f'■ tgt-input\\n{ids2text(_sample_tgt[\"input\"], _ptb_tgt)}')\n",
    "print(f'■ tgt-target\\n{ids2text(_sample_tgt[\"target\"], _ptb_tgt)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptb import SOS_INDEX, EOS_INDEX, PAD_INDEX, UNK_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10293, 10293)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets[('train', 'src')].w2i), len(datasets[('valid', 'src')].w2i) #, len(datasets[('test', 'src')].w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10293, 10293)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets[('train', 'tgt')].w2i), len(datasets[('valid', 'tgt')].w2i) #, len(datasets[('test', 'tgt')].w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets[('train', 'pos')].w2i), len(datasets[('valid', 'pos')].w2i) #, len(datasets[('test', 'pos')].w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    'text': {'w2i': datasets[('train', 'src')].w2i, }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AttrDict{'data_dir': '/root/user/work/src/Sentence-VAE/data/eccos_v2', 'create_data': False, 'src_max_sequence_length': 51, 'tgt_max_sequence_length': 51, 'pos_max_sequence_length': 48, 'min_occ': 1, 'test': False, 'epochs': 10, 'batch_size': 32, 'learning_rate': 0.001, 'embedding_size': 300, 'pos_embedding_size': 20, 'rnn_type': 'gru', 'hidden_size': 256, 'num_layers': 1, 'bidirectional': False, 'latent_size': 16, 'word_dropout': 0, 'embedding_dropout': 0.5, 'anneal_function': 'logistic', 'k': 0.0025, 'x0': 2500, 'print_every': 50, 'tensorboard_logging': True, 'logdir': '/root/user/work/src/Sentence-VAE/runs', 'save_model_path': '/root/user/work/src/Sentence-VAE/runs', 'experiment_name': 'posvae_eccos_v2', 'debug': False}>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = POSVAE(\n",
    "    vocab_size=datasets[('train', 'src')].vocab_size,\n",
    "    pos_vocab_size=datasets[('train', 'pos')].vocab_size,\n",
    "    embedding_size=args.embedding_size,\n",
    "    pos_embedding_size=args.pos_embedding_size,\n",
    "    \n",
    "    rnn_type=args.rnn_type,\n",
    "    hidden_size=args.hidden_size,\n",
    "    word_dropout=args.word_dropout,\n",
    "    embedding_dropout=args.embedding_dropout,\n",
    "    latent_size=args.latent_size,\n",
    "    num_layers=args.num_layers,\n",
    "    bidirectional=args.bidirectional,\n",
    "    \n",
    "    tgt_max_sequence_length=args.tgt_max_sequence_length,\n",
    "    pos_max_sequence_length=args.pos_max_sequence_length,\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POSVAE(\n",
       "  (embedding): Embedding(10293, 300)\n",
       "  (embedding_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (pos_embedding): Embedding(16, 20)\n",
       "  (encoder_rnn): GRU(300, 256, batch_first=True)\n",
       "  (pos_decoder_rnn): GRU(20, 256, batch_first=True)\n",
       "  (hidden2mean): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (hidden2logv): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (latent2pos_decoder_hidden): Linear(in_features=16, out_features=256, bias=True)\n",
       "  (outputs2pos): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (pos_encoder_rnn): GRU(20, 256, batch_first=True)\n",
       "  (text_decoder_rnn): GRU(300, 256, batch_first=True)\n",
       "  (latent2pos_encoder_hidden): Linear(in_features=16, out_features=256, bias=True)\n",
       "  (outputs2vocab): Linear(in_features=256, out_features=10293, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cstr(obj):\n",
    "    return f'```{obj}```'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_dict(_dict):\n",
    "    return '  \\n'.join([f'{k}: {v}' for k,v in _dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_model_dict(model, args):\n",
    "    meta_dict = {k:v for k, v in model.__dict__.items() if not k[0] == '_'}\n",
    "    meta_dict.update(args.obj)\n",
    "    return meta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard logging: True\n",
      "▼tensorboard logging\n",
      "/root/user/work/src/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-10-061639_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500\n",
      "▼ model save\n",
      "/root/user/work/src/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-10-061639_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models\n"
     ]
    }
   ],
   "source": [
    "print(f'tensorboard logging: {args.tensorboard_logging}')\n",
    "ts = time.strftime('%Y-%m-%d-%H%M%S', time.localtime())\n",
    "exp_name = experiment_name(args,ts)\n",
    "\n",
    "if args.tensorboard_logging:\n",
    "    writer_path = os.path.join(args.logdir, exp_name)\n",
    "    writer = SummaryWriter(writer_path)\n",
    "    writer.add_text(\"model\", cstr(model.__repr__().replace('\\n', '  \\n')))\n",
    "    writer.add_text(\"args\", cstr(str_dict(args.obj)))\n",
    "    writer.add_text(\"ts\", ts)\n",
    "    print(f'▼tensorboard logging\\n{writer_path}')\n",
    "    \n",
    "save_model_path = os.path.join(args.save_model_path, exp_name, 'models')\n",
    "os.makedirs(save_model_path, exist_ok=True)\n",
    "print(f'▼ model save\\n{save_model_path}')\n",
    "\n",
    "# メタパラメータ保存\n",
    "with open(os.path.join(save_model_path, 'model_meta.json'), 'w') as f:\n",
    "    meta_dict = get_meta_model_dict(model, args)\n",
    "    json.dump(meta_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "LongTensor = torch.cuda.LongTensor if torch.cuda.is_available() else torch.LongTensor\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys([('train', 'src'), ('train', 'tgt'), ('train', 'pos'), ('valid', 'src'), ('valid', 'tgt'), ('valid', 'pos')])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> 水 なし ! <sep> 美容 成分 しか 入っ て ない ! <sep> セラミド <num> 倍 ジェル が やばい 笑 <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "水 なし ! <sep> 美容 成分 しか 入っ て ない ! <sep> セラミド <num> 倍 ジェル が やばい 笑 <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "ae_datasets = {split: dataset for (split, src_tgt), dataset in datasets.items() if src_tgt == 'tgt'}\n",
    "print(ids2text(ae_datasets['train'][0]['input'], ae_datasets['train']))\n",
    "print(ids2text(ae_datasets['train'][0]['target'], ae_datasets['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['src', 'tgt', 'pos']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets[('train', 'src')].data['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': array([ 2, 21, 22, 23, 24, 25,  9, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "        36,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'target': array([21, 22, 23, 24, 25,  9, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "         3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'length': 18}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[('train', 'src')]['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 44596\n",
      "valid: 2477\n"
     ]
    }
   ],
   "source": [
    "# ループ内で扱う用に変形\n",
    "_datasets = {}\n",
    "for split in splits:\n",
    "    dataset = []\n",
    "    print(f\"{split}: {len(datasets[(split, 'src')])}\")\n",
    "    for i in range(len(datasets[(split, 'src')])):\n",
    "        _data = {}\n",
    "        for data_type in data_types:\n",
    "            d = datasets[(split, data_type)][f'{i}']\n",
    "            _data.update({f'{data_type}_{k}': v for k, v in d.items()})\n",
    "            _data.update({f'{data_type}_raw': d['input'][1:]})\n",
    "            _data.update({f'{data_type}_raw_length': d['length'] - 1})\n",
    "            \n",
    "        dataset.append(_data)\n",
    "    _datasets[split] = dataset\n",
    "    if args.debug:\n",
    "        _data_limit = 300\n",
    "        _datasets[split] = dataset[:_data_limit]\n",
    "        print(f'debug → {_data_limit}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ptb.PTB at 0x7f36826f61d0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target_ptb = datasets[('train', 'tgt')]\n",
    "train_target_ptb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ids2text as ids2ptext\n",
    "from metric import write_tensorboard_valid_metric, remove_pad_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0/1393, Loss  182.2608, POS_NLL_Loss   40.2418, TEXT_NLL_Loss  142.0183, KL_Weight    0.0019, KL_Loss    0.3811\n",
      "TRAIN Batch 50/1393, Loss  122.5289, POS_NLL_Loss   22.7304, TEXT_NLL_Loss   99.7328, KL_Weight    0.0022, KL_Loss   30.1311\n",
      "TRAIN Batch 100/1393, Loss  130.4967, POS_NLL_Loss   24.2937, TEXT_NLL_Loss  106.0730, KL_Weight    0.0025, KL_Loss   52.6010\n",
      "TRAIN Batch 150/1393, Loss  116.2031, POS_NLL_Loss   23.1222, TEXT_NLL_Loss   92.8945, KL_Weight    0.0028, KL_Loss   66.5694\n",
      "TRAIN Batch 200/1393, Loss  108.3966, POS_NLL_Loss   21.4142, TEXT_NLL_Loss   86.7880, KL_Weight    0.0032, KL_Loss   61.2917\n",
      "TRAIN Batch 250/1393, Loss   99.3851, POS_NLL_Loss   20.4497, TEXT_NLL_Loss   78.6888, KL_Weight    0.0036, KL_Loss   68.6508\n",
      "TRAIN Batch 300/1393, Loss   99.9991, POS_NLL_Loss   19.5884, TEXT_NLL_Loss   80.1176, KL_Weight    0.0041, KL_Loss   72.0244\n",
      "TRAIN Batch 350/1393, Loss   96.0992, POS_NLL_Loss   19.3805, TEXT_NLL_Loss   76.3898, KL_Weight    0.0046, KL_Loss   71.3513\n",
      "TRAIN Batch 400/1393, Loss   99.4591, POS_NLL_Loss   20.5198, TEXT_NLL_Loss   78.5387, KL_Weight    0.0052, KL_Loss   76.7399\n",
      "TRAIN Batch 450/1393, Loss   93.2425, POS_NLL_Loss   20.0467, TEXT_NLL_Loss   72.7757, KL_Weight    0.0059, KL_Loss   71.0553\n",
      "TRAIN Batch 500/1393, Loss   90.5932, POS_NLL_Loss   18.3510, TEXT_NLL_Loss   71.7431, KL_Weight    0.0067, KL_Loss   74.5798\n",
      "TRAIN Batch 550/1393, Loss   79.3578, POS_NLL_Loss   14.8450, TEXT_NLL_Loss   64.0047, KL_Weight    0.0076, KL_Loss   67.0558\n",
      "TRAIN Batch 600/1393, Loss   93.6885, POS_NLL_Loss   18.4982, TEXT_NLL_Loss   74.6395, KL_Weight    0.0086, KL_Loss   64.2148\n",
      "TRAIN Batch 650/1393, Loss   86.3871, POS_NLL_Loss   18.3514, TEXT_NLL_Loss   67.3295, KL_Weight    0.0097, KL_Loss   72.7419\n",
      "TRAIN Batch 700/1393, Loss   97.6140, POS_NLL_Loss   18.2944, TEXT_NLL_Loss   78.6067, KL_Weight    0.0110, KL_Loss   64.8832\n",
      "TRAIN Batch 750/1393, Loss   87.7590, POS_NLL_Loss   16.0935, TEXT_NLL_Loss   70.7865, KL_Weight    0.0124, KL_Loss   70.7059\n",
      "TRAIN Batch 800/1393, Loss   89.7160, POS_NLL_Loss   17.7785, TEXT_NLL_Loss   71.0035, KL_Weight    0.0141, KL_Loss   66.4127\n",
      "TRAIN Batch 850/1393, Loss   91.9628, POS_NLL_Loss   18.5705, TEXT_NLL_Loss   72.3860, KL_Weight    0.0159, KL_Loss   63.2647\n",
      "TRAIN Batch 900/1393, Loss   85.3529, POS_NLL_Loss   16.6791, TEXT_NLL_Loss   67.5786, KL_Weight    0.0180, KL_Loss   60.8918\n",
      "TRAIN Batch 950/1393, Loss   87.8440, POS_NLL_Loss   16.0531, TEXT_NLL_Loss   70.6485, KL_Weight    0.0203, KL_Loss   56.1873\n",
      "TRAIN Batch 1000/1393, Loss   87.4436, POS_NLL_Loss   14.7456, TEXT_NLL_Loss   71.4402, KL_Weight    0.0230, KL_Loss   54.7377\n",
      "TRAIN Batch 1050/1393, Loss   76.5466, POS_NLL_Loss   14.6219, TEXT_NLL_Loss   60.5220, KL_Weight    0.0260, KL_Loss   54.0413\n",
      "TRAIN Batch 1100/1393, Loss   78.0389, POS_NLL_Loss   16.0646, TEXT_NLL_Loss   60.3430, KL_Weight    0.0293, KL_Loss   55.6529\n",
      "TRAIN Batch 1150/1393, Loss   69.2290, POS_NLL_Loss   12.4797, TEXT_NLL_Loss   55.1248, KL_Weight    0.0331, KL_Loss   49.0987\n",
      "TRAIN Batch 1200/1393, Loss   85.6113, POS_NLL_Loss   17.5656, TEXT_NLL_Loss   66.0885, KL_Weight    0.0373, KL_Loss   52.4342\n",
      "TRAIN Batch 1250/1393, Loss   75.4850, POS_NLL_Loss   13.9126, TEXT_NLL_Loss   59.5261, KL_Weight    0.0421, KL_Loss   48.6219\n",
      "TRAIN Batch 1300/1393, Loss   80.6894, POS_NLL_Loss   15.9681, TEXT_NLL_Loss   62.6320, KL_Weight    0.0474, KL_Loss   44.0555\n",
      "TRAIN Batch 1350/1393, Loss   76.8836, POS_NLL_Loss   15.0834, TEXT_NLL_Loss   59.3552, KL_Weight    0.0534, KL_Loss   45.7842\n",
      "TRAIN Batch 1393/1393, Loss   76.2188, POS_NLL_Loss   15.0361, TEXT_NLL_Loss   58.4764, KL_Weight    0.0591, KL_Loss   45.7878\n",
      "TRAIN Epoch 00/10, Mean Loss   94.5734\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-10-061639_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E0.pytorch\n",
      "VALID Batch 0/77, Loss   75.1442, POS_NLL_Loss   15.7110, TEXT_NLL_Loss   56.9032, KL_Weight    0.0592, KL_Loss   42.7038\n",
      "VALID Batch 50/77, Loss   84.6744, POS_NLL_Loss   15.2411, TEXT_NLL_Loss   66.8745, KL_Weight    0.0592, KL_Loss   43.1905\n",
      "VALID Batch 77/77, Loss   71.9042, POS_NLL_Loss   13.4807, TEXT_NLL_Loss   55.8475, KL_Weight    0.0592, KL_Loss   43.4817\n",
      "VALID Epoch 00/10, Mean Loss   76.6038\n",
      "TRAIN Batch 0/1393, Loss   85.3452, POS_NLL_Loss   17.1314, TEXT_NLL_Loss   65.5691, KL_Weight    0.0592, KL_Loss   44.6397\n",
      "TRAIN Batch 50/1393, Loss   74.2968, POS_NLL_Loss   13.5240, TEXT_NLL_Loss   57.8606, KL_Weight    0.0666, KL_Loss   43.7215\n",
      "TRAIN Batch 100/1393, Loss   73.2596, POS_NLL_Loss   13.7200, TEXT_NLL_Loss   56.4125, KL_Weight    0.0748, KL_Loss   41.7986\n",
      "TRAIN Batch 150/1393, Loss   64.9218, POS_NLL_Loss   12.4130, TEXT_NLL_Loss   49.2681, KL_Weight    0.0839, KL_Loss   38.6084\n",
      "TRAIN Batch 200/1393, Loss   81.0038, POS_NLL_Loss   14.1292, TEXT_NLL_Loss   63.1405, KL_Weight    0.0941, KL_Loss   39.6981\n",
      "TRAIN Batch 250/1393, Loss   87.0541, POS_NLL_Loss   17.8754, TEXT_NLL_Loss   65.2474, KL_Weight    0.1053, KL_Loss   37.3454\n",
      "TRAIN Batch 300/1393, Loss   84.7656, POS_NLL_Loss   16.6792, TEXT_NLL_Loss   63.9272, KL_Weight    0.1176, KL_Loss   35.3563\n",
      "TRAIN Batch 350/1393, Loss   78.0464, POS_NLL_Loss   14.6259, TEXT_NLL_Loss   59.1331, KL_Weight    0.1312, KL_Loss   32.6674\n",
      "TRAIN Batch 400/1393, Loss   77.8240, POS_NLL_Loss   15.7028, TEXT_NLL_Loss   57.2023, KL_Weight    0.1462, KL_Loss   33.6530\n",
      "TRAIN Batch 450/1393, Loss   76.6366, POS_NLL_Loss   13.2010, TEXT_NLL_Loss   58.3400, KL_Weight    0.1625, KL_Loss   31.3638\n",
      "TRAIN Batch 500/1393, Loss   81.6571, POS_NLL_Loss   16.1285, TEXT_NLL_Loss   60.3038, KL_Weight    0.1802, KL_Loss   28.9949\n",
      "TRAIN Batch 550/1393, Loss   85.8093, POS_NLL_Loss   15.5412, TEXT_NLL_Loss   64.5547, KL_Weight    0.1994, KL_Loss   28.6524\n",
      "TRAIN Batch 600/1393, Loss   75.2016, POS_NLL_Loss   15.2934, TEXT_NLL_Loss   54.3200, KL_Weight    0.2201, KL_Loss   25.3877\n",
      "TRAIN Batch 650/1393, Loss   76.5347, POS_NLL_Loss   14.9203, TEXT_NLL_Loss   55.4782, KL_Weight    0.2423, KL_Loss   25.3228\n",
      "TRAIN Batch 700/1393, Loss   72.7339, POS_NLL_Loss   14.0528, TEXT_NLL_Loss   52.1574, KL_Weight    0.2660, KL_Loss   24.5249\n",
      "TRAIN Batch 750/1393, Loss   81.8429, POS_NLL_Loss   14.1275, TEXT_NLL_Loss   60.7781, KL_Weight    0.2911, KL_Loss   23.8307\n",
      "TRAIN Batch 800/1393, Loss   79.3059, POS_NLL_Loss   15.8708, TEXT_NLL_Loss   56.2588, KL_Weight    0.3176, KL_Loss   22.5981\n",
      "TRAIN Batch 850/1393, Loss   80.3354, POS_NLL_Loss   15.8731, TEXT_NLL_Loss   57.3229, KL_Weight    0.3452, KL_Loss   20.6790\n",
      "TRAIN Batch 900/1393, Loss   82.7438, POS_NLL_Loss   15.9950, TEXT_NLL_Loss   58.8724, KL_Weight    0.3740, KL_Loss   21.0584\n",
      "TRAIN Batch 950/1393, Loss   73.2318, POS_NLL_Loss   13.8252, TEXT_NLL_Loss   51.3578, KL_Weight    0.4037, KL_Loss   19.9367\n",
      "TRAIN Batch 1000/1393, Loss   92.5288, POS_NLL_Loss   17.5338, TEXT_NLL_Loss   66.9494, KL_Weight    0.4341, KL_Loss   18.5326\n",
      "TRAIN Batch 1050/1393, Loss   78.7189, POS_NLL_Loss   14.5829, TEXT_NLL_Loss   55.5737, KL_Weight    0.4651, KL_Loss   18.4114\n",
      "TRAIN Batch 1100/1393, Loss   83.5563, POS_NLL_Loss   16.8496, TEXT_NLL_Loss   57.9713, KL_Weight    0.4963, KL_Loss   17.6030\n",
      "TRAIN Batch 1150/1393, Loss   71.0461, POS_NLL_Loss   11.4707, TEXT_NLL_Loss   50.7845, KL_Weight    0.5275, KL_Loss   16.6663\n",
      "TRAIN Batch 1200/1393, Loss   83.6914, POS_NLL_Loss   16.0731, TEXT_NLL_Loss   58.8417, KL_Weight    0.5585, KL_Loss   15.7151\n",
      "TRAIN Batch 1250/1393, Loss   99.8457, POS_NLL_Loss   19.8740, TEXT_NLL_Loss   70.1324, KL_Weight    0.5890, KL_Loss   16.7040\n",
      "TRAIN Batch 1300/1393, Loss   67.5164, POS_NLL_Loss   12.4529, TEXT_NLL_Loss   45.6920, KL_Weight    0.6189, KL_Loss   15.1414\n",
      "TRAIN Batch 1350/1393, Loss   77.8708, POS_NLL_Loss   16.0013, TEXT_NLL_Loss   52.4300, KL_Weight    0.6479, KL_Loss   14.5684\n",
      "TRAIN Batch 1393/1393, Loss   99.2566, POS_NLL_Loss   16.6796, TEXT_NLL_Loss   72.9061, KL_Weight    0.6721, KL_Loss   14.3899\n",
      "TRAIN Epoch 01/10, Mean Loss   79.6215\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-10-061639_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E1.pytorch\n",
      "VALID Batch 0/77, Loss   79.7380, POS_NLL_Loss   16.2121, TEXT_NLL_Loss   53.9087, KL_Weight    0.6726, KL_Loss   14.2985\n",
      "VALID Batch 50/77, Loss   89.8967, POS_NLL_Loss   16.7426, TEXT_NLL_Loss   63.3869, KL_Weight    0.6726, KL_Loss   14.5214\n",
      "VALID Batch 77/77, Loss   75.8135, POS_NLL_Loss   14.7039, TEXT_NLL_Loss   51.5754, KL_Weight    0.6726, KL_Loss   14.1750\n",
      "VALID Epoch 01/10, Mean Loss   80.2974\n",
      "TRAIN Batch 0/1393, Loss   80.8482, POS_NLL_Loss   14.8552, TEXT_NLL_Loss   56.2983, KL_Weight    0.6726, KL_Loss   14.4137\n",
      "TRAIN Batch 50/1393, Loss   89.2955, POS_NLL_Loss   17.4853, TEXT_NLL_Loss   61.1924, KL_Weight    0.6995, KL_Loss   15.1787\n",
      "TRAIN Batch 100/1393, Loss   84.7539, POS_NLL_Loss   16.3262, TEXT_NLL_Loss   58.3957, KL_Weight    0.7251, KL_Loss   13.8350\n",
      "TRAIN Batch 150/1393, Loss   78.8156, POS_NLL_Loss   15.4659, TEXT_NLL_Loss   53.4663, KL_Weight    0.7493, KL_Loss   13.1897\n",
      "TRAIN Batch 200/1393, Loss   91.1874, POS_NLL_Loss   17.6575, TEXT_NLL_Loss   63.7783, KL_Weight    0.7721, KL_Loss   12.6305\n",
      "TRAIN Batch 250/1393, Loss   92.4256, POS_NLL_Loss   19.7837, TEXT_NLL_Loss   62.1578, KL_Weight    0.7933, KL_Loss   13.2156\n",
      "TRAIN Batch 300/1393, Loss   78.8317, POS_NLL_Loss   15.1276, TEXT_NLL_Loss   54.2813, KL_Weight    0.8131, KL_Loss   11.5893\n",
      "TRAIN Batch 350/1393, Loss   85.9976, POS_NLL_Loss   16.8054, TEXT_NLL_Loss   59.0847, KL_Weight    0.8313, KL_Loss   12.1584\n",
      "TRAIN Batch 400/1393, Loss   86.2705, POS_NLL_Loss   17.0897, TEXT_NLL_Loss   59.4628, KL_Weight    0.8481, KL_Loss   11.4581\n",
      "TRAIN Batch 450/1393, Loss   85.4654, POS_NLL_Loss   18.1296, TEXT_NLL_Loss   57.5867, KL_Weight    0.8635, KL_Loss   11.2897\n",
      "TRAIN Batch 500/1393, Loss   82.2561, POS_NLL_Loss   16.7109, TEXT_NLL_Loss   55.6207, KL_Weight    0.8776, KL_Loss   11.3085\n",
      "TRAIN Batch 550/1393, Loss   87.6068, POS_NLL_Loss   17.8342, TEXT_NLL_Loss   60.3809, KL_Weight    0.8904, KL_Loss   10.5475\n",
      "TRAIN Batch 600/1393, Loss   86.9388, POS_NLL_Loss   19.1499, TEXT_NLL_Loss   58.2228, KL_Weight    0.9020, KL_Loss   10.6050\n",
      "TRAIN Batch 650/1393, Loss   87.1722, POS_NLL_Loss   16.2913, TEXT_NLL_Loss   61.8129, KL_Weight    0.9125, KL_Loss    9.9371\n",
      "TRAIN Batch 700/1393, Loss   79.6004, POS_NLL_Loss   18.2229, TEXT_NLL_Loss   51.9168, KL_Weight    0.9220, KL_Loss   10.2609\n",
      "TRAIN Batch 750/1393, Loss   88.4893, POS_NLL_Loss   19.8384, TEXT_NLL_Loss   58.6794, KL_Weight    0.9305, KL_Loss   10.7158\n",
      "TRAIN Batch 800/1393, Loss   69.7800, POS_NLL_Loss   14.0312, TEXT_NLL_Loss   47.7459, KL_Weight    0.9382, KL_Loss    8.5301\n",
      "TRAIN Batch 850/1393, Loss   84.7385, POS_NLL_Loss   17.7612, TEXT_NLL_Loss   57.9077, KL_Weight    0.9451, KL_Loss    9.5969\n",
      "TRAIN Batch 900/1393, Loss   74.0512, POS_NLL_Loss   16.9212, TEXT_NLL_Loss   48.9295, KL_Weight    0.9512, KL_Loss    8.6211\n",
      "TRAIN Batch 950/1393, Loss   79.7964, POS_NLL_Loss   18.2156, TEXT_NLL_Loss   52.9705, KL_Weight    0.9567, KL_Loss    9.0002\n",
      "TRAIN Batch 1000/1393, Loss   84.0278, POS_NLL_Loss   16.7661, TEXT_NLL_Loss   58.3133, KL_Weight    0.9616, KL_Loss    9.3060\n",
      "TRAIN Batch 1050/1393, Loss   80.7650, POS_NLL_Loss   18.1671, TEXT_NLL_Loss   53.9889, KL_Weight    0.9659, KL_Loss    8.9126\n",
      "TRAIN Batch 1100/1393, Loss   80.6163, POS_NLL_Loss   16.6424, TEXT_NLL_Loss   55.8528, KL_Weight    0.9698, KL_Loss    8.3738\n",
      "TRAIN Batch 1150/1393, Loss   77.2859, POS_NLL_Loss   16.6993, TEXT_NLL_Loss   52.4320, KL_Weight    0.9733, KL_Loss    8.3785\n",
      "TRAIN Batch 1200/1393, Loss   80.2012, POS_NLL_Loss   15.9710, TEXT_NLL_Loss   56.0337, KL_Weight    0.9763, KL_Loss    8.3951\n",
      "TRAIN Batch 1250/1393, Loss   77.5039, POS_NLL_Loss   17.6344, TEXT_NLL_Loss   52.0970, KL_Weight    0.9791, KL_Loss    7.9388\n",
      "TRAIN Batch 1300/1393, Loss   79.2187, POS_NLL_Loss   17.1394, TEXT_NLL_Loss   54.2753, KL_Weight    0.9815, KL_Loss    7.9514\n",
      "TRAIN Batch 1350/1393, Loss   86.1522, POS_NLL_Loss   19.1462, TEXT_NLL_Loss   59.2066, KL_Weight    0.9836, KL_Loss    7.9293\n",
      "TRAIN Batch 1393/1393, Loss   85.9896, POS_NLL_Loss   20.1561, TEXT_NLL_Loss   57.2319, KL_Weight    0.9853, KL_Loss    8.7303\n",
      "TRAIN Epoch 02/10, Mean Loss   81.3804\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-10-061639_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E2.pytorch\n",
      "VALID Batch 0/77, Loss   78.3385, POS_NLL_Loss   17.8857, TEXT_NLL_Loss   52.8589, KL_Weight    0.9853, KL_Loss    7.7072\n",
      "VALID Batch 50/77, Loss   88.8487, POS_NLL_Loss   17.9733, TEXT_NLL_Loss   63.1734, KL_Weight    0.9853, KL_Loss    7.8169\n",
      "VALID Batch 77/77, Loss   72.7605, POS_NLL_Loss   15.3368, TEXT_NLL_Loss   50.2166, KL_Weight    0.9853, KL_Loss    7.3146\n",
      "VALID Epoch 02/10, Mean Loss   79.2400\n",
      "TRAIN Batch 0/1393, Loss   78.0027, POS_NLL_Loss   18.4326, TEXT_NLL_Loss   51.5071, KL_Weight    0.9853, KL_Loss    8.1834\n",
      "TRAIN Batch 50/1393, Loss   71.7355, POS_NLL_Loss   15.9428, TEXT_NLL_Loss   48.3560, KL_Weight    0.9870, KL_Loss    7.5346\n",
      "TRAIN Batch 100/1393, Loss   79.0997, POS_NLL_Loss   17.9621, TEXT_NLL_Loss   53.7301, KL_Weight    0.9885, KL_Loss    7.4936\n",
      "TRAIN Batch 150/1393, Loss   67.1594, POS_NLL_Loss   13.5230, TEXT_NLL_Loss   46.3157, KL_Weight    0.9898, KL_Loss    7.3958\n",
      "TRAIN Batch 200/1393, Loss   69.3111, POS_NLL_Loss   15.8246, TEXT_NLL_Loss   45.7283, KL_Weight    0.9910, KL_Loss    7.8285\n",
      "TRAIN Batch 250/1393, Loss   75.7646, POS_NLL_Loss   18.6785, TEXT_NLL_Loss   49.6112, KL_Weight    0.9921, KL_Loss    7.5345\n",
      "TRAIN Batch 300/1393, Loss   81.7953, POS_NLL_Loss   18.7128, TEXT_NLL_Loss   55.3991, KL_Weight    0.9930, KL_Loss    7.7376\n",
      "TRAIN Batch 350/1393, Loss   75.3391, POS_NLL_Loss   17.9142, TEXT_NLL_Loss   50.0104, KL_Weight    0.9938, KL_Loss    7.4607\n",
      "TRAIN Batch 400/1393, Loss   67.0942, POS_NLL_Loss   15.3931, TEXT_NLL_Loss   44.6043, KL_Weight    0.9945, KL_Loss    7.1357\n",
      "TRAIN Batch 450/1393, Loss   78.3298, POS_NLL_Loss   18.1780, TEXT_NLL_Loss   52.1839, KL_Weight    0.9952, KL_Loss    8.0065\n",
      "TRAIN Batch 500/1393, Loss   80.6870, POS_NLL_Loss   18.2457, TEXT_NLL_Loss   54.8147, KL_Weight    0.9957, KL_Loss    7.6592\n",
      "TRAIN Batch 550/1393, Loss   77.9617, POS_NLL_Loss   18.4509, TEXT_NLL_Loss   52.1238, KL_Weight    0.9962, KL_Loss    7.4149\n",
      "TRAIN Batch 600/1393, Loss   70.5772, POS_NLL_Loss   16.2074, TEXT_NLL_Loss   46.8997, KL_Weight    0.9967, KL_Loss    7.4950\n",
      "TRAIN Batch 650/1393, Loss   77.9044, POS_NLL_Loss   18.7005, TEXT_NLL_Loss   51.7918, KL_Weight    0.9971, KL_Loss    7.4339\n",
      "TRAIN Batch 700/1393, Loss   75.6985, POS_NLL_Loss   17.1390, TEXT_NLL_Loss   51.4554, KL_Weight    0.9974, KL_Loss    7.1225\n",
      "TRAIN Batch 750/1393, Loss   80.8333, POS_NLL_Loss   18.4471, TEXT_NLL_Loss   55.1892, KL_Weight    0.9977, KL_Loss    7.2134\n",
      "TRAIN Batch 800/1393, Loss   80.7052, POS_NLL_Loss   18.8781, TEXT_NLL_Loss   54.6414, KL_Weight    0.9980, KL_Loss    7.2003\n",
      "TRAIN Batch 850/1393, Loss   60.9346, POS_NLL_Loss   15.0391, TEXT_NLL_Loss   39.0061, KL_Weight    0.9982, KL_Loss    6.9017\n",
      "TRAIN Batch 900/1393, Loss   75.4867, POS_NLL_Loss   18.4412, TEXT_NLL_Loss   50.2939, KL_Weight    0.9984, KL_Loss    6.7622\n",
      "TRAIN Batch 950/1393, Loss   82.3163, POS_NLL_Loss   19.1433, TEXT_NLL_Loss   56.4623, KL_Weight    0.9986, KL_Loss    6.7201\n",
      "TRAIN Batch 1000/1393, Loss   64.9028, POS_NLL_Loss   18.5998, TEXT_NLL_Loss   39.5148, KL_Weight    0.9988, KL_Loss    6.7965\n",
      "TRAIN Batch 1050/1393, Loss   73.7679, POS_NLL_Loss   18.1273, TEXT_NLL_Loss   49.6857, KL_Weight    0.9989, KL_Loss    5.9613\n",
      "TRAIN Batch 1100/1393, Loss   80.0209, POS_NLL_Loss   18.6296, TEXT_NLL_Loss   54.6974, KL_Weight    0.9990, KL_Loss    6.7003\n",
      "TRAIN Batch 1150/1393, Loss   69.1162, POS_NLL_Loss   16.2114, TEXT_NLL_Loss   46.3511, KL_Weight    0.9992, KL_Loss    6.5593\n",
      "TRAIN Batch 1200/1393, Loss   84.1683, POS_NLL_Loss   20.7292, TEXT_NLL_Loss   56.0859, KL_Weight    0.9993, KL_Loss    7.3586\n",
      "TRAIN Batch 1250/1393, Loss   72.2030, POS_NLL_Loss   16.8953, TEXT_NLL_Loss   48.4837, KL_Weight    0.9993, KL_Loss    6.8284\n",
      "TRAIN Batch 1300/1393, Loss   71.5596, POS_NLL_Loss   17.4533, TEXT_NLL_Loss   47.8154, KL_Weight    0.9994, KL_Loss    6.2945\n",
      "TRAIN Batch 1350/1393, Loss   76.5082, POS_NLL_Loss   18.2063, TEXT_NLL_Loss   52.3267, KL_Weight    0.9995, KL_Loss    5.9782\n",
      "TRAIN Batch 1393/1393, Loss   74.2177, POS_NLL_Loss   18.3997, TEXT_NLL_Loss   49.2417, KL_Weight    0.9995, KL_Loss    6.5793\n",
      "TRAIN Epoch 03/10, Mean Loss   77.3401\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-10-061639_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E3.pytorch\n",
      "VALID Batch 0/77, Loss   76.2163, POS_NLL_Loss   18.8638, TEXT_NLL_Loss   51.0940, KL_Weight    0.9995, KL_Loss    6.2614\n",
      "VALID Batch 50/77, Loss   86.8199, POS_NLL_Loss   18.3851, TEXT_NLL_Loss   62.0681, KL_Weight    0.9995, KL_Loss    6.3696\n",
      "VALID Batch 77/77, Loss   70.6560, POS_NLL_Loss   15.4354, TEXT_NLL_Loss   49.3219, KL_Weight    0.9995, KL_Loss    5.9015\n",
      "VALID Epoch 03/10, Mean Loss   76.1376\n",
      "TRAIN Batch 0/1393, Loss   64.6628, POS_NLL_Loss   15.0382, TEXT_NLL_Loss   43.3076, KL_Weight    0.9995, KL_Loss    6.3199\n",
      "TRAIN Batch 50/1393, Loss   77.1780, POS_NLL_Loss   20.4151, TEXT_NLL_Loss   50.0564, KL_Weight    0.9996, KL_Loss    6.7092\n",
      "TRAIN Batch 100/1393, Loss   79.4755, POS_NLL_Loss   20.3757, TEXT_NLL_Loss   52.5483, KL_Weight    0.9996, KL_Loss    6.5537\n",
      "TRAIN Batch 150/1393, Loss   77.7459, POS_NLL_Loss   18.9586, TEXT_NLL_Loss   52.4349, KL_Weight    0.9997, KL_Loss    6.3544\n",
      "TRAIN Batch 200/1393, Loss   74.9830, POS_NLL_Loss   18.8784, TEXT_NLL_Loss   49.5104, KL_Weight    0.9997, KL_Loss    6.5959\n",
      "TRAIN Batch 250/1393, Loss   79.8021, POS_NLL_Loss   20.7134, TEXT_NLL_Loss   52.4980, KL_Weight    0.9998, KL_Loss    6.5923\n",
      "TRAIN Batch 300/1393, Loss   70.7729, POS_NLL_Loss   17.9484, TEXT_NLL_Loss   46.3356, KL_Weight    0.9998, KL_Loss    6.4903\n",
      "TRAIN Batch 350/1393, Loss   64.9576, POS_NLL_Loss   17.0674, TEXT_NLL_Loss   41.2553, KL_Weight    0.9998, KL_Loss    6.6361\n",
      "TRAIN Batch 400/1393, Loss   76.3831, POS_NLL_Loss   18.8706, TEXT_NLL_Loss   50.6452, KL_Weight    0.9998, KL_Loss    6.8685\n",
      "TRAIN Batch 450/1393, Loss   72.1091, POS_NLL_Loss   17.5302, TEXT_NLL_Loss   48.4565, KL_Weight    0.9999, KL_Loss    6.1233\n",
      "TRAIN Batch 500/1393, Loss   69.6530, POS_NLL_Loss   18.1673, TEXT_NLL_Loss   44.6329, KL_Weight    0.9999, KL_Loss    6.8537\n",
      "TRAIN Batch 550/1393, Loss   68.4598, POS_NLL_Loss   17.8938, TEXT_NLL_Loss   44.5940, KL_Weight    0.9999, KL_Loss    5.9728\n",
      "TRAIN Batch 600/1393, Loss   80.3204, POS_NLL_Loss   19.9402, TEXT_NLL_Loss   53.9625, KL_Weight    0.9999, KL_Loss    6.4183\n",
      "TRAIN Batch 650/1393, Loss   79.2867, POS_NLL_Loss   19.6512, TEXT_NLL_Loss   53.9069, KL_Weight    0.9999, KL_Loss    5.7291\n",
      "TRAIN Batch 700/1393, Loss   77.0104, POS_NLL_Loss   19.9032, TEXT_NLL_Loss   50.6027, KL_Weight    0.9999, KL_Loss    6.5050\n",
      "TRAIN Batch 750/1393, Loss   81.5947, POS_NLL_Loss   18.9104, TEXT_NLL_Loss   55.9394, KL_Weight    0.9999, KL_Loss    6.7453\n",
      "TRAIN Batch 800/1393, Loss   75.7416, POS_NLL_Loss   19.8566, TEXT_NLL_Loss   49.5499, KL_Weight    0.9999, KL_Loss    6.3355\n",
      "TRAIN Batch 850/1393, Loss   82.7328, POS_NLL_Loss   20.0905, TEXT_NLL_Loss   55.6725, KL_Weight    0.9999, KL_Loss    6.9703\n",
      "TRAIN Batch 900/1393, Loss   70.9985, POS_NLL_Loss   18.1980, TEXT_NLL_Loss   46.5621, KL_Weight    1.0000, KL_Loss    6.2387\n",
      "TRAIN Batch 950/1393, Loss   72.4441, POS_NLL_Loss   16.9908, TEXT_NLL_Loss   49.3814, KL_Weight    1.0000, KL_Loss    6.0722\n",
      "TRAIN Batch 1000/1393, Loss   72.9751, POS_NLL_Loss   18.7918, TEXT_NLL_Loss   48.2251, KL_Weight    1.0000, KL_Loss    5.9585\n",
      "TRAIN Batch 1050/1393, Loss   75.4292, POS_NLL_Loss   19.6566, TEXT_NLL_Loss   49.6491, KL_Weight    1.0000, KL_Loss    6.1237\n",
      "TRAIN Batch 1100/1393, Loss   74.1795, POS_NLL_Loss   18.4617, TEXT_NLL_Loss   50.0679, KL_Weight    1.0000, KL_Loss    5.6500\n",
      "TRAIN Batch 1150/1393, Loss   79.8434, POS_NLL_Loss   19.9069, TEXT_NLL_Loss   53.2095, KL_Weight    1.0000, KL_Loss    6.7271\n",
      "TRAIN Batch 1200/1393, Loss   74.8037, POS_NLL_Loss   19.7117, TEXT_NLL_Loss   48.9445, KL_Weight    1.0000, KL_Loss    6.1476\n",
      "TRAIN Batch 1250/1393, Loss   76.8122, POS_NLL_Loss   18.0155, TEXT_NLL_Loss   52.4033, KL_Weight    1.0000, KL_Loss    6.3936\n",
      "TRAIN Batch 1300/1393, Loss   72.1894, POS_NLL_Loss   17.9815, TEXT_NLL_Loss   48.4801, KL_Weight    1.0000, KL_Loss    5.7279\n",
      "TRAIN Batch 1350/1393, Loss   64.8014, POS_NLL_Loss   18.0890, TEXT_NLL_Loss   40.7773, KL_Weight    1.0000, KL_Loss    5.9352\n",
      "TRAIN Batch 1393/1393, Loss   83.9343, POS_NLL_Loss   20.9126, TEXT_NLL_Loss   56.9389, KL_Weight    1.0000, KL_Loss    6.0830\n",
      "TRAIN Epoch 04/10, Mean Loss   73.9259\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-10-061639_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E4.pytorch\n",
      "VALID Batch 0/77, Loss   71.8999, POS_NLL_Loss   18.2346, TEXT_NLL_Loss   47.9895, KL_Weight    1.0000, KL_Loss    5.6758\n",
      "VALID Batch 50/77, Loss   84.9548, POS_NLL_Loss   19.2003, TEXT_NLL_Loss   59.9435, KL_Weight    1.0000, KL_Loss    5.8111\n",
      "VALID Batch 77/77, Loss   68.5265, POS_NLL_Loss   15.7830, TEXT_NLL_Loss   47.4882, KL_Weight    1.0000, KL_Loss    5.2554\n",
      "VALID Epoch 04/10, Mean Loss   73.7953\n",
      "TRAIN Batch 0/1393, Loss   68.4925, POS_NLL_Loss   18.4102, TEXT_NLL_Loss   44.0077, KL_Weight    1.0000, KL_Loss    6.0747\n",
      "TRAIN Batch 50/1393, Loss   67.9973, POS_NLL_Loss   17.7843, TEXT_NLL_Loss   44.4621, KL_Weight    1.0000, KL_Loss    5.7510\n",
      "TRAIN Batch 100/1393, Loss   70.3216, POS_NLL_Loss   18.5487, TEXT_NLL_Loss   46.2328, KL_Weight    1.0000, KL_Loss    5.5401\n",
      "TRAIN Batch 150/1393, Loss   73.3276, POS_NLL_Loss   20.2440, TEXT_NLL_Loss   46.0917, KL_Weight    1.0000, KL_Loss    6.9919\n",
      "TRAIN Batch 200/1393, Loss   73.9524, POS_NLL_Loss   17.9836, TEXT_NLL_Loss   49.6627, KL_Weight    1.0000, KL_Loss    6.3062\n",
      "TRAIN Batch 250/1393, Loss   70.1336, POS_NLL_Loss   18.1030, TEXT_NLL_Loss   45.8505, KL_Weight    1.0000, KL_Loss    6.1801\n",
      "TRAIN Batch 300/1393, Loss   70.2517, POS_NLL_Loss   18.6007, TEXT_NLL_Loss   45.9221, KL_Weight    1.0000, KL_Loss    5.7289\n",
      "TRAIN Batch 350/1393, Loss   78.5017, POS_NLL_Loss   21.0233, TEXT_NLL_Loss   51.6941, KL_Weight    1.0000, KL_Loss    5.7843\n",
      "TRAIN Batch 400/1393, Loss   82.2960, POS_NLL_Loss   23.3751, TEXT_NLL_Loss   53.0719, KL_Weight    1.0000, KL_Loss    5.8491\n",
      "TRAIN Batch 450/1393, Loss   67.5046, POS_NLL_Loss   19.4524, TEXT_NLL_Loss   42.4758, KL_Weight    1.0000, KL_Loss    5.5765\n",
      "TRAIN Batch 500/1393, Loss   69.6039, POS_NLL_Loss   18.5729, TEXT_NLL_Loss   45.1235, KL_Weight    1.0000, KL_Loss    5.9075\n",
      "TRAIN Batch 550/1393, Loss   69.0825, POS_NLL_Loss   16.6647, TEXT_NLL_Loss   46.8310, KL_Weight    1.0000, KL_Loss    5.5868\n",
      "TRAIN Batch 600/1393, Loss   71.4840, POS_NLL_Loss   18.5953, TEXT_NLL_Loss   46.9377, KL_Weight    1.0000, KL_Loss    5.9510\n",
      "TRAIN Batch 650/1393, Loss   81.4215, POS_NLL_Loss   20.9386, TEXT_NLL_Loss   54.3920, KL_Weight    1.0000, KL_Loss    6.0910\n",
      "TRAIN Batch 700/1393, Loss   70.8065, POS_NLL_Loss   17.7837, TEXT_NLL_Loss   47.4135, KL_Weight    1.0000, KL_Loss    5.6093\n",
      "TRAIN Batch 750/1393, Loss   81.6842, POS_NLL_Loss   21.3394, TEXT_NLL_Loss   54.6226, KL_Weight    1.0000, KL_Loss    5.7222\n",
      "TRAIN Batch 800/1393, Loss   72.1480, POS_NLL_Loss   19.7075, TEXT_NLL_Loss   47.0531, KL_Weight    1.0000, KL_Loss    5.3874\n",
      "TRAIN Batch 850/1393, Loss   67.8720, POS_NLL_Loss   17.8829, TEXT_NLL_Loss   44.1414, KL_Weight    1.0000, KL_Loss    5.8477\n",
      "TRAIN Batch 900/1393, Loss   75.6160, POS_NLL_Loss   20.8913, TEXT_NLL_Loss   49.0092, KL_Weight    1.0000, KL_Loss    5.7156\n",
      "TRAIN Batch 950/1393, Loss   68.1909, POS_NLL_Loss   17.9958, TEXT_NLL_Loss   44.6687, KL_Weight    1.0000, KL_Loss    5.5264\n",
      "TRAIN Batch 1000/1393, Loss   74.4445, POS_NLL_Loss   19.3428, TEXT_NLL_Loss   49.6910, KL_Weight    1.0000, KL_Loss    5.4108\n",
      "TRAIN Batch 1050/1393, Loss   73.2902, POS_NLL_Loss   18.8980, TEXT_NLL_Loss   48.4939, KL_Weight    1.0000, KL_Loss    5.8984\n",
      "TRAIN Batch 1100/1393, Loss   62.7679, POS_NLL_Loss   16.3757, TEXT_NLL_Loss   40.9272, KL_Weight    1.0000, KL_Loss    5.4650\n",
      "TRAIN Batch 1150/1393, Loss   67.9744, POS_NLL_Loss   17.6992, TEXT_NLL_Loss   44.9942, KL_Weight    1.0000, KL_Loss    5.2810\n",
      "TRAIN Batch 1200/1393, Loss   72.6125, POS_NLL_Loss   17.6900, TEXT_NLL_Loss   49.6407, KL_Weight    1.0000, KL_Loss    5.2819\n",
      "TRAIN Batch 1250/1393, Loss   66.2157, POS_NLL_Loss   17.9664, TEXT_NLL_Loss   43.1545, KL_Weight    1.0000, KL_Loss    5.0947\n",
      "TRAIN Batch 1300/1393, Loss   72.2041, POS_NLL_Loss   19.8490, TEXT_NLL_Loss   46.6606, KL_Weight    1.0000, KL_Loss    5.6945\n",
      "TRAIN Batch 1350/1393, Loss   73.8121, POS_NLL_Loss   20.3706, TEXT_NLL_Loss   47.2981, KL_Weight    1.0000, KL_Loss    6.1435\n",
      "TRAIN Batch 1393/1393, Loss   68.9850, POS_NLL_Loss   18.6441, TEXT_NLL_Loss   44.5064, KL_Weight    1.0000, KL_Loss    5.8345\n",
      "TRAIN Epoch 05/10, Mean Loss   71.1556\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-10-061639_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E5.pytorch\n",
      "VALID Batch 0/77, Loss   70.6582, POS_NLL_Loss   18.7723, TEXT_NLL_Loss   46.5880, KL_Weight    1.0000, KL_Loss    5.2979\n",
      "VALID Batch 50/77, Loss   83.4133, POS_NLL_Loss   19.3455, TEXT_NLL_Loss   58.6719, KL_Weight    1.0000, KL_Loss    5.3960\n",
      "VALID Batch 77/77, Loss   67.0005, POS_NLL_Loss   16.4791, TEXT_NLL_Loss   45.3985, KL_Weight    1.0000, KL_Loss    5.1228\n",
      "VALID Epoch 05/10, Mean Loss   72.2472\n",
      "TRAIN Batch 0/1393, Loss   62.3495, POS_NLL_Loss   18.0369, TEXT_NLL_Loss   38.7357, KL_Weight    1.0000, KL_Loss    5.5769\n",
      "TRAIN Batch 50/1393, Loss   58.4440, POS_NLL_Loss   15.8650, TEXT_NLL_Loss   37.1580, KL_Weight    1.0000, KL_Loss    5.4209\n",
      "TRAIN Batch 100/1393, Loss   65.1221, POS_NLL_Loss   17.7100, TEXT_NLL_Loss   42.3229, KL_Weight    1.0000, KL_Loss    5.0892\n",
      "TRAIN Batch 150/1393, Loss   72.6756, POS_NLL_Loss   19.3079, TEXT_NLL_Loss   47.9952, KL_Weight    1.0000, KL_Loss    5.3725\n",
      "TRAIN Batch 200/1393, Loss   75.8067, POS_NLL_Loss   19.9514, TEXT_NLL_Loss   50.7775, KL_Weight    1.0000, KL_Loss    5.0778\n",
      "TRAIN Batch 250/1393, Loss   65.7232, POS_NLL_Loss   17.4377, TEXT_NLL_Loss   43.0035, KL_Weight    1.0000, KL_Loss    5.2820\n",
      "TRAIN Batch 300/1393, Loss   63.2950, POS_NLL_Loss   17.1721, TEXT_NLL_Loss   41.0206, KL_Weight    1.0000, KL_Loss    5.1022\n",
      "TRAIN Batch 350/1393, Loss   69.1014, POS_NLL_Loss   19.8602, TEXT_NLL_Loss   43.1927, KL_Weight    1.0000, KL_Loss    6.0485\n",
      "TRAIN Batch 400/1393, Loss   66.8626, POS_NLL_Loss   17.8192, TEXT_NLL_Loss   43.9360, KL_Weight    1.0000, KL_Loss    5.1075\n",
      "TRAIN Batch 450/1393, Loss   73.7300, POS_NLL_Loss   19.5184, TEXT_NLL_Loss   48.3556, KL_Weight    1.0000, KL_Loss    5.8560\n",
      "TRAIN Batch 500/1393, Loss   66.4102, POS_NLL_Loss   18.4570, TEXT_NLL_Loss   42.4993, KL_Weight    1.0000, KL_Loss    5.4540\n",
      "TRAIN Batch 550/1393, Loss   58.8234, POS_NLL_Loss   16.6029, TEXT_NLL_Loss   37.3018, KL_Weight    1.0000, KL_Loss    4.9188\n",
      "TRAIN Batch 600/1393, Loss   71.5141, POS_NLL_Loss   19.7966, TEXT_NLL_Loss   46.4787, KL_Weight    1.0000, KL_Loss    5.2388\n",
      "TRAIN Batch 650/1393, Loss   72.2901, POS_NLL_Loss   19.8964, TEXT_NLL_Loss   47.1572, KL_Weight    1.0000, KL_Loss    5.2364\n",
      "TRAIN Batch 700/1393, Loss   62.4599, POS_NLL_Loss   17.1231, TEXT_NLL_Loss   40.0106, KL_Weight    1.0000, KL_Loss    5.3263\n",
      "TRAIN Batch 750/1393, Loss   70.3997, POS_NLL_Loss   19.5052, TEXT_NLL_Loss   45.6828, KL_Weight    1.0000, KL_Loss    5.2116\n",
      "TRAIN Batch 800/1393, Loss   58.8112, POS_NLL_Loss   17.3543, TEXT_NLL_Loss   36.2093, KL_Weight    1.0000, KL_Loss    5.2476\n",
      "TRAIN Batch 850/1393, Loss   67.5693, POS_NLL_Loss   20.3683, TEXT_NLL_Loss   41.7402, KL_Weight    1.0000, KL_Loss    5.4608\n",
      "TRAIN Batch 900/1393, Loss   70.1191, POS_NLL_Loss   20.1539, TEXT_NLL_Loss   43.8814, KL_Weight    1.0000, KL_Loss    6.0838\n",
      "TRAIN Batch 950/1393, Loss   71.9531, POS_NLL_Loss   19.2638, TEXT_NLL_Loss   47.2682, KL_Weight    1.0000, KL_Loss    5.4211\n",
      "TRAIN Batch 1000/1393, Loss   76.8525, POS_NLL_Loss   21.5439, TEXT_NLL_Loss   49.4351, KL_Weight    1.0000, KL_Loss    5.8736\n",
      "TRAIN Batch 1050/1393, Loss   63.6010, POS_NLL_Loss   16.9989, TEXT_NLL_Loss   41.4165, KL_Weight    1.0000, KL_Loss    5.1856\n",
      "TRAIN Batch 1100/1393, Loss   65.5964, POS_NLL_Loss   17.9625, TEXT_NLL_Loss   42.8604, KL_Weight    1.0000, KL_Loss    4.7735\n",
      "TRAIN Batch 1150/1393, Loss   65.3307, POS_NLL_Loss   16.8898, TEXT_NLL_Loss   43.7979, KL_Weight    1.0000, KL_Loss    4.6429\n",
      "TRAIN Batch 1200/1393, Loss   74.4782, POS_NLL_Loss   19.6742, TEXT_NLL_Loss   49.5382, KL_Weight    1.0000, KL_Loss    5.2659\n",
      "TRAIN Batch 1250/1393, Loss   75.3012, POS_NLL_Loss   20.8881, TEXT_NLL_Loss   48.7144, KL_Weight    1.0000, KL_Loss    5.6986\n",
      "TRAIN Batch 1300/1393, Loss   79.6988, POS_NLL_Loss   21.5324, TEXT_NLL_Loss   52.8746, KL_Weight    1.0000, KL_Loss    5.2918\n",
      "TRAIN Batch 1350/1393, Loss   70.9389, POS_NLL_Loss   19.4241, TEXT_NLL_Loss   46.1712, KL_Weight    1.0000, KL_Loss    5.3436\n",
      "TRAIN Batch 1393/1393, Loss   67.3755, POS_NLL_Loss   18.5618, TEXT_NLL_Loss   43.7472, KL_Weight    1.0000, KL_Loss    5.0665\n",
      "TRAIN Epoch 06/10, Mean Loss   68.9603\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-10-061639_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E6.pytorch\n",
      "VALID Batch 0/77, Loss   69.7952, POS_NLL_Loss   18.7813, TEXT_NLL_Loss   45.9853, KL_Weight    1.0000, KL_Loss    5.0285\n",
      "VALID Batch 50/77, Loss   82.2188, POS_NLL_Loss   19.0419, TEXT_NLL_Loss   57.9923, KL_Weight    1.0000, KL_Loss    5.1846\n",
      "VALID Batch 77/77, Loss   65.0400, POS_NLL_Loss   16.5963, TEXT_NLL_Loss   43.4942, KL_Weight    1.0000, KL_Loss    4.9495\n",
      "VALID Epoch 06/10, Mean Loss   71.0658\n",
      "TRAIN Batch 0/1393, Loss   55.8797, POS_NLL_Loss   16.5947, TEXT_NLL_Loss   34.4990, KL_Weight    1.0000, KL_Loss    4.7860\n",
      "TRAIN Batch 50/1393, Loss   59.3907, POS_NLL_Loss   17.9773, TEXT_NLL_Loss   36.5747, KL_Weight    1.0000, KL_Loss    4.8386\n",
      "TRAIN Batch 100/1393, Loss   70.3269, POS_NLL_Loss   20.2758, TEXT_NLL_Loss   44.7009, KL_Weight    1.0000, KL_Loss    5.3502\n",
      "TRAIN Batch 150/1393, Loss   70.5659, POS_NLL_Loss   20.4734, TEXT_NLL_Loss   44.6816, KL_Weight    1.0000, KL_Loss    5.4108\n",
      "TRAIN Batch 200/1393, Loss   65.9521, POS_NLL_Loss   18.5367, TEXT_NLL_Loss   42.5620, KL_Weight    1.0000, KL_Loss    4.8534\n",
      "TRAIN Batch 250/1393, Loss   64.2140, POS_NLL_Loss   18.1130, TEXT_NLL_Loss   41.1370, KL_Weight    1.0000, KL_Loss    4.9641\n",
      "TRAIN Batch 300/1393, Loss   71.0827, POS_NLL_Loss   20.2321, TEXT_NLL_Loss   45.5721, KL_Weight    1.0000, KL_Loss    5.2785\n",
      "TRAIN Batch 350/1393, Loss   60.8687, POS_NLL_Loss   17.7542, TEXT_NLL_Loss   37.8471, KL_Weight    1.0000, KL_Loss    5.2673\n",
      "TRAIN Batch 400/1393, Loss   78.4831, POS_NLL_Loss   20.6320, TEXT_NLL_Loss   52.3512, KL_Weight    1.0000, KL_Loss    5.5000\n",
      "TRAIN Batch 450/1393, Loss   64.2717, POS_NLL_Loss   17.4410, TEXT_NLL_Loss   41.5260, KL_Weight    1.0000, KL_Loss    5.3047\n",
      "TRAIN Batch 500/1393, Loss   67.4651, POS_NLL_Loss   19.1603, TEXT_NLL_Loss   42.5512, KL_Weight    1.0000, KL_Loss    5.7536\n",
      "TRAIN Batch 550/1393, Loss   61.5543, POS_NLL_Loss   17.4551, TEXT_NLL_Loss   38.9968, KL_Weight    1.0000, KL_Loss    5.1024\n",
      "TRAIN Batch 600/1393, Loss   64.9983, POS_NLL_Loss   18.3836, TEXT_NLL_Loss   41.4253, KL_Weight    1.0000, KL_Loss    5.1893\n",
      "TRAIN Batch 650/1393, Loss   71.7163, POS_NLL_Loss   18.7830, TEXT_NLL_Loss   47.6984, KL_Weight    1.0000, KL_Loss    5.2349\n",
      "TRAIN Batch 700/1393, Loss   68.1776, POS_NLL_Loss   18.0511, TEXT_NLL_Loss   45.0427, KL_Weight    1.0000, KL_Loss    5.0838\n",
      "TRAIN Batch 750/1393, Loss   69.4415, POS_NLL_Loss   19.0084, TEXT_NLL_Loss   45.2155, KL_Weight    1.0000, KL_Loss    5.2176\n",
      "TRAIN Batch 800/1393, Loss   69.9041, POS_NLL_Loss   18.9418, TEXT_NLL_Loss   45.4505, KL_Weight    1.0000, KL_Loss    5.5118\n",
      "TRAIN Batch 850/1393, Loss   68.2328, POS_NLL_Loss   18.1466, TEXT_NLL_Loss   45.2027, KL_Weight    1.0000, KL_Loss    4.8835\n",
      "TRAIN Batch 900/1393, Loss   65.7959, POS_NLL_Loss   18.2414, TEXT_NLL_Loss   42.6996, KL_Weight    1.0000, KL_Loss    4.8549\n",
      "TRAIN Batch 950/1393, Loss   68.0857, POS_NLL_Loss   19.7114, TEXT_NLL_Loss   43.0104, KL_Weight    1.0000, KL_Loss    5.3639\n",
      "TRAIN Batch 1000/1393, Loss   72.1754, POS_NLL_Loss   19.5775, TEXT_NLL_Loss   46.8209, KL_Weight    1.0000, KL_Loss    5.7770\n",
      "TRAIN Batch 1050/1393, Loss   67.6077, POS_NLL_Loss   19.3768, TEXT_NLL_Loss   43.6201, KL_Weight    1.0000, KL_Loss    4.6109\n",
      "TRAIN Batch 1100/1393, Loss   57.2114, POS_NLL_Loss   16.3538, TEXT_NLL_Loss   35.9223, KL_Weight    1.0000, KL_Loss    4.9353\n",
      "TRAIN Batch 1150/1393, Loss   66.6284, POS_NLL_Loss   18.3479, TEXT_NLL_Loss   43.0831, KL_Weight    1.0000, KL_Loss    5.1974\n",
      "TRAIN Batch 1200/1393, Loss   66.1280, POS_NLL_Loss   18.4646, TEXT_NLL_Loss   42.9232, KL_Weight    1.0000, KL_Loss    4.7402\n",
      "TRAIN Batch 1250/1393, Loss   81.9356, POS_NLL_Loss   21.0293, TEXT_NLL_Loss   55.4423, KL_Weight    1.0000, KL_Loss    5.4640\n",
      "TRAIN Batch 1300/1393, Loss   74.4729, POS_NLL_Loss   21.0117, TEXT_NLL_Loss   48.3116, KL_Weight    1.0000, KL_Loss    5.1496\n",
      "TRAIN Batch 1350/1393, Loss   66.2563, POS_NLL_Loss   18.7982, TEXT_NLL_Loss   42.2807, KL_Weight    1.0000, KL_Loss    5.1773\n",
      "TRAIN Batch 1393/1393, Loss   69.2762, POS_NLL_Loss   19.6933, TEXT_NLL_Loss   44.7158, KL_Weight    1.0000, KL_Loss    4.8671\n",
      "TRAIN Epoch 07/10, Mean Loss   67.1449\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-10-061639_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E7.pytorch\n",
      "VALID Batch 0/77, Loss   68.9204, POS_NLL_Loss   18.8749, TEXT_NLL_Loss   45.3159, KL_Weight    1.0000, KL_Loss    4.7295\n",
      "VALID Batch 50/77, Loss   83.0610, POS_NLL_Loss   20.0720, TEXT_NLL_Loss   58.0818, KL_Weight    1.0000, KL_Loss    4.9072\n",
      "VALID Batch 77/77, Loss   65.1119, POS_NLL_Loss   16.2644, TEXT_NLL_Loss   44.1373, KL_Weight    1.0000, KL_Loss    4.7102\n",
      "VALID Epoch 07/10, Mean Loss   70.2722\n",
      "TRAIN Batch 0/1393, Loss   73.0874, POS_NLL_Loss   22.9369, TEXT_NLL_Loss   44.9753, KL_Weight    1.0000, KL_Loss    5.1752\n",
      "TRAIN Batch 50/1393, Loss   73.4101, POS_NLL_Loss   20.9045, TEXT_NLL_Loss   46.7481, KL_Weight    1.0000, KL_Loss    5.7576\n",
      "TRAIN Batch 100/1393, Loss   65.1640, POS_NLL_Loss   20.4096, TEXT_NLL_Loss   39.4418, KL_Weight    1.0000, KL_Loss    5.3126\n",
      "TRAIN Batch 150/1393, Loss   68.2864, POS_NLL_Loss   19.4850, TEXT_NLL_Loss   43.6922, KL_Weight    1.0000, KL_Loss    5.1092\n",
      "TRAIN Batch 200/1393, Loss   64.3864, POS_NLL_Loss   17.7089, TEXT_NLL_Loss   41.5263, KL_Weight    1.0000, KL_Loss    5.1513\n",
      "TRAIN Batch 250/1393, Loss   58.8196, POS_NLL_Loss   18.0849, TEXT_NLL_Loss   35.9405, KL_Weight    1.0000, KL_Loss    4.7942\n",
      "TRAIN Batch 300/1393, Loss   66.1911, POS_NLL_Loss   19.8018, TEXT_NLL_Loss   41.2115, KL_Weight    1.0000, KL_Loss    5.1778\n",
      "TRAIN Batch 350/1393, Loss   63.3349, POS_NLL_Loss   18.6771, TEXT_NLL_Loss   39.6132, KL_Weight    1.0000, KL_Loss    5.0446\n",
      "TRAIN Batch 400/1393, Loss   67.0355, POS_NLL_Loss   18.7609, TEXT_NLL_Loss   42.9422, KL_Weight    1.0000, KL_Loss    5.3323\n",
      "TRAIN Batch 450/1393, Loss   71.1406, POS_NLL_Loss   20.2648, TEXT_NLL_Loss   45.5599, KL_Weight    1.0000, KL_Loss    5.3159\n",
      "TRAIN Batch 500/1393, Loss   66.8646, POS_NLL_Loss   20.1417, TEXT_NLL_Loss   41.1613, KL_Weight    1.0000, KL_Loss    5.5616\n",
      "TRAIN Batch 550/1393, Loss   71.1228, POS_NLL_Loss   18.5792, TEXT_NLL_Loss   47.2873, KL_Weight    1.0000, KL_Loss    5.2563\n",
      "TRAIN Batch 600/1393, Loss   60.2468, POS_NLL_Loss   17.5405, TEXT_NLL_Loss   37.6613, KL_Weight    1.0000, KL_Loss    5.0450\n",
      "TRAIN Batch 650/1393, Loss   67.6974, POS_NLL_Loss   19.4272, TEXT_NLL_Loss   42.9430, KL_Weight    1.0000, KL_Loss    5.3272\n",
      "TRAIN Batch 700/1393, Loss   64.1896, POS_NLL_Loss   17.9676, TEXT_NLL_Loss   41.6070, KL_Weight    1.0000, KL_Loss    4.6150\n",
      "TRAIN Batch 750/1393, Loss   56.1324, POS_NLL_Loss   15.8885, TEXT_NLL_Loss   35.1814, KL_Weight    1.0000, KL_Loss    5.0625\n",
      "TRAIN Batch 800/1393, Loss   66.4373, POS_NLL_Loss   18.6209, TEXT_NLL_Loss   42.0362, KL_Weight    1.0000, KL_Loss    5.7802\n",
      "TRAIN Batch 850/1393, Loss   64.7992, POS_NLL_Loss   18.4424, TEXT_NLL_Loss   41.7032, KL_Weight    1.0000, KL_Loss    4.6536\n",
      "TRAIN Batch 900/1393, Loss   62.9436, POS_NLL_Loss   18.1566, TEXT_NLL_Loss   39.3062, KL_Weight    1.0000, KL_Loss    5.4808\n",
      "TRAIN Batch 950/1393, Loss   62.6444, POS_NLL_Loss   17.9454, TEXT_NLL_Loss   39.7476, KL_Weight    1.0000, KL_Loss    4.9514\n",
      "TRAIN Batch 1000/1393, Loss   66.8315, POS_NLL_Loss   19.8204, TEXT_NLL_Loss   41.5952, KL_Weight    1.0000, KL_Loss    5.4160\n",
      "TRAIN Batch 1050/1393, Loss   70.3548, POS_NLL_Loss   19.5225, TEXT_NLL_Loss   45.9528, KL_Weight    1.0000, KL_Loss    4.8796\n",
      "TRAIN Batch 1100/1393, Loss   70.2570, POS_NLL_Loss   20.6615, TEXT_NLL_Loss   44.7093, KL_Weight    1.0000, KL_Loss    4.8862\n",
      "TRAIN Batch 1150/1393, Loss   66.6371, POS_NLL_Loss   19.5265, TEXT_NLL_Loss   42.1606, KL_Weight    1.0000, KL_Loss    4.9500\n",
      "TRAIN Batch 1200/1393, Loss   66.0991, POS_NLL_Loss   19.0864, TEXT_NLL_Loss   41.4222, KL_Weight    1.0000, KL_Loss    5.5904\n",
      "TRAIN Batch 1250/1393, Loss   69.1117, POS_NLL_Loss   19.6675, TEXT_NLL_Loss   44.3850, KL_Weight    1.0000, KL_Loss    5.0592\n",
      "TRAIN Batch 1300/1393, Loss   67.2913, POS_NLL_Loss   18.6728, TEXT_NLL_Loss   43.6128, KL_Weight    1.0000, KL_Loss    5.0057\n",
      "TRAIN Batch 1350/1393, Loss   64.9773, POS_NLL_Loss   17.5931, TEXT_NLL_Loss   42.5958, KL_Weight    1.0000, KL_Loss    4.7885\n",
      "TRAIN Batch 1393/1393, Loss   73.5161, POS_NLL_Loss   21.7485, TEXT_NLL_Loss   46.8532, KL_Weight    1.0000, KL_Loss    4.9144\n",
      "TRAIN Epoch 08/10, Mean Loss   65.6213\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-10-061639_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E8.pytorch\n",
      "VALID Batch 0/77, Loss   69.9691, POS_NLL_Loss   19.3760, TEXT_NLL_Loss   45.9602, KL_Weight    1.0000, KL_Loss    4.6329\n",
      "VALID Batch 50/77, Loss   80.5674, POS_NLL_Loss   19.2888, TEXT_NLL_Loss   56.5864, KL_Weight    1.0000, KL_Loss    4.6922\n",
      "VALID Batch 77/77, Loss   63.8543, POS_NLL_Loss   16.4005, TEXT_NLL_Loss   42.7698, KL_Weight    1.0000, KL_Loss    4.6840\n",
      "VALID Epoch 08/10, Mean Loss   69.4569\n",
      "TRAIN Batch 0/1393, Loss   62.6884, POS_NLL_Loss   18.4759, TEXT_NLL_Loss   39.4792, KL_Weight    1.0000, KL_Loss    4.7334\n",
      "TRAIN Batch 50/1393, Loss   62.4415, POS_NLL_Loss   20.1121, TEXT_NLL_Loss   37.3526, KL_Weight    1.0000, KL_Loss    4.9768\n",
      "TRAIN Batch 100/1393, Loss   81.1586, POS_NLL_Loss   24.3465, TEXT_NLL_Loss   51.1423, KL_Weight    1.0000, KL_Loss    5.6698\n",
      "TRAIN Batch 150/1393, Loss   61.4519, POS_NLL_Loss   17.9169, TEXT_NLL_Loss   39.1080, KL_Weight    1.0000, KL_Loss    4.4271\n",
      "TRAIN Batch 200/1393, Loss   55.0145, POS_NLL_Loss   16.7545, TEXT_NLL_Loss   33.5511, KL_Weight    1.0000, KL_Loss    4.7089\n",
      "TRAIN Batch 250/1393, Loss   65.8763, POS_NLL_Loss   18.5750, TEXT_NLL_Loss   42.3499, KL_Weight    1.0000, KL_Loss    4.9514\n",
      "TRAIN Batch 300/1393, Loss   61.1465, POS_NLL_Loss   16.7690, TEXT_NLL_Loss   39.9433, KL_Weight    1.0000, KL_Loss    4.4342\n",
      "TRAIN Batch 350/1393, Loss   64.1991, POS_NLL_Loss   19.6285, TEXT_NLL_Loss   39.7877, KL_Weight    1.0000, KL_Loss    4.7829\n",
      "TRAIN Batch 400/1393, Loss   70.5222, POS_NLL_Loss   20.0046, TEXT_NLL_Loss   45.2239, KL_Weight    1.0000, KL_Loss    5.2937\n",
      "TRAIN Batch 450/1393, Loss   62.4609, POS_NLL_Loss   17.7984, TEXT_NLL_Loss   39.7435, KL_Weight    1.0000, KL_Loss    4.9191\n",
      "TRAIN Batch 500/1393, Loss   59.2856, POS_NLL_Loss   17.8331, TEXT_NLL_Loss   36.0966, KL_Weight    1.0000, KL_Loss    5.3559\n",
      "TRAIN Batch 550/1393, Loss   60.7532, POS_NLL_Loss   19.5687, TEXT_NLL_Loss   36.2088, KL_Weight    1.0000, KL_Loss    4.9758\n",
      "TRAIN Batch 600/1393, Loss   65.8672, POS_NLL_Loss   18.9111, TEXT_NLL_Loss   42.1116, KL_Weight    1.0000, KL_Loss    4.8445\n",
      "TRAIN Batch 650/1393, Loss   62.9227, POS_NLL_Loss   18.2545, TEXT_NLL_Loss   39.8089, KL_Weight    1.0000, KL_Loss    4.8593\n",
      "TRAIN Batch 700/1393, Loss   60.8824, POS_NLL_Loss   19.1815, TEXT_NLL_Loss   36.9072, KL_Weight    1.0000, KL_Loss    4.7938\n",
      "TRAIN Batch 750/1393, Loss   71.7562, POS_NLL_Loss   20.5020, TEXT_NLL_Loss   46.1162, KL_Weight    1.0000, KL_Loss    5.1380\n",
      "TRAIN Batch 800/1393, Loss   66.5618, POS_NLL_Loss   20.0564, TEXT_NLL_Loss   41.4157, KL_Weight    1.0000, KL_Loss    5.0897\n",
      "TRAIN Batch 850/1393, Loss   62.7741, POS_NLL_Loss   18.5620, TEXT_NLL_Loss   39.7008, KL_Weight    1.0000, KL_Loss    4.5114\n",
      "TRAIN Batch 900/1393, Loss   63.5028, POS_NLL_Loss   17.4478, TEXT_NLL_Loss   41.7708, KL_Weight    1.0000, KL_Loss    4.2842\n",
      "TRAIN Batch 950/1393, Loss   65.1059, POS_NLL_Loss   19.2819, TEXT_NLL_Loss   41.0915, KL_Weight    1.0000, KL_Loss    4.7325\n",
      "TRAIN Batch 1000/1393, Loss   63.5398, POS_NLL_Loss   19.4783, TEXT_NLL_Loss   39.0243, KL_Weight    1.0000, KL_Loss    5.0372\n",
      "TRAIN Batch 1050/1393, Loss   61.9564, POS_NLL_Loss   19.4585, TEXT_NLL_Loss   37.7890, KL_Weight    1.0000, KL_Loss    4.7089\n",
      "TRAIN Batch 1100/1393, Loss   62.9827, POS_NLL_Loss   17.2009, TEXT_NLL_Loss   41.4762, KL_Weight    1.0000, KL_Loss    4.3056\n",
      "TRAIN Batch 1150/1393, Loss   63.6466, POS_NLL_Loss   19.6889, TEXT_NLL_Loss   38.6906, KL_Weight    1.0000, KL_Loss    5.2671\n",
      "TRAIN Batch 1200/1393, Loss   66.8331, POS_NLL_Loss   19.9952, TEXT_NLL_Loss   42.2089, KL_Weight    1.0000, KL_Loss    4.6290\n",
      "TRAIN Batch 1250/1393, Loss   59.6800, POS_NLL_Loss   18.0865, TEXT_NLL_Loss   37.0938, KL_Weight    1.0000, KL_Loss    4.4997\n",
      "TRAIN Batch 1300/1393, Loss   73.4702, POS_NLL_Loss   22.5811, TEXT_NLL_Loss   45.4958, KL_Weight    1.0000, KL_Loss    5.3932\n",
      "TRAIN Batch 1350/1393, Loss   53.9962, POS_NLL_Loss   15.1047, TEXT_NLL_Loss   34.5297, KL_Weight    1.0000, KL_Loss    4.3618\n",
      "TRAIN Batch 1393/1393, Loss   71.4781, POS_NLL_Loss   23.1619, TEXT_NLL_Loss   42.6977, KL_Weight    1.0000, KL_Loss    5.6185\n",
      "TRAIN Epoch 09/10, Mean Loss   64.2938\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-10-061639_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E9.pytorch\n",
      "VALID Batch 0/77, Loss   67.5222, POS_NLL_Loss   18.7598, TEXT_NLL_Loss   44.2183, KL_Weight    1.0000, KL_Loss    4.5442\n",
      "VALID Batch 50/77, Loss   80.7514, POS_NLL_Loss   20.0302, TEXT_NLL_Loss   56.0095, KL_Weight    1.0000, KL_Loss    4.7117\n",
      "VALID Batch 77/77, Loss   64.8871, POS_NLL_Loss   17.0205, TEXT_NLL_Loss   43.0163, KL_Weight    1.0000, KL_Loss    4.8503\n",
      "VALID Epoch 09/10, Mean Loss   69.0156\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "\n",
    "    for split in splits:\n",
    "        \n",
    "        data_loader = DataLoader(\n",
    "            dataset=_datasets[split],\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=split=='train',\n",
    "            num_workers=cpu_count(),\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "\n",
    "        tracker = defaultdict(Tensor)\n",
    "\n",
    "        # Enable/Disable Dropout\n",
    "        if split == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        for iteration, batch in enumerate(data_loader):\n",
    "            batch_size = batch['src_input'].size(0)\n",
    "            \n",
    "            for k, v in batch.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    batch[k] = to_var(v)\n",
    "            \n",
    "            # model output\n",
    "            b = label_dict = batch\n",
    "            out_dict = model(b['src_input'], b['src_length'], b['pos_input'], b['pos_length'])\n",
    "            mean, logv, z = out_dict['mean'], out_dict['logv'], out_dict['z']\n",
    "            # loss calculation\n",
    "            loss_dict = model.loss(out_dict, label_dict, step, args)\n",
    "            loss = loss_dict['Loss']\n",
    "\n",
    "            # backward + optimization\n",
    "            if split == 'train':\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                step += 1\n",
    "\n",
    "            # for log\n",
    "            loss_value_dict = {}\n",
    "            for _name, _loss in loss_dict.items():\n",
    "                loss_value = _loss.detach().item() if 'detach' in dir(_loss) else _loss\n",
    "                if _name not in ['Loss', 'KL_Weight']:\n",
    "                    loss_value /= batch_size\n",
    "                loss_value_dict[_name] = loss_value\n",
    "                \n",
    "            # bookkeepeing\n",
    "            tracker['Loss'] = torch.cat((tracker['Loss'], loss.detach().view(1)))\n",
    "\n",
    "            if args.tensorboard_logging:\n",
    "                _ = [writer.add_scalar(f'{split.upper()}/{name}', value, epoch*len(data_loader) + iteration) for name, value in loss_value_dict.items()]\n",
    "\n",
    "            if iteration % args.print_every == 0 or iteration+1 == len(data_loader):\n",
    "                print_text = f'{split.upper()} Batch {iteration}/{len(data_loader)-1}'\n",
    "                for k, v in loss_value_dict.items():\n",
    "                    print_text += f', {k} {v:9.4f}'\n",
    "                print(print_text)\n",
    "\n",
    "            if split == 'valid':\n",
    "                def add_tokens_tracker(id_key, token_key, ids, dataset_key, sep=''):\n",
    "                    tracker[id_key] = torch.cat((tracker.get(id_key, LongTensor()), ids.detach()), dim=0)\n",
    "                    tracker[token_key] = tracker.get(token_key, []) + [ids2ptext(text_ids, datasets[dataset_key].i2w, sep=sep) for text_ids in ids]\n",
    "                \n",
    "                add_tokens_tracker('target_text_ids', 'target_texts', batch['tgt_target'].data, ('train', 'tgt'))\n",
    "                add_tokens_tracker('target_pos_ids', 'target_poses', batch['pos_target'].data, ('train', 'pos'), sep=' ')\n",
    "                tracker['z'] = torch.cat((tracker['z'], z.detach()), dim=0)\n",
    "                with torch.no_grad():\n",
    "                    # Single Inference\n",
    "                    pos_decoded_ids = model.pos_inference(z=z) # z → POS\n",
    "                    pos_decoded_input, pos_decoded_length = model.target2input(pos_decoded_ids)  # POS → POS Input\n",
    "                    text_decoded_ids = model.text_inference(pos_decoded_input, pos_decoded_length, z) # POS Input → Text\n",
    "                    add_tokens_tracker('decoded_text_ids', 'decoded_texts',  text_decoded_ids, ('train', 'tgt'))\n",
    "                    add_tokens_tracker('decoded_pos_ids', 'decoded_poses',  pos_decoded_ids, ('train', 'pos'), sep=' ')\n",
    "                    \n",
    "                    # Multiple Inference\n",
    "                    z_list = sample_z(mean, logv, n=10)\n",
    "                    z_list = z_list.permute(1, 0, 2) # バッチ数, サンプル数, 次元数\n",
    "                    text_decoded_ids = [model.pos_text_inference(z=_z)['text_decoded_ids'].tolist() for _z in z_list]\n",
    "                    tracker['multi_text_decoded_ids'] = tracker.get('multi_text_decoded_ids', []) + text_decoded_ids\n",
    "\n",
    "                    \n",
    "        print(\"%s Epoch %02d/%i, Mean Loss %9.4f\"%(split.upper(), epoch, args.epochs, torch.mean(tracker['Loss'])))\n",
    "\n",
    "        if args.tensorboard_logging:\n",
    "            writer.add_scalar(\"%s-Epoch/Loss\"%split.upper(), torch.mean(tracker['Loss']), epoch)\n",
    "        \n",
    "        if split == 'valid':\n",
    "            decoded_id_list = remove_pad_index(tracker['decoded_text_ids'])\n",
    "            multi_decoded_id_list = remove_pad_index(tracker['multi_text_decoded_ids'])\n",
    "            valid_tgt_id_list = remove_pad_index(tracker['target_text_ids'])\n",
    "            train_tgt_id_list = remove_pad_index([d['tgt_target'] for d in _datasets['train']]) # コピー率用\n",
    "            write_tensorboard_valid_metric(writer, valid_tgt_id_list, decoded_id_list, multi_decoded_id_list, train_tgt_id_list, datasets[('train', 'tgt')].i2w, split, epoch)\n",
    "\n",
    "        # save checkpoint\n",
    "        if split == 'train':\n",
    "            checkpoint_path = os.path.join(save_model_path, f\"model_E{epoch}.pytorch\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(\"Model saved at %s\"%checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
