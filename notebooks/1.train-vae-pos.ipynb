{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POSベースモデル\n",
    "<div align=\"center\">\n",
    "<img src='https://user-images.githubusercontent.com/17490886/71782891-a09ebb80-3022-11ea-9558-ad6618f7171b.png' width=1000>\n",
    "</div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil -m rsync -d -r gs://kawamoto-ramiel/experiments_v3_pos_20200104/data ../data/eccos_v2/\n",
    "# ! gsutil -m rsync -d -r ../data/eccos_v2/ gs://kawamoto-ramiel/experiments_v3_pos_20200104/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from multiprocessing import cpu_count\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "from ptb import PTB\n",
    "from utils import idx2word, experiment_name, AttributeDict\n",
    "from models.model_pos import POSVAE\n",
    "from models.model_utils import to_var\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_dir: /root/user/work/src/Sentence-VAE\n",
      "runs_dir: /root/user/work/src/Sentence-VAE/runs\n"
     ]
    }
   ],
   "source": [
    "top_dir = os.path.abspath('..')\n",
    "runs_dir = f'{top_dir}/runs'\n",
    "print(f'top_dir: {top_dir}\\nruns_dir: {runs_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/user/work/src/Sentence-VAE/data/eccos_v2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_base_dir = f'{top_dir}/data'\n",
    "data_name = 'eccos_v2'\n",
    "data_dir = f'{data_base_dir}/{data_name}'\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/root/user/work/src/Sentence-VAE/runs',\n",
       " '/root/user/work/src/Sentence-VAE/runs')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir, save_model_path = runs_dir, runs_dir\n",
    "log_dir, save_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src max: 51, tgt max: 51, pos max: 48\n"
     ]
    }
   ],
   "source": [
    "def readlines(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return [s.replace('\\n', '') for s in f.readlines()]\n",
    "\n",
    "def cal_max_file_lines(path):\n",
    "    lines = readlines(path)\n",
    "    line_lengths = [len(line.split(' ')) for line in lines]\n",
    "    return max(line_lengths)\n",
    "    \n",
    "src_max_length = cal_max_file_lines(f'{data_dir}/src/ptb.train.txt')\n",
    "tgt_max_length = cal_max_file_lines(f'{data_dir}/tgt/ptb.train.txt')\n",
    "pos_max_length = cal_max_file_lines(f'{data_dir}/pos/ptb.train.txt')\n",
    "print(f'src max: {src_max_length}, tgt max: {tgt_max_length}, pos max: {pos_max_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AttrDict{'data_dir': '/root/user/work/src/Sentence-VAE/data/eccos_v2', 'create_data': False, 'src_max_sequence_length': 51, 'tgt_max_sequence_length': 51, 'pos_max_sequence_length': 48, 'min_occ': 1, 'test': False, 'epochs': 10, 'batch_size': 32, 'learning_rate': 0.001, 'embedding_size': 300, 'pos_embedding_size': 20, 'rnn_type': 'gru', 'hidden_size': 256, 'num_layers': 1, 'bidirectional': False, 'latent_size': 16, 'word_dropout': 0, 'embedding_dropout': 0.5, 'anneal_function': 'logistic', 'k': 0.0025, 'x0': 2500, 'print_every': 50, 'tensorboard_logging': True, 'logdir': '/root/user/work/src/Sentence-VAE/runs', 'save_model_path': '/root/user/work/src/Sentence-VAE/runs', 'experiment_name': 'posvae_eccos_v2', 'debug': False}>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = {\n",
    "    'data_dir': data_dir,\n",
    "    'create_data': False,\n",
    "    'src_max_sequence_length': src_max_length,\n",
    "    'tgt_max_sequence_length': tgt_max_length,\n",
    "    'pos_max_sequence_length': pos_max_length,\n",
    "    \n",
    "    'min_occ': 1,\n",
    "    'test': False,\n",
    "\n",
    "    'epochs': 10,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "    \n",
    "    'embedding_size': 300,\n",
    "    'pos_embedding_size': 20,\n",
    "    'rnn_type': 'gru',\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 1,\n",
    "    'bidirectional': False,\n",
    "    'latent_size': 16,\n",
    "    'word_dropout': 0,\n",
    "    'embedding_dropout': 0.5,\n",
    "\n",
    "    'anneal_function': 'logistic',\n",
    "    'k': 0.0025,\n",
    "    'x0': 2500,\n",
    "\n",
    "    'print_every': 50,\n",
    "    'tensorboard_logging': True,\n",
    "    'logdir': log_dir,\n",
    "    'save_model_path': save_model_path,\n",
    "    'experiment_name': f'posvae_{data_name}',\n",
    "    \n",
    "    'debug': False,\n",
    "}\n",
    "\n",
    "args = AttributeDict(args)\n",
    "\n",
    "args.rnn_type = args.rnn_type.lower()\n",
    "args.anneal_function = args.anneal_function.lower()\n",
    "\n",
    "assert args.rnn_type in ['rnn', 'lstm', 'gru']\n",
    "assert args.anneal_function in ['logistic', 'linear']\n",
    "assert 0 <= args.word_dropout <= 1\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /root/user/work/src/Sentence-VAE/data/eccos_v2\n",
      "('train', 'src')\n",
      "vocab: 10293, records: 44596\n",
      "('train', 'tgt')\n",
      "vocab: 10293, records: 44596\n",
      "('train', 'pos')\n",
      "vocab: 16, records: 44596\n",
      "('valid', 'src')\n",
      "vocab: 10293, records: 2477\n",
      "('valid', 'tgt')\n",
      "vocab: 10293, records: 2477\n",
      "('valid', 'pos')\n",
      "vocab: 16, records: 2477\n",
      "CPU times: user 2.89 s, sys: 143 ms, total: 3.03 s\n",
      "Wall time: 3.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import itertools\n",
    "splits = ['train', 'valid'] + (['test'] if args.test else [])\n",
    "data_types = ['src', 'tgt', 'pos']\n",
    "datasets = OrderedDict()\n",
    "print(f'loading {args.data_dir}')\n",
    "for split, src_tgt in itertools.product(splits, data_types):\n",
    "    key = (split, src_tgt)\n",
    "    print(key)\n",
    "    datasets[key] = PTB(\n",
    "        data_dir=f'{args.data_dir}/{src_tgt}',\n",
    "        split=split,\n",
    "        create_data=args.create_data,\n",
    "        max_sequence_length=args.obj[f'{src_tgt}_max_sequence_length'],\n",
    "        min_occ=args.min_occ\n",
    "    )\n",
    "    print(f'vocab: {datasets[key].vocab_size}, records: {len(datasets[key].data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- src --------\n",
      "■ src-input \n",
      "<sos> 特別 な ケア を ルルルン で ! <sep> web 限定 の セット で しっかり お 顔 の 隅々 に まで アプローチ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "■ src-target \n",
      "特別 な ケア を ルルルン で ! <sep> web 限定 の セット で しっかり お 顔 の 隅々 に まで アプローチ <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------- pos --------\n",
      "■ pos-input \n",
      "<sos> 名詞 助動詞 名詞 助詞 名詞 助詞 記号 名詞 名詞 助詞 名詞 助詞 副詞 接頭詞 名詞 助詞 名詞 助詞 助詞 名詞 <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "■ pos-target \n",
      "名詞 助動詞 名詞 助詞 名詞 助詞 記号 名詞 名詞 助詞 名詞 助詞 副詞 接頭詞 名詞 助詞 名詞 助詞 助詞 名詞 <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------- tgt --------\n",
      "■ tgt-input\n",
      "<sos> 特別 な ケア を ルルルン で ! <sep> web 限定 の セット で しっかり お 顔 の 隅々 に まで アプローチ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "■ tgt-target\n",
      "特別 な ケア を ルルルン で ! <sep> web 限定 の セット で しっかり お 顔 の 隅々 に まで アプローチ <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "# 実際のデータ確認\n",
    "def ids2text(id_list, ptb):\n",
    "    return ' '.join([ptb.i2w[f'{i}'] for i in id_list])\n",
    "\n",
    "_ptb_src = datasets[('train', 'src')]\n",
    "_ptb_pos = datasets[('train', 'pos')]\n",
    "_ptb_tgt = datasets[('train', 'tgt')]\n",
    "index = str(100)\n",
    "_sample_src, _sample_tgt, _sample_pos = _ptb_src[index], _ptb_tgt[index], _ptb_pos[index]\n",
    "print(f'------- src --------')\n",
    "print(f'■ src-input \\n{ids2text(_sample_src[\"input\"], _ptb_src)}')\n",
    "print(f'■ src-target \\n{ids2text(_sample_src[\"target\"], _ptb_src)}')\n",
    "print(f'------- pos --------')\n",
    "print(f'■ pos-input \\n{ids2text(_sample_pos[\"input\"], _ptb_pos)}')\n",
    "print(f'■ pos-target \\n{ids2text(_sample_pos[\"target\"], _ptb_pos)}')\n",
    "print(f'------- tgt --------')\n",
    "print(f'■ tgt-input\\n{ids2text(_sample_tgt[\"input\"], _ptb_tgt)}')\n",
    "print(f'■ tgt-target\\n{ids2text(_sample_tgt[\"target\"], _ptb_tgt)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptb import SOS_INDEX, EOS_INDEX, PAD_INDEX, UNK_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10293, 10293)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets[('train', 'src')].w2i), len(datasets[('valid', 'src')].w2i) #, len(datasets[('test', 'src')].w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10293, 10293)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets[('train', 'tgt')].w2i), len(datasets[('valid', 'tgt')].w2i) #, len(datasets[('test', 'tgt')].w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets[('train', 'pos')].w2i), len(datasets[('valid', 'pos')].w2i) #, len(datasets[('test', 'pos')].w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    'text': {'w2i': datasets[('train', 'src')].w2i, }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AttrDict{'data_dir': '/root/user/work/src/Sentence-VAE/data/eccos_v2', 'create_data': False, 'src_max_sequence_length': 51, 'tgt_max_sequence_length': 51, 'pos_max_sequence_length': 48, 'min_occ': 1, 'test': False, 'epochs': 10, 'batch_size': 32, 'learning_rate': 0.001, 'embedding_size': 300, 'pos_embedding_size': 20, 'rnn_type': 'gru', 'hidden_size': 256, 'num_layers': 1, 'bidirectional': False, 'latent_size': 16, 'word_dropout': 0, 'embedding_dropout': 0.5, 'anneal_function': 'logistic', 'k': 0.0025, 'x0': 2500, 'print_every': 50, 'tensorboard_logging': True, 'logdir': '/root/user/work/src/Sentence-VAE/runs', 'save_model_path': '/root/user/work/src/Sentence-VAE/runs', 'experiment_name': 'posvae_eccos_v2', 'debug': False}>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = POSVAE(\n",
    "    vocab_size=datasets[('train', 'src')].vocab_size,\n",
    "    pos_vocab_size=datasets[('train', 'pos')].vocab_size,\n",
    "    embedding_size=args.embedding_size,\n",
    "    pos_embedding_size=args.pos_embedding_size,\n",
    "    \n",
    "    rnn_type=args.rnn_type,\n",
    "    hidden_size=args.hidden_size,\n",
    "    word_dropout=args.word_dropout,\n",
    "    embedding_dropout=args.embedding_dropout,\n",
    "    latent_size=args.latent_size,\n",
    "    num_layers=args.num_layers,\n",
    "    bidirectional=args.bidirectional,\n",
    "    \n",
    "    tgt_max_sequence_length=args.tgt_max_sequence_length,\n",
    "    pos_max_sequence_length=args.pos_max_sequence_length,\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POSVAE(\n",
       "  (embedding): Embedding(10293, 300)\n",
       "  (embedding_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (pos_embedding): Embedding(16, 20)\n",
       "  (encoder_rnn): GRU(300, 256, batch_first=True)\n",
       "  (pos_decoder_rnn): GRU(20, 256, batch_first=True)\n",
       "  (hidden2mean): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (hidden2logv): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (latent2pos_decoder_hidden): Linear(in_features=16, out_features=256, bias=True)\n",
       "  (outputs2pos): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (pos_encoder_rnn): GRU(20, 256, batch_first=True)\n",
       "  (text_decoder_rnn): GRU(300, 256, batch_first=True)\n",
       "  (latent2pos_encoder_hidden): Linear(in_features=16, out_features=256, bias=True)\n",
       "  (outputs2vocab): Linear(in_features=256, out_features=10293, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cstr(obj):\n",
    "    return f'```{obj}```'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_dict(_dict):\n",
    "    return '  \\n'.join([f'{k}: {v}' for k,v in _dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_model_dict(model, args):\n",
    "    meta_dict = {k:v for k, v in model.__dict__.items() if not k[0] == '_'}\n",
    "    meta_dict.update(args.obj)\n",
    "    return meta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard logging: True\n",
      "▼tensorboard logging\n",
      "/root/user/work/src/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-05-173701_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500\n",
      "▼ model save\n",
      "/root/user/work/src/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-05-173701_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models\n"
     ]
    }
   ],
   "source": [
    "print(f'tensorboard logging: {args.tensorboard_logging}')\n",
    "ts = time.strftime('%Y-%m-%d-%H%M%S', time.localtime())\n",
    "exp_name = experiment_name(args,ts)\n",
    "\n",
    "if args.tensorboard_logging:\n",
    "    writer_path = os.path.join(args.logdir, exp_name)\n",
    "    writer = SummaryWriter(writer_path)\n",
    "    writer.add_text(\"model\", cstr(model.__repr__().replace('\\n', '  \\n')))\n",
    "    writer.add_text(\"args\", cstr(str_dict(args.obj)))\n",
    "    writer.add_text(\"ts\", ts)\n",
    "    print(f'▼tensorboard logging\\n{writer_path}')\n",
    "    \n",
    "save_model_path = os.path.join(args.save_model_path, exp_name, 'models')\n",
    "os.makedirs(save_model_path, exist_ok=True)\n",
    "print(f'▼ model save\\n{save_model_path}')\n",
    "\n",
    "# メタパラメータ保存\n",
    "with open(os.path.join(save_model_path, 'model_meta.json'), 'w') as f:\n",
    "    meta_dict = get_meta_model_dict(model, args)\n",
    "    json.dump(meta_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "LongTensor = torch.cuda.LongTensor if torch.cuda.is_available() else torch.LongTensor\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys([('train', 'src'), ('train', 'tgt'), ('train', 'pos'), ('valid', 'src'), ('valid', 'tgt'), ('valid', 'pos')])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> 水 なし ! <sep> 美容 成分 しか 入っ て ない ! <sep> セラミド <num> 倍 ジェル が やばい 笑 <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "水 なし ! <sep> 美容 成分 しか 入っ て ない ! <sep> セラミド <num> 倍 ジェル が やばい 笑 <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "ae_datasets = {split: dataset for (split, src_tgt), dataset in datasets.items() if src_tgt == 'tgt'}\n",
    "print(ids2text(ae_datasets['train'][0]['input'], ae_datasets['train']))\n",
    "print(ids2text(ae_datasets['train'][0]['target'], ae_datasets['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['src', 'tgt', 'pos']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets[('train', 'src')].data['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': array([ 2, 21, 22, 23, 24, 25,  9, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "        36,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'target': array([21, 22, 23, 24, 25,  9, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "         3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'length': 18}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[('train', 'src')]['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 44596\n",
      "valid: 2477\n"
     ]
    }
   ],
   "source": [
    "# ループ内で扱う用に変形\n",
    "_datasets = {}\n",
    "for split in splits:\n",
    "    dataset = []\n",
    "    print(f\"{split}: {len(datasets[(split, 'src')])}\")\n",
    "    for i in range(len(datasets[(split, 'src')])):\n",
    "        _data = {}\n",
    "        for data_type in data_types:\n",
    "            d = datasets[(split, data_type)][f'{i}']\n",
    "            _data.update({f'{data_type}_{k}': v for k, v in d.items()})\n",
    "            _data.update({f'{data_type}_raw': d['input'][1:]})\n",
    "            _data.update({f'{data_type}_raw_length': d['length'] - 1})\n",
    "            \n",
    "        dataset.append(_data)\n",
    "    _datasets[split] = dataset\n",
    "    if args.debug:\n",
    "        _data_limit = 300\n",
    "        _datasets[split] = dataset[:_data_limit]\n",
    "        print(f'debug → {_data_limit}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ptb.PTB at 0x7f3db7370bd0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target_ptb = datasets[('train', 'tgt')]\n",
    "train_target_ptb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ids2ptext\n",
    "from metric import write_tensorboard_valid_metric, remove_pad_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0/1393, Loss  191.1531, POS_NLL_Loss   41.1316, TEXT_NLL_Loss  150.0208, KL_Weight    0.0019, KL_Loss    0.3602\n",
      "TRAIN Batch 50/1393, Loss  140.9181, POS_NLL_Loss   26.0990, TEXT_NLL_Loss  114.7427, KL_Weight    0.0022, KL_Loss   34.9772\n",
      "TRAIN Batch 100/1393, Loss  104.8928, POS_NLL_Loss   19.7447, TEXT_NLL_Loss   85.0582, KL_Weight    0.0025, KL_Loss   36.3390\n",
      "TRAIN Batch 150/1393, Loss  117.6060, POS_NLL_Loss   22.6482, TEXT_NLL_Loss   94.7984, KL_Weight    0.0028, KL_Loss   56.9244\n",
      "TRAIN Batch 200/1393, Loss  121.0281, POS_NLL_Loss   23.5318, TEXT_NLL_Loss   97.2833, KL_Weight    0.0032, KL_Loss   67.1343\n",
      "TRAIN Batch 250/1393, Loss   96.7670, POS_NLL_Loss   18.3009, TEXT_NLL_Loss   78.2233, KL_Weight    0.0036, KL_Loss   67.5764\n",
      "TRAIN Batch 300/1393, Loss   91.2141, POS_NLL_Loss   17.9020, TEXT_NLL_Loss   73.0343, KL_Weight    0.0041, KL_Loss   68.2370\n",
      "TRAIN Batch 350/1393, Loss  109.3915, POS_NLL_Loss   21.1119, TEXT_NLL_Loss   87.9304, KL_Weight    0.0046, KL_Loss   75.7408\n",
      "TRAIN Batch 400/1393, Loss   98.9423, POS_NLL_Loss   21.1205, TEXT_NLL_Loss   77.4750, KL_Weight    0.0052, KL_Loss   66.4426\n",
      "TRAIN Batch 450/1393, Loss   94.9698, POS_NLL_Loss   19.6147, TEXT_NLL_Loss   74.9323, KL_Weight    0.0059, KL_Loss   71.5184\n",
      "TRAIN Batch 500/1393, Loss   94.6273, POS_NLL_Loss   18.0720, TEXT_NLL_Loss   76.0913, KL_Weight    0.0067, KL_Loss   69.3399\n",
      "TRAIN Batch 550/1393, Loss   80.4675, POS_NLL_Loss   16.1649, TEXT_NLL_Loss   63.7569, KL_Weight    0.0076, KL_Loss   72.0112\n",
      "TRAIN Batch 600/1393, Loss  103.4708, POS_NLL_Loss   19.5141, TEXT_NLL_Loss   83.3414, KL_Weight    0.0086, KL_Loss   71.7203\n",
      "TRAIN Batch 650/1393, Loss   95.9192, POS_NLL_Loss   18.8587, TEXT_NLL_Loss   76.3796, KL_Weight    0.0097, KL_Loss   70.1410\n",
      "TRAIN Batch 700/1393, Loss   96.0806, POS_NLL_Loss   19.9949, TEXT_NLL_Loss   75.3575, KL_Weight    0.0110, KL_Loss   66.2757\n",
      "TRAIN Batch 750/1393, Loss   86.2937, POS_NLL_Loss   16.0216, TEXT_NLL_Loss   69.4923, KL_Weight    0.0124, KL_Loss   62.7319\n",
      "TRAIN Batch 800/1393, Loss   95.9057, POS_NLL_Loss   19.5068, TEXT_NLL_Loss   75.4517, KL_Weight    0.0141, KL_Loss   67.3506\n",
      "TRAIN Batch 850/1393, Loss   85.5122, POS_NLL_Loss   16.4547, TEXT_NLL_Loss   68.0312, KL_Weight    0.0159, KL_Loss   64.5176\n",
      "TRAIN Batch 900/1393, Loss   91.5485, POS_NLL_Loss   18.5173, TEXT_NLL_Loss   71.9124, KL_Weight    0.0180, KL_Loss   62.2016\n",
      "TRAIN Batch 950/1393, Loss   83.3488, POS_NLL_Loss   16.1438, TEXT_NLL_Loss   66.0049, KL_Weight    0.0203, KL_Loss   59.0206\n",
      "TRAIN Batch 1000/1393, Loss   77.5064, POS_NLL_Loss   15.1166, TEXT_NLL_Loss   61.0951, KL_Weight    0.0230, KL_Loss   56.3476\n",
      "TRAIN Batch 1050/1393, Loss   79.4205, POS_NLL_Loss   15.3854, TEXT_NLL_Loss   62.6156, KL_Weight    0.0260, KL_Loss   54.6884\n",
      "TRAIN Batch 1100/1393, Loss   79.3631, POS_NLL_Loss   15.7744, TEXT_NLL_Loss   62.0248, KL_Weight    0.0293, KL_Loss   53.3535\n",
      "TRAIN Batch 1150/1393, Loss   78.6730, POS_NLL_Loss   15.3386, TEXT_NLL_Loss   61.6216, KL_Weight    0.0331, KL_Loss   51.7691\n",
      "TRAIN Batch 1200/1393, Loss   78.6135, POS_NLL_Loss   14.3268, TEXT_NLL_Loss   62.3653, KL_Weight    0.0373, KL_Loss   51.4761\n",
      "TRAIN Batch 1250/1393, Loss   66.7255, POS_NLL_Loss   13.6873, TEXT_NLL_Loss   51.0616, KL_Weight    0.0421, KL_Loss   46.9638\n",
      "TRAIN Batch 1300/1393, Loss   85.1405, POS_NLL_Loss   17.9080, TEXT_NLL_Loss   64.8014, KL_Weight    0.0474, KL_Loss   51.2603\n",
      "TRAIN Batch 1350/1393, Loss   92.8086, POS_NLL_Loss   17.6114, TEXT_NLL_Loss   72.6222, KL_Weight    0.0534, KL_Loss   48.2171\n",
      "TRAIN Batch 1393/1393, Loss   74.0902, POS_NLL_Loss   13.4452, TEXT_NLL_Loss   58.0497, KL_Weight    0.0591, KL_Loss   43.9095\n",
      "TRAIN Epoch 00/10, Mean Loss   93.7873\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-05-173701_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E0.pytorch\n",
      "VALID Batch 0/77, Loss   74.7663, POS_NLL_Loss   15.6277, TEXT_NLL_Loss   56.5666, KL_Weight    0.0592, KL_Loss   43.4117\n",
      "VALID Batch 50/77, Loss   83.2646, POS_NLL_Loss   15.9131, TEXT_NLL_Loss   64.6457, KL_Weight    0.0592, KL_Loss   45.6716\n",
      "VALID Batch 77/77, Loss   70.5579, POS_NLL_Loss   12.9575, TEXT_NLL_Loss   55.0747, KL_Weight    0.0592, KL_Loss   42.6313\n",
      "VALID Epoch 00/10, Mean Loss   75.7055\n",
      "TRAIN Batch 0/1393, Loss   67.3549, POS_NLL_Loss   13.5505, TEXT_NLL_Loss   51.3532, KL_Weight    0.0592, KL_Loss   41.3736\n",
      "TRAIN Batch 50/1393, Loss   69.1475, POS_NLL_Loss   14.9122, TEXT_NLL_Loss   51.3888, KL_Weight    0.0666, KL_Loss   42.7350\n",
      "TRAIN Batch 100/1393, Loss   89.7409, POS_NLL_Loss   18.5861, TEXT_NLL_Loss   67.8643, KL_Weight    0.0748, KL_Loss   43.9823\n",
      "TRAIN Batch 150/1393, Loss   77.6231, POS_NLL_Loss   14.1031, TEXT_NLL_Loss   60.2824, KL_Weight    0.0839, KL_Loss   38.5707\n",
      "TRAIN Batch 200/1393, Loss   79.4828, POS_NLL_Loss   17.3176, TEXT_NLL_Loss   58.6014, KL_Weight    0.0941, KL_Loss   37.8879\n",
      "TRAIN Batch 250/1393, Loss   72.6127, POS_NLL_Loss   13.2920, TEXT_NLL_Loss   55.7219, KL_Weight    0.1053, KL_Loss   34.1861\n",
      "TRAIN Batch 300/1393, Loss   76.3679, POS_NLL_Loss   15.1404, TEXT_NLL_Loss   57.0962, KL_Weight    0.1176, KL_Loss   35.1188\n",
      "TRAIN Batch 350/1393, Loss   71.3681, POS_NLL_Loss   14.4977, TEXT_NLL_Loss   52.4294, KL_Weight    0.1312, KL_Loss   33.8379\n",
      "TRAIN Batch 400/1393, Loss   79.5049, POS_NLL_Loss   15.7842, TEXT_NLL_Loss   58.8446, KL_Weight    0.1462, KL_Loss   33.3605\n",
      "TRAIN Batch 450/1393, Loss   73.4036, POS_NLL_Loss   12.8631, TEXT_NLL_Loss   55.5171, KL_Weight    0.1625, KL_Loss   30.9201\n",
      "TRAIN Batch 500/1393, Loss   80.1484, POS_NLL_Loss   16.0800, TEXT_NLL_Loss   58.6924, KL_Weight    0.1802, KL_Loss   29.8335\n",
      "TRAIN Batch 550/1393, Loss   82.3127, POS_NLL_Loss   16.8991, TEXT_NLL_Loss   59.6943, KL_Weight    0.1994, KL_Loss   28.6819\n",
      "TRAIN Batch 600/1393, Loss   83.5702, POS_NLL_Loss   15.5561, TEXT_NLL_Loss   61.9088, KL_Weight    0.2201, KL_Loss   27.7370\n",
      "TRAIN Batch 650/1393, Loss   72.0458, POS_NLL_Loss   14.4588, TEXT_NLL_Loss   51.4322, KL_Weight    0.2423, KL_Loss   25.3993\n",
      "TRAIN Batch 700/1393, Loss   75.2122, POS_NLL_Loss   14.3904, TEXT_NLL_Loss   54.4013, KL_Weight    0.2660, KL_Loss   24.1368\n",
      "TRAIN Batch 750/1393, Loss   85.6322, POS_NLL_Loss   15.7424, TEXT_NLL_Loss   62.8370, KL_Weight    0.2911, KL_Loss   24.2275\n",
      "TRAIN Batch 800/1393, Loss   87.0778, POS_NLL_Loss   17.2959, TEXT_NLL_Loss   62.4859, KL_Weight    0.3176, KL_Loss   22.9753\n",
      "TRAIN Batch 850/1393, Loss   88.7439, POS_NLL_Loss   16.5106, TEXT_NLL_Loss   65.2227, KL_Weight    0.3452, KL_Loss   20.3062\n",
      "TRAIN Batch 900/1393, Loss   74.3690, POS_NLL_Loss   14.6941, TEXT_NLL_Loss   52.3875, KL_Weight    0.3740, KL_Loss   19.4839\n",
      "TRAIN Batch 950/1393, Loss   71.3738, POS_NLL_Loss   14.0077, TEXT_NLL_Loss   49.5185, KL_Weight    0.4037, KL_Loss   19.4385\n",
      "TRAIN Batch 1000/1393, Loss   83.1693, POS_NLL_Loss   17.0375, TEXT_NLL_Loss   58.6473, KL_Weight    0.4341, KL_Loss   17.2400\n",
      "TRAIN Batch 1050/1393, Loss   76.1803, POS_NLL_Loss   15.5624, TEXT_NLL_Loss   52.7950, KL_Weight    0.4651, KL_Loss   16.8214\n",
      "TRAIN Batch 1100/1393, Loss   84.7887, POS_NLL_Loss   18.4866, TEXT_NLL_Loss   57.2689, KL_Weight    0.4963, KL_Loss   18.2030\n",
      "TRAIN Batch 1150/1393, Loss   70.4042, POS_NLL_Loss   14.6249, TEXT_NLL_Loss   47.9197, KL_Weight    0.5275, KL_Loss   14.9005\n",
      "TRAIN Batch 1200/1393, Loss   82.2592, POS_NLL_Loss   17.7596, TEXT_NLL_Loss   55.9260, KL_Weight    0.5585, KL_Loss   15.3516\n",
      "TRAIN Batch 1250/1393, Loss   71.6338, POS_NLL_Loss   14.2510, TEXT_NLL_Loss   48.8273, KL_Weight    0.5890, KL_Loss   14.5245\n",
      "TRAIN Batch 1300/1393, Loss   88.8322, POS_NLL_Loss   15.7561, TEXT_NLL_Loss   63.9046, KL_Weight    0.6189, KL_Loss   14.8183\n",
      "TRAIN Batch 1350/1393, Loss   76.3385, POS_NLL_Loss   15.2253, TEXT_NLL_Loss   52.0192, KL_Weight    0.6479, KL_Loss   14.0351\n",
      "TRAIN Batch 1393/1393, Loss   66.2337, POS_NLL_Loss   13.6130, TEXT_NLL_Loss   44.1718, KL_Weight    0.6721, KL_Loss   12.5716\n",
      "TRAIN Epoch 01/10, Mean Loss   78.7830\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-05-173701_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E1.pytorch\n",
      "VALID Batch 0/77, Loss   77.4523, POS_NLL_Loss   15.6635, TEXT_NLL_Loss   53.5917, KL_Weight    0.6726, KL_Loss   12.1870\n",
      "VALID Batch 50/77, Loss   88.8380, POS_NLL_Loss   17.1361, TEXT_NLL_Loss   62.9014, KL_Weight    0.6726, KL_Loss   13.0841\n",
      "VALID Batch 77/77, Loss   71.5959, POS_NLL_Loss   13.8910, TEXT_NLL_Loss   49.6408, KL_Weight    0.6726, KL_Loss   11.9894\n",
      "VALID Epoch 01/10, Mean Loss   79.2158\n",
      "TRAIN Batch 0/1393, Loss   83.4970, POS_NLL_Loss   16.7467, TEXT_NLL_Loss   57.8260, KL_Weight    0.6726, KL_Loss   13.2683\n",
      "TRAIN Batch 50/1393, Loss   84.5831, POS_NLL_Loss   17.2719, TEXT_NLL_Loss   57.9032, KL_Weight    0.6995, KL_Loss   13.4494\n",
      "TRAIN Batch 100/1393, Loss   78.8374, POS_NLL_Loss   16.2836, TEXT_NLL_Loss   53.7167, KL_Weight    0.7251, KL_Loss   12.1871\n",
      "TRAIN Batch 150/1393, Loss   79.0511, POS_NLL_Loss   16.9490, TEXT_NLL_Loss   52.7646, KL_Weight    0.7493, KL_Loss   12.4612\n",
      "TRAIN Batch 200/1393, Loss   80.6441, POS_NLL_Loss   16.9040, TEXT_NLL_Loss   54.6161, KL_Weight    0.7721, KL_Loss   11.8177\n",
      "TRAIN Batch 250/1393, Loss   85.1143, POS_NLL_Loss   18.2846, TEXT_NLL_Loss   57.5990, KL_Weight    0.7933, KL_Loss   11.6356\n",
      "TRAIN Batch 300/1393, Loss   85.6823, POS_NLL_Loss   16.9312, TEXT_NLL_Loss   59.6791, KL_Weight    0.8131, KL_Loss   11.1580\n",
      "TRAIN Batch 350/1393, Loss   91.3492, POS_NLL_Loss   16.7012, TEXT_NLL_Loss   65.3729, KL_Weight    0.8313, KL_Loss   11.1571\n",
      "TRAIN Batch 400/1393, Loss   75.5983, POS_NLL_Loss   15.6647, TEXT_NLL_Loss   51.4631, KL_Weight    0.8481, KL_Loss    9.9873\n",
      "TRAIN Batch 450/1393, Loss   77.3513, POS_NLL_Loss   17.0451, TEXT_NLL_Loss   51.8909, KL_Weight    0.8635, KL_Loss    9.7452\n",
      "TRAIN Batch 500/1393, Loss   71.6789, POS_NLL_Loss   14.3240, TEXT_NLL_Loss   48.8441, KL_Weight    0.8776, KL_Loss    9.6977\n",
      "TRAIN Batch 550/1393, Loss   81.4399, POS_NLL_Loss   17.0165, TEXT_NLL_Loss   55.3007, KL_Weight    0.8904, KL_Loss   10.2454\n",
      "TRAIN Batch 600/1393, Loss   86.5910, POS_NLL_Loss   18.1758, TEXT_NLL_Loss   59.4784, KL_Weight    0.9020, KL_Loss    9.9074\n",
      "TRAIN Batch 650/1393, Loss   76.9870, POS_NLL_Loss   16.3326, TEXT_NLL_Loss   52.0292, KL_Weight    0.9125, KL_Loss    9.4519\n",
      "TRAIN Batch 700/1393, Loss   71.5590, POS_NLL_Loss   15.2962, TEXT_NLL_Loss   47.8897, KL_Weight    0.9220, KL_Loss    9.0813\n",
      "TRAIN Batch 750/1393, Loss   85.7420, POS_NLL_Loss   18.6312, TEXT_NLL_Loss   58.5790, KL_Weight    0.9305, KL_Loss    9.1687\n",
      "TRAIN Batch 800/1393, Loss   78.1371, POS_NLL_Loss   17.0606, TEXT_NLL_Loss   52.7655, KL_Weight    0.9382, KL_Loss    8.8584\n",
      "TRAIN Batch 850/1393, Loss   73.8264, POS_NLL_Loss   16.3957, TEXT_NLL_Loss   49.8261, KL_Weight    0.9451, KL_Loss    8.0466\n",
      "TRAIN Batch 900/1393, Loss   80.6637, POS_NLL_Loss   17.4896, TEXT_NLL_Loss   54.8351, KL_Weight    0.9512, KL_Loss    8.7668\n",
      "TRAIN Batch 950/1393, Loss   85.7211, POS_NLL_Loss   19.0617, TEXT_NLL_Loss   58.5223, KL_Weight    0.9567, KL_Loss    8.5055\n",
      "TRAIN Batch 1000/1393, Loss   79.4191, POS_NLL_Loss   15.4321, TEXT_NLL_Loss   56.0592, KL_Weight    0.9616, KL_Loss    8.2446\n",
      "TRAIN Batch 1050/1393, Loss   75.8888, POS_NLL_Loss   18.3056, TEXT_NLL_Loss   49.5493, KL_Weight    0.9659, KL_Loss    8.3172\n",
      "TRAIN Batch 1100/1393, Loss   81.6500, POS_NLL_Loss   16.9884, TEXT_NLL_Loss   56.5463, KL_Weight    0.9698, KL_Loss    8.3679\n",
      "TRAIN Batch 1150/1393, Loss   68.2560, POS_NLL_Loss   16.3248, TEXT_NLL_Loss   43.9322, KL_Weight    0.9733, KL_Loss    8.2187\n",
      "TRAIN Batch 1200/1393, Loss   76.2294, POS_NLL_Loss   17.0393, TEXT_NLL_Loss   51.1667, KL_Weight    0.9763, KL_Loss    8.2177\n",
      "TRAIN Batch 1250/1393, Loss   78.9595, POS_NLL_Loss   17.1941, TEXT_NLL_Loss   54.5868, KL_Weight    0.9791, KL_Loss    7.3321\n",
      "TRAIN Batch 1300/1393, Loss   81.7524, POS_NLL_Loss   18.7386, TEXT_NLL_Loss   55.5316, KL_Weight    0.9815, KL_Loss    7.6234\n",
      "TRAIN Batch 1350/1393, Loss   75.2781, POS_NLL_Loss   18.8343, TEXT_NLL_Loss   49.2543, KL_Weight    0.9836, KL_Loss    7.3093\n",
      "TRAIN Batch 1393/1393, Loss   89.2649, POS_NLL_Loss   18.6187, TEXT_NLL_Loss   63.1811, KL_Weight    0.9853, KL_Loss    7.5768\n",
      "TRAIN Epoch 02/10, Mean Loss   79.8436\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-05-173701_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E2.pytorch\n",
      "VALID Batch 0/77, Loss   78.7608, POS_NLL_Loss   18.1893, TEXT_NLL_Loss   53.5521, KL_Weight    0.9853, KL_Loss    7.1241\n",
      "VALID Batch 50/77, Loss   86.4301, POS_NLL_Loss   18.2993, TEXT_NLL_Loss   60.7155, KL_Weight    0.9853, KL_Loss    7.5260\n",
      "VALID Batch 77/77, Loss   71.2915, POS_NLL_Loss   15.8966, TEXT_NLL_Loss   48.6283, KL_Weight    0.9853, KL_Loss    6.8676\n",
      "VALID Epoch 02/10, Mean Loss   77.9925\n",
      "TRAIN Batch 0/1393, Loss   79.8291, POS_NLL_Loss   19.5195, TEXT_NLL_Loss   52.3073, KL_Weight    0.9853, KL_Loss    8.1217\n",
      "TRAIN Batch 50/1393, Loss   72.5798, POS_NLL_Loss   16.4554, TEXT_NLL_Loss   48.9228, KL_Weight    0.9870, KL_Loss    7.2965\n",
      "TRAIN Batch 100/1393, Loss   81.8246, POS_NLL_Loss   20.1666, TEXT_NLL_Loss   53.4253, KL_Weight    0.9885, KL_Loss    8.3284\n",
      "TRAIN Batch 150/1393, Loss   75.5903, POS_NLL_Loss   17.7696, TEXT_NLL_Loss   50.8369, KL_Weight    0.9898, KL_Loss    7.0554\n",
      "TRAIN Batch 200/1393, Loss   72.2728, POS_NLL_Loss   16.2200, TEXT_NLL_Loss   49.3656, KL_Weight    0.9910, KL_Loss    6.7477\n",
      "TRAIN Batch 250/1393, Loss   75.6591, POS_NLL_Loss   16.1432, TEXT_NLL_Loss   52.1787, KL_Weight    0.9921, KL_Loss    7.3959\n",
      "TRAIN Batch 300/1393, Loss   72.5270, POS_NLL_Loss   17.9498, TEXT_NLL_Loss   47.1378, KL_Weight    0.9930, KL_Loss    7.4919\n",
      "TRAIN Batch 350/1393, Loss   68.8653, POS_NLL_Loss   16.9749, TEXT_NLL_Loss   44.8660, KL_Weight    0.9938, KL_Loss    7.0680\n",
      "TRAIN Batch 400/1393, Loss   67.7909, POS_NLL_Loss   15.8654, TEXT_NLL_Loss   45.1744, KL_Weight    0.9945, KL_Loss    6.7881\n",
      "TRAIN Batch 450/1393, Loss   86.8429, POS_NLL_Loss   19.9754, TEXT_NLL_Loss   59.4637, KL_Weight    0.9952, KL_Loss    7.4397\n",
      "TRAIN Batch 500/1393, Loss   70.9334, POS_NLL_Loss   16.6085, TEXT_NLL_Loss   47.1367, KL_Weight    0.9957, KL_Loss    7.2190\n",
      "TRAIN Batch 550/1393, Loss   77.8264, POS_NLL_Loss   18.1536, TEXT_NLL_Loss   53.4010, KL_Weight    0.9962, KL_Loss    6.2956\n",
      "TRAIN Batch 600/1393, Loss   80.1829, POS_NLL_Loss   18.6759, TEXT_NLL_Loss   53.9181, KL_Weight    0.9967, KL_Loss    7.6142\n",
      "TRAIN Batch 650/1393, Loss   68.7933, POS_NLL_Loss   15.0982, TEXT_NLL_Loss   47.1667, KL_Weight    0.9971, KL_Loss    6.5476\n",
      "TRAIN Batch 700/1393, Loss   76.4748, POS_NLL_Loss   18.7980, TEXT_NLL_Loss   49.9921, KL_Weight    0.9974, KL_Loss    7.7046\n",
      "TRAIN Batch 750/1393, Loss   68.4462, POS_NLL_Loss   17.7653, TEXT_NLL_Loss   43.9774, KL_Weight    0.9977, KL_Loss    6.7189\n",
      "TRAIN Batch 800/1393, Loss   72.5301, POS_NLL_Loss   16.3549, TEXT_NLL_Loss   49.2270, KL_Weight    0.9980, KL_Loss    6.9622\n",
      "TRAIN Batch 850/1393, Loss   66.4110, POS_NLL_Loss   15.5889, TEXT_NLL_Loss   43.8330, KL_Weight    0.9982, KL_Loss    7.0016\n",
      "TRAIN Batch 900/1393, Loss   88.8637, POS_NLL_Loss   21.1217, TEXT_NLL_Loss   60.1701, KL_Weight    0.9984, KL_Loss    7.5838\n",
      "TRAIN Batch 950/1393, Loss   79.4810, POS_NLL_Loss   17.6065, TEXT_NLL_Loss   55.0388, KL_Weight    0.9986, KL_Loss    6.8452\n",
      "TRAIN Batch 1000/1393, Loss   71.4462, POS_NLL_Loss   17.4846, TEXT_NLL_Loss   47.7507, KL_Weight    0.9988, KL_Loss    6.2185\n",
      "TRAIN Batch 1050/1393, Loss   88.2421, POS_NLL_Loss   21.9362, TEXT_NLL_Loss   59.4192, KL_Weight    0.9989, KL_Loss    6.8941\n",
      "TRAIN Batch 1100/1393, Loss   73.8461, POS_NLL_Loss   18.0459, TEXT_NLL_Loss   49.4433, KL_Weight    0.9990, KL_Loss    6.3630\n",
      "TRAIN Batch 1150/1393, Loss   74.7661, POS_NLL_Loss   19.1868, TEXT_NLL_Loss   48.5698, KL_Weight    0.9992, KL_Loss    7.0154\n",
      "TRAIN Batch 1200/1393, Loss   69.5216, POS_NLL_Loss   16.7977, TEXT_NLL_Loss   46.5170, KL_Weight    0.9993, KL_Loss    6.2115\n",
      "TRAIN Batch 1250/1393, Loss   73.4693, POS_NLL_Loss   17.9622, TEXT_NLL_Loss   48.9154, KL_Weight    0.9993, KL_Loss    6.5960\n",
      "TRAIN Batch 1300/1393, Loss   68.0939, POS_NLL_Loss   16.9839, TEXT_NLL_Loss   44.7001, KL_Weight    0.9994, KL_Loss    6.4135\n",
      "TRAIN Batch 1350/1393, Loss   78.5294, POS_NLL_Loss   18.8007, TEXT_NLL_Loss   53.4688, KL_Weight    0.9995, KL_Loss    6.2630\n",
      "TRAIN Batch 1393/1393, Loss   77.0708, POS_NLL_Loss   18.1448, TEXT_NLL_Loss   52.0702, KL_Weight    0.9995, KL_Loss    6.8589\n",
      "TRAIN Epoch 03/10, Mean Loss   76.0196\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-05-173701_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E3.pytorch\n",
      "VALID Batch 0/77, Loss   72.8974, POS_NLL_Loss   17.8291, TEXT_NLL_Loss   48.7628, KL_Weight    0.9995, KL_Loss    6.3084\n",
      "VALID Batch 50/77, Loss   84.5260, POS_NLL_Loss   18.5947, TEXT_NLL_Loss   59.1306, KL_Weight    0.9995, KL_Loss    6.8038\n",
      "VALID Batch 77/77, Loss   67.3127, POS_NLL_Loss   15.7920, TEXT_NLL_Loss   45.4119, KL_Weight    0.9995, KL_Loss    6.1115\n",
      "VALID Epoch 03/10, Mean Loss   74.7962\n",
      "TRAIN Batch 0/1393, Loss   71.3439, POS_NLL_Loss   18.3280, TEXT_NLL_Loss   47.0134, KL_Weight    0.9995, KL_Loss    6.0052\n",
      "TRAIN Batch 50/1393, Loss   71.6709, POS_NLL_Loss   17.7579, TEXT_NLL_Loss   47.5455, KL_Weight    0.9996, KL_Loss    6.3701\n",
      "TRAIN Batch 100/1393, Loss   71.5088, POS_NLL_Loss   18.2638, TEXT_NLL_Loss   46.5118, KL_Weight    0.9996, KL_Loss    6.7356\n",
      "TRAIN Batch 150/1393, Loss   77.9645, POS_NLL_Loss   19.0944, TEXT_NLL_Loss   52.0325, KL_Weight    0.9997, KL_Loss    6.8398\n",
      "TRAIN Batch 200/1393, Loss   78.2995, POS_NLL_Loss   19.5492, TEXT_NLL_Loss   52.1150, KL_Weight    0.9997, KL_Loss    6.6372\n",
      "TRAIN Batch 250/1393, Loss   68.7495, POS_NLL_Loss   17.1430, TEXT_NLL_Loss   45.1637, KL_Weight    0.9998, KL_Loss    6.4444\n",
      "TRAIN Batch 300/1393, Loss   70.5022, POS_NLL_Loss   16.0541, TEXT_NLL_Loss   48.8206, KL_Weight    0.9998, KL_Loss    5.6288\n",
      "TRAIN Batch 350/1393, Loss   79.3387, POS_NLL_Loss   19.2657, TEXT_NLL_Loss   53.8256, KL_Weight    0.9998, KL_Loss    6.2486\n",
      "TRAIN Batch 400/1393, Loss   72.7863, POS_NLL_Loss   17.8987, TEXT_NLL_Loss   48.4988, KL_Weight    0.9998, KL_Loss    6.3898\n",
      "TRAIN Batch 450/1393, Loss   68.3550, POS_NLL_Loss   17.0291, TEXT_NLL_Loss   45.8991, KL_Weight    0.9999, KL_Loss    5.4276\n",
      "TRAIN Batch 500/1393, Loss   68.7604, POS_NLL_Loss   18.3927, TEXT_NLL_Loss   43.8366, KL_Weight    0.9999, KL_Loss    6.5319\n",
      "TRAIN Batch 550/1393, Loss   73.2129, POS_NLL_Loss   19.6694, TEXT_NLL_Loss   46.9999, KL_Weight    0.9999, KL_Loss    6.5444\n",
      "TRAIN Batch 600/1393, Loss   71.4046, POS_NLL_Loss   17.8706, TEXT_NLL_Loss   47.6788, KL_Weight    0.9999, KL_Loss    5.8558\n",
      "TRAIN Batch 650/1393, Loss   65.4590, POS_NLL_Loss   15.2637, TEXT_NLL_Loss   44.3682, KL_Weight    0.9999, KL_Loss    5.8276\n",
      "TRAIN Batch 700/1393, Loss   71.7501, POS_NLL_Loss   17.6482, TEXT_NLL_Loss   48.0067, KL_Weight    0.9999, KL_Loss    6.0957\n",
      "TRAIN Batch 750/1393, Loss   72.7349, POS_NLL_Loss   19.6866, TEXT_NLL_Loss   46.7105, KL_Weight    0.9999, KL_Loss    6.3383\n",
      "TRAIN Batch 800/1393, Loss   73.0157, POS_NLL_Loss   17.6188, TEXT_NLL_Loss   49.1274, KL_Weight    0.9999, KL_Loss    6.2699\n",
      "TRAIN Batch 850/1393, Loss   73.6224, POS_NLL_Loss   19.1107, TEXT_NLL_Loss   48.4682, KL_Weight    0.9999, KL_Loss    6.0438\n",
      "TRAIN Batch 900/1393, Loss   70.9282, POS_NLL_Loss   17.7893, TEXT_NLL_Loss   47.2485, KL_Weight    1.0000, KL_Loss    5.8907\n",
      "TRAIN Batch 950/1393, Loss   80.9479, POS_NLL_Loss   20.7863, TEXT_NLL_Loss   53.6539, KL_Weight    1.0000, KL_Loss    6.5079\n",
      "TRAIN Batch 1000/1393, Loss   82.7249, POS_NLL_Loss   18.9451, TEXT_NLL_Loss   57.1377, KL_Weight    1.0000, KL_Loss    6.6424\n",
      "TRAIN Batch 1050/1393, Loss   80.3230, POS_NLL_Loss   18.7414, TEXT_NLL_Loss   55.4115, KL_Weight    1.0000, KL_Loss    6.1703\n",
      "TRAIN Batch 1100/1393, Loss   57.5168, POS_NLL_Loss   16.3936, TEXT_NLL_Loss   35.7001, KL_Weight    1.0000, KL_Loss    5.4233\n",
      "TRAIN Batch 1150/1393, Loss   79.6560, POS_NLL_Loss   20.1706, TEXT_NLL_Loss   52.9483, KL_Weight    1.0000, KL_Loss    6.5373\n",
      "TRAIN Batch 1200/1393, Loss   71.9645, POS_NLL_Loss   19.5500, TEXT_NLL_Loss   46.2499, KL_Weight    1.0000, KL_Loss    6.1647\n",
      "TRAIN Batch 1250/1393, Loss   77.3488, POS_NLL_Loss   18.7708, TEXT_NLL_Loss   52.5798, KL_Weight    1.0000, KL_Loss    5.9984\n",
      "TRAIN Batch 1300/1393, Loss   72.4386, POS_NLL_Loss   17.5543, TEXT_NLL_Loss   48.7481, KL_Weight    1.0000, KL_Loss    6.1364\n",
      "TRAIN Batch 1350/1393, Loss   78.2536, POS_NLL_Loss   19.7528, TEXT_NLL_Loss   51.8618, KL_Weight    1.0000, KL_Loss    6.6392\n",
      "TRAIN Batch 1393/1393, Loss   74.2451, POS_NLL_Loss   18.0373, TEXT_NLL_Loss   50.2368, KL_Weight    1.0000, KL_Loss    5.9711\n",
      "TRAIN Epoch 04/10, Mean Loss   72.7017\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-05-173701_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E4.pytorch\n",
      "VALID Batch 0/77, Loss   72.3599, POS_NLL_Loss   18.2087, TEXT_NLL_Loss   48.6078, KL_Weight    1.0000, KL_Loss    5.5434\n",
      "VALID Batch 50/77, Loss   82.6223, POS_NLL_Loss   18.6504, TEXT_NLL_Loss   58.0564, KL_Weight    1.0000, KL_Loss    5.9155\n",
      "VALID Batch 77/77, Loss   66.3112, POS_NLL_Loss   16.6661, TEXT_NLL_Loss   44.2984, KL_Weight    1.0000, KL_Loss    5.3468\n",
      "VALID Epoch 04/10, Mean Loss   73.1707\n",
      "TRAIN Batch 0/1393, Loss   69.4188, POS_NLL_Loss   20.6545, TEXT_NLL_Loss   42.2854, KL_Weight    1.0000, KL_Loss    6.4790\n",
      "TRAIN Batch 50/1393, Loss   67.6654, POS_NLL_Loss   17.5740, TEXT_NLL_Loss   43.9715, KL_Weight    1.0000, KL_Loss    6.1200\n",
      "TRAIN Batch 100/1393, Loss   72.4093, POS_NLL_Loss   18.8343, TEXT_NLL_Loss   47.2276, KL_Weight    1.0000, KL_Loss    6.3475\n",
      "TRAIN Batch 150/1393, Loss   71.5732, POS_NLL_Loss   19.1567, TEXT_NLL_Loss   46.4024, KL_Weight    1.0000, KL_Loss    6.0141\n",
      "TRAIN Batch 200/1393, Loss   72.2416, POS_NLL_Loss   18.5763, TEXT_NLL_Loss   47.2457, KL_Weight    1.0000, KL_Loss    6.4197\n",
      "TRAIN Batch 250/1393, Loss   64.9222, POS_NLL_Loss   18.7399, TEXT_NLL_Loss   40.1410, KL_Weight    1.0000, KL_Loss    6.0413\n",
      "TRAIN Batch 300/1393, Loss   73.5063, POS_NLL_Loss   18.7919, TEXT_NLL_Loss   49.1566, KL_Weight    1.0000, KL_Loss    5.5578\n",
      "TRAIN Batch 350/1393, Loss   67.9245, POS_NLL_Loss   17.6457, TEXT_NLL_Loss   44.5043, KL_Weight    1.0000, KL_Loss    5.7745\n",
      "TRAIN Batch 400/1393, Loss   70.8860, POS_NLL_Loss   18.5348, TEXT_NLL_Loss   46.3053, KL_Weight    1.0000, KL_Loss    6.0459\n",
      "TRAIN Batch 450/1393, Loss   59.5152, POS_NLL_Loss   15.5792, TEXT_NLL_Loss   38.6467, KL_Weight    1.0000, KL_Loss    5.2893\n",
      "TRAIN Batch 500/1393, Loss   73.0828, POS_NLL_Loss   20.3666, TEXT_NLL_Loss   46.2950, KL_Weight    1.0000, KL_Loss    6.4211\n",
      "TRAIN Batch 550/1393, Loss   78.2114, POS_NLL_Loss   19.7169, TEXT_NLL_Loss   52.1858, KL_Weight    1.0000, KL_Loss    6.3088\n",
      "TRAIN Batch 600/1393, Loss   75.3099, POS_NLL_Loss   19.9802, TEXT_NLL_Loss   49.4538, KL_Weight    1.0000, KL_Loss    5.8759\n",
      "TRAIN Batch 650/1393, Loss   65.3090, POS_NLL_Loss   17.6837, TEXT_NLL_Loss   42.0305, KL_Weight    1.0000, KL_Loss    5.5948\n",
      "TRAIN Batch 700/1393, Loss   64.1635, POS_NLL_Loss   17.3895, TEXT_NLL_Loss   40.9326, KL_Weight    1.0000, KL_Loss    5.8415\n",
      "TRAIN Batch 750/1393, Loss   76.8375, POS_NLL_Loss   21.0379, TEXT_NLL_Loss   50.0250, KL_Weight    1.0000, KL_Loss    5.7745\n",
      "TRAIN Batch 800/1393, Loss   67.2457, POS_NLL_Loss   18.1345, TEXT_NLL_Loss   43.7079, KL_Weight    1.0000, KL_Loss    5.4034\n",
      "TRAIN Batch 850/1393, Loss   65.7260, POS_NLL_Loss   16.7797, TEXT_NLL_Loss   43.2253, KL_Weight    1.0000, KL_Loss    5.7209\n",
      "TRAIN Batch 900/1393, Loss   77.0012, POS_NLL_Loss   20.9298, TEXT_NLL_Loss   49.9244, KL_Weight    1.0000, KL_Loss    6.1469\n",
      "TRAIN Batch 950/1393, Loss   65.5037, POS_NLL_Loss   16.4049, TEXT_NLL_Loss   43.0542, KL_Weight    1.0000, KL_Loss    6.0446\n",
      "TRAIN Batch 1000/1393, Loss   69.4502, POS_NLL_Loss   18.7082, TEXT_NLL_Loss   44.7170, KL_Weight    1.0000, KL_Loss    6.0250\n",
      "TRAIN Batch 1050/1393, Loss   74.6732, POS_NLL_Loss   19.1788, TEXT_NLL_Loss   49.7374, KL_Weight    1.0000, KL_Loss    5.7570\n",
      "TRAIN Batch 1100/1393, Loss   71.1538, POS_NLL_Loss   18.2244, TEXT_NLL_Loss   47.1621, KL_Weight    1.0000, KL_Loss    5.7673\n",
      "TRAIN Batch 1150/1393, Loss   70.3226, POS_NLL_Loss   20.2499, TEXT_NLL_Loss   44.5984, KL_Weight    1.0000, KL_Loss    5.4743\n",
      "TRAIN Batch 1200/1393, Loss   70.3412, POS_NLL_Loss   18.1579, TEXT_NLL_Loss   46.5281, KL_Weight    1.0000, KL_Loss    5.6552\n",
      "TRAIN Batch 1250/1393, Loss   80.4930, POS_NLL_Loss   20.4089, TEXT_NLL_Loss   53.4511, KL_Weight    1.0000, KL_Loss    6.6329\n",
      "TRAIN Batch 1300/1393, Loss   71.6452, POS_NLL_Loss   18.3664, TEXT_NLL_Loss   47.1369, KL_Weight    1.0000, KL_Loss    6.1419\n",
      "TRAIN Batch 1350/1393, Loss   69.9440, POS_NLL_Loss   17.8294, TEXT_NLL_Loss   46.8554, KL_Weight    1.0000, KL_Loss    5.2592\n",
      "TRAIN Batch 1393/1393, Loss   63.4114, POS_NLL_Loss   16.7874, TEXT_NLL_Loss   41.3527, KL_Weight    1.0000, KL_Loss    5.2714\n",
      "TRAIN Epoch 05/10, Mean Loss   70.1454\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-05-173701_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E5.pytorch\n",
      "VALID Batch 0/77, Loss   70.6370, POS_NLL_Loss   18.8022, TEXT_NLL_Loss   46.4189, KL_Weight    1.0000, KL_Loss    5.4160\n",
      "VALID Batch 50/77, Loss   81.4378, POS_NLL_Loss   18.8723, TEXT_NLL_Loss   56.7298, KL_Weight    1.0000, KL_Loss    5.8357\n",
      "VALID Batch 77/77, Loss   66.1189, POS_NLL_Loss   16.7418, TEXT_NLL_Loss   43.9565, KL_Weight    1.0000, KL_Loss    5.4205\n",
      "VALID Epoch 05/10, Mean Loss   71.5143\n",
      "TRAIN Batch 0/1393, Loss   72.9263, POS_NLL_Loss   20.5294, TEXT_NLL_Loss   46.6433, KL_Weight    1.0000, KL_Loss    5.7536\n",
      "TRAIN Batch 50/1393, Loss   61.4618, POS_NLL_Loss   17.1702, TEXT_NLL_Loss   38.8452, KL_Weight    1.0000, KL_Loss    5.4464\n",
      "TRAIN Batch 100/1393, Loss   62.1431, POS_NLL_Loss   16.9075, TEXT_NLL_Loss   39.8342, KL_Weight    1.0000, KL_Loss    5.4014\n",
      "TRAIN Batch 150/1393, Loss   67.6623, POS_NLL_Loss   18.7381, TEXT_NLL_Loss   43.0485, KL_Weight    1.0000, KL_Loss    5.8756\n",
      "TRAIN Batch 200/1393, Loss   75.7686, POS_NLL_Loss   20.0662, TEXT_NLL_Loss   49.8136, KL_Weight    1.0000, KL_Loss    5.8888\n",
      "TRAIN Batch 250/1393, Loss   60.0320, POS_NLL_Loss   17.1211, TEXT_NLL_Loss   37.6188, KL_Weight    1.0000, KL_Loss    5.2921\n",
      "TRAIN Batch 300/1393, Loss   73.3127, POS_NLL_Loss   20.0656, TEXT_NLL_Loss   47.5321, KL_Weight    1.0000, KL_Loss    5.7150\n",
      "TRAIN Batch 350/1393, Loss   68.0118, POS_NLL_Loss   17.1499, TEXT_NLL_Loss   44.9072, KL_Weight    1.0000, KL_Loss    5.9547\n",
      "TRAIN Batch 400/1393, Loss   72.7621, POS_NLL_Loss   20.0349, TEXT_NLL_Loss   47.1127, KL_Weight    1.0000, KL_Loss    5.6146\n",
      "TRAIN Batch 450/1393, Loss   64.2915, POS_NLL_Loss   17.7538, TEXT_NLL_Loss   40.5125, KL_Weight    1.0000, KL_Loss    6.0253\n",
      "TRAIN Batch 500/1393, Loss   63.6495, POS_NLL_Loss   17.9052, TEXT_NLL_Loss   40.2379, KL_Weight    1.0000, KL_Loss    5.5064\n",
      "TRAIN Batch 550/1393, Loss   68.7603, POS_NLL_Loss   20.0679, TEXT_NLL_Loss   42.9531, KL_Weight    1.0000, KL_Loss    5.7393\n",
      "TRAIN Batch 600/1393, Loss   71.4405, POS_NLL_Loss   19.7655, TEXT_NLL_Loss   46.0102, KL_Weight    1.0000, KL_Loss    5.6648\n",
      "TRAIN Batch 650/1393, Loss   66.7139, POS_NLL_Loss   18.6995, TEXT_NLL_Loss   41.4325, KL_Weight    1.0000, KL_Loss    6.5819\n",
      "TRAIN Batch 700/1393, Loss   68.6234, POS_NLL_Loss   18.7622, TEXT_NLL_Loss   43.9911, KL_Weight    1.0000, KL_Loss    5.8701\n",
      "TRAIN Batch 750/1393, Loss   63.8735, POS_NLL_Loss   17.6501, TEXT_NLL_Loss   40.9129, KL_Weight    1.0000, KL_Loss    5.3106\n",
      "TRAIN Batch 800/1393, Loss   63.9214, POS_NLL_Loss   18.1163, TEXT_NLL_Loss   40.2365, KL_Weight    1.0000, KL_Loss    5.5686\n",
      "TRAIN Batch 850/1393, Loss   68.4296, POS_NLL_Loss   18.0471, TEXT_NLL_Loss   44.3305, KL_Weight    1.0000, KL_Loss    6.0521\n",
      "TRAIN Batch 900/1393, Loss   68.1761, POS_NLL_Loss   17.0410, TEXT_NLL_Loss   45.3303, KL_Weight    1.0000, KL_Loss    5.8048\n",
      "TRAIN Batch 950/1393, Loss   67.7587, POS_NLL_Loss   18.8800, TEXT_NLL_Loss   43.4325, KL_Weight    1.0000, KL_Loss    5.4463\n",
      "TRAIN Batch 1000/1393, Loss   66.1217, POS_NLL_Loss   18.8640, TEXT_NLL_Loss   41.8735, KL_Weight    1.0000, KL_Loss    5.3841\n",
      "TRAIN Batch 1050/1393, Loss   63.6153, POS_NLL_Loss   16.2023, TEXT_NLL_Loss   42.0471, KL_Weight    1.0000, KL_Loss    5.3658\n",
      "TRAIN Batch 1100/1393, Loss   67.8050, POS_NLL_Loss   18.2155, TEXT_NLL_Loss   43.7975, KL_Weight    1.0000, KL_Loss    5.7920\n",
      "TRAIN Batch 1150/1393, Loss   66.8632, POS_NLL_Loss   16.8229, TEXT_NLL_Loss   44.3621, KL_Weight    1.0000, KL_Loss    5.6783\n",
      "TRAIN Batch 1200/1393, Loss   68.5447, POS_NLL_Loss   17.7972, TEXT_NLL_Loss   44.8447, KL_Weight    1.0000, KL_Loss    5.9028\n",
      "TRAIN Batch 1250/1393, Loss   73.7043, POS_NLL_Loss   20.9424, TEXT_NLL_Loss   47.0767, KL_Weight    1.0000, KL_Loss    5.6852\n",
      "TRAIN Batch 1300/1393, Loss   58.3989, POS_NLL_Loss   16.4369, TEXT_NLL_Loss   36.4538, KL_Weight    1.0000, KL_Loss    5.5081\n",
      "TRAIN Batch 1350/1393, Loss   72.8117, POS_NLL_Loss   20.0189, TEXT_NLL_Loss   47.1831, KL_Weight    1.0000, KL_Loss    5.6097\n",
      "TRAIN Batch 1393/1393, Loss   64.1793, POS_NLL_Loss   18.3892, TEXT_NLL_Loss   40.5186, KL_Weight    1.0000, KL_Loss    5.2715\n",
      "TRAIN Epoch 06/10, Mean Loss   68.0734\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-05-173701_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E6.pytorch\n",
      "VALID Batch 0/77, Loss   70.1791, POS_NLL_Loss   18.8200, TEXT_NLL_Loss   46.3851, KL_Weight    1.0000, KL_Loss    4.9741\n",
      "VALID Batch 50/77, Loss   81.0186, POS_NLL_Loss   19.1920, TEXT_NLL_Loss   56.4407, KL_Weight    1.0000, KL_Loss    5.3860\n",
      "VALID Batch 77/77, Loss   62.8529, POS_NLL_Loss   16.2987, TEXT_NLL_Loss   41.7026, KL_Weight    1.0000, KL_Loss    4.8516\n",
      "VALID Epoch 06/10, Mean Loss   70.4162\n",
      "TRAIN Batch 0/1393, Loss   78.0841, POS_NLL_Loss   23.7090, TEXT_NLL_Loss   48.5820, KL_Weight    1.0000, KL_Loss    5.7930\n",
      "TRAIN Batch 50/1393, Loss   62.1129, POS_NLL_Loss   18.3623, TEXT_NLL_Loss   38.6142, KL_Weight    1.0000, KL_Loss    5.1364\n",
      "TRAIN Batch 100/1393, Loss   60.9593, POS_NLL_Loss   17.4305, TEXT_NLL_Loss   37.7983, KL_Weight    1.0000, KL_Loss    5.7305\n",
      "TRAIN Batch 150/1393, Loss   67.4705, POS_NLL_Loss   19.5311, TEXT_NLL_Loss   41.9900, KL_Weight    1.0000, KL_Loss    5.9494\n",
      "TRAIN Batch 200/1393, Loss   53.4791, POS_NLL_Loss   14.2347, TEXT_NLL_Loss   34.1644, KL_Weight    1.0000, KL_Loss    5.0800\n",
      "TRAIN Batch 250/1393, Loss   69.7402, POS_NLL_Loss   19.8737, TEXT_NLL_Loss   44.2027, KL_Weight    1.0000, KL_Loss    5.6638\n",
      "TRAIN Batch 300/1393, Loss   66.0472, POS_NLL_Loss   19.2447, TEXT_NLL_Loss   41.2458, KL_Weight    1.0000, KL_Loss    5.5567\n",
      "TRAIN Batch 350/1393, Loss   59.6985, POS_NLL_Loss   16.4569, TEXT_NLL_Loss   38.0601, KL_Weight    1.0000, KL_Loss    5.1815\n",
      "TRAIN Batch 400/1393, Loss   71.1096, POS_NLL_Loss   19.8023, TEXT_NLL_Loss   46.0105, KL_Weight    1.0000, KL_Loss    5.2968\n",
      "TRAIN Batch 450/1393, Loss   65.1547, POS_NLL_Loss   19.5527, TEXT_NLL_Loss   40.1949, KL_Weight    1.0000, KL_Loss    5.4071\n",
      "TRAIN Batch 500/1393, Loss   70.7889, POS_NLL_Loss   19.5160, TEXT_NLL_Loss   45.6215, KL_Weight    1.0000, KL_Loss    5.6514\n",
      "TRAIN Batch 550/1393, Loss   66.2449, POS_NLL_Loss   20.4236, TEXT_NLL_Loss   39.9490, KL_Weight    1.0000, KL_Loss    5.8723\n",
      "TRAIN Batch 600/1393, Loss   55.8517, POS_NLL_Loss   15.9169, TEXT_NLL_Loss   35.1958, KL_Weight    1.0000, KL_Loss    4.7390\n",
      "TRAIN Batch 650/1393, Loss   69.5587, POS_NLL_Loss   19.7945, TEXT_NLL_Loss   43.9905, KL_Weight    1.0000, KL_Loss    5.7736\n",
      "TRAIN Batch 700/1393, Loss   57.5608, POS_NLL_Loss   16.7993, TEXT_NLL_Loss   35.7045, KL_Weight    1.0000, KL_Loss    5.0569\n",
      "TRAIN Batch 750/1393, Loss   60.3834, POS_NLL_Loss   19.2325, TEXT_NLL_Loss   35.6005, KL_Weight    1.0000, KL_Loss    5.5504\n",
      "TRAIN Batch 800/1393, Loss   63.1432, POS_NLL_Loss   17.2743, TEXT_NLL_Loss   40.6294, KL_Weight    1.0000, KL_Loss    5.2394\n",
      "TRAIN Batch 850/1393, Loss   62.3423, POS_NLL_Loss   17.1125, TEXT_NLL_Loss   40.1581, KL_Weight    1.0000, KL_Loss    5.0717\n",
      "TRAIN Batch 900/1393, Loss   69.3636, POS_NLL_Loss   20.2226, TEXT_NLL_Loss   43.6914, KL_Weight    1.0000, KL_Loss    5.4496\n",
      "TRAIN Batch 950/1393, Loss   68.3769, POS_NLL_Loss   18.3622, TEXT_NLL_Loss   44.3089, KL_Weight    1.0000, KL_Loss    5.7057\n",
      "TRAIN Batch 1000/1393, Loss   63.3876, POS_NLL_Loss   17.8390, TEXT_NLL_Loss   40.1598, KL_Weight    1.0000, KL_Loss    5.3888\n",
      "TRAIN Batch 1050/1393, Loss   65.0700, POS_NLL_Loss   19.5295, TEXT_NLL_Loss   40.0892, KL_Weight    1.0000, KL_Loss    5.4512\n",
      "TRAIN Batch 1100/1393, Loss   73.1472, POS_NLL_Loss   19.4624, TEXT_NLL_Loss   48.1068, KL_Weight    1.0000, KL_Loss    5.5780\n",
      "TRAIN Batch 1150/1393, Loss   64.2068, POS_NLL_Loss   19.2628, TEXT_NLL_Loss   39.6605, KL_Weight    1.0000, KL_Loss    5.2836\n",
      "TRAIN Batch 1200/1393, Loss   69.6481, POS_NLL_Loss   20.0236, TEXT_NLL_Loss   43.9497, KL_Weight    1.0000, KL_Loss    5.6748\n",
      "TRAIN Batch 1250/1393, Loss   71.3984, POS_NLL_Loss   20.7973, TEXT_NLL_Loss   45.1348, KL_Weight    1.0000, KL_Loss    5.4664\n",
      "TRAIN Batch 1300/1393, Loss   61.4399, POS_NLL_Loss   17.3395, TEXT_NLL_Loss   39.0842, KL_Weight    1.0000, KL_Loss    5.0162\n",
      "TRAIN Batch 1350/1393, Loss   76.5265, POS_NLL_Loss   20.6142, TEXT_NLL_Loss   50.3243, KL_Weight    1.0000, KL_Loss    5.5880\n",
      "TRAIN Batch 1393/1393, Loss   62.1565, POS_NLL_Loss   18.3736, TEXT_NLL_Loss   38.8483, KL_Weight    1.0000, KL_Loss    4.9346\n",
      "TRAIN Epoch 07/10, Mean Loss   66.3802\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-05-173701_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E7.pytorch\n",
      "VALID Batch 0/77, Loss   70.3691, POS_NLL_Loss   18.9024, TEXT_NLL_Loss   46.4143, KL_Weight    1.0000, KL_Loss    5.0524\n",
      "VALID Batch 50/77, Loss   81.3215, POS_NLL_Loss   18.8207, TEXT_NLL_Loss   56.8996, KL_Weight    1.0000, KL_Loss    5.6013\n",
      "VALID Batch 77/77, Loss   62.2385, POS_NLL_Loss   16.1686, TEXT_NLL_Loss   40.9908, KL_Weight    1.0000, KL_Loss    5.0791\n",
      "VALID Epoch 07/10, Mean Loss   69.8740\n",
      "TRAIN Batch 0/1393, Loss   67.4807, POS_NLL_Loss   19.2148, TEXT_NLL_Loss   42.6811, KL_Weight    1.0000, KL_Loss    5.5847\n",
      "TRAIN Batch 50/1393, Loss   62.1057, POS_NLL_Loss   18.5751, TEXT_NLL_Loss   38.4310, KL_Weight    1.0000, KL_Loss    5.0996\n",
      "TRAIN Batch 100/1393, Loss   64.4587, POS_NLL_Loss   17.8247, TEXT_NLL_Loss   41.8113, KL_Weight    1.0000, KL_Loss    4.8227\n",
      "TRAIN Batch 150/1393, Loss   69.7774, POS_NLL_Loss   20.2776, TEXT_NLL_Loss   43.9854, KL_Weight    1.0000, KL_Loss    5.5144\n",
      "TRAIN Batch 200/1393, Loss   65.5341, POS_NLL_Loss   19.2565, TEXT_NLL_Loss   40.6290, KL_Weight    1.0000, KL_Loss    5.6486\n",
      "TRAIN Batch 250/1393, Loss   62.2552, POS_NLL_Loss   18.5729, TEXT_NLL_Loss   38.1626, KL_Weight    1.0000, KL_Loss    5.5198\n",
      "TRAIN Batch 300/1393, Loss   65.4462, POS_NLL_Loss   18.6144, TEXT_NLL_Loss   41.4399, KL_Weight    1.0000, KL_Loss    5.3919\n",
      "TRAIN Batch 350/1393, Loss   68.8116, POS_NLL_Loss   20.3256, TEXT_NLL_Loss   42.8776, KL_Weight    1.0000, KL_Loss    5.6084\n",
      "TRAIN Batch 400/1393, Loss   63.8416, POS_NLL_Loss   20.2187, TEXT_NLL_Loss   37.7406, KL_Weight    1.0000, KL_Loss    5.8823\n",
      "TRAIN Batch 450/1393, Loss   61.0270, POS_NLL_Loss   16.5636, TEXT_NLL_Loss   39.1060, KL_Weight    1.0000, KL_Loss    5.3574\n",
      "TRAIN Batch 500/1393, Loss   65.2747, POS_NLL_Loss   19.3499, TEXT_NLL_Loss   40.7520, KL_Weight    1.0000, KL_Loss    5.1728\n",
      "TRAIN Batch 550/1393, Loss   58.3359, POS_NLL_Loss   19.2811, TEXT_NLL_Loss   33.2399, KL_Weight    1.0000, KL_Loss    5.8150\n",
      "TRAIN Batch 600/1393, Loss   62.2544, POS_NLL_Loss   19.7916, TEXT_NLL_Loss   37.3981, KL_Weight    1.0000, KL_Loss    5.0647\n",
      "TRAIN Batch 650/1393, Loss   62.9490, POS_NLL_Loss   18.8103, TEXT_NLL_Loss   38.6803, KL_Weight    1.0000, KL_Loss    5.4584\n",
      "TRAIN Batch 700/1393, Loss   72.7009, POS_NLL_Loss   20.9683, TEXT_NLL_Loss   46.1342, KL_Weight    1.0000, KL_Loss    5.5984\n",
      "TRAIN Batch 750/1393, Loss   56.8408, POS_NLL_Loss   16.7981, TEXT_NLL_Loss   35.0961, KL_Weight    1.0000, KL_Loss    4.9466\n",
      "TRAIN Batch 800/1393, Loss   61.1535, POS_NLL_Loss   17.0796, TEXT_NLL_Loss   39.0205, KL_Weight    1.0000, KL_Loss    5.0535\n",
      "TRAIN Batch 850/1393, Loss   64.2927, POS_NLL_Loss   18.3925, TEXT_NLL_Loss   40.9688, KL_Weight    1.0000, KL_Loss    4.9314\n",
      "TRAIN Batch 900/1393, Loss   69.6304, POS_NLL_Loss   20.8485, TEXT_NLL_Loss   42.9415, KL_Weight    1.0000, KL_Loss    5.8403\n",
      "TRAIN Batch 950/1393, Loss   64.2312, POS_NLL_Loss   17.9465, TEXT_NLL_Loss   40.8484, KL_Weight    1.0000, KL_Loss    5.4362\n",
      "TRAIN Batch 1000/1393, Loss   69.2972, POS_NLL_Loss   20.1719, TEXT_NLL_Loss   43.6770, KL_Weight    1.0000, KL_Loss    5.4483\n",
      "TRAIN Batch 1050/1393, Loss   67.5052, POS_NLL_Loss   19.4610, TEXT_NLL_Loss   43.0521, KL_Weight    1.0000, KL_Loss    4.9922\n",
      "TRAIN Batch 1100/1393, Loss   68.3640, POS_NLL_Loss   19.8670, TEXT_NLL_Loss   42.8650, KL_Weight    1.0000, KL_Loss    5.6321\n",
      "TRAIN Batch 1150/1393, Loss   61.6964, POS_NLL_Loss   19.8914, TEXT_NLL_Loss   36.5124, KL_Weight    1.0000, KL_Loss    5.2926\n",
      "TRAIN Batch 1200/1393, Loss   60.5837, POS_NLL_Loss   16.7272, TEXT_NLL_Loss   38.9261, KL_Weight    1.0000, KL_Loss    4.9305\n",
      "TRAIN Batch 1250/1393, Loss   59.5041, POS_NLL_Loss   17.2554, TEXT_NLL_Loss   37.2584, KL_Weight    1.0000, KL_Loss    4.9902\n",
      "TRAIN Batch 1300/1393, Loss   61.1668, POS_NLL_Loss   18.4782, TEXT_NLL_Loss   37.6929, KL_Weight    1.0000, KL_Loss    4.9958\n",
      "TRAIN Batch 1350/1393, Loss   76.7261, POS_NLL_Loss   21.4921, TEXT_NLL_Loss   49.7852, KL_Weight    1.0000, KL_Loss    5.4489\n",
      "TRAIN Batch 1393/1393, Loss   63.1798, POS_NLL_Loss   17.7130, TEXT_NLL_Loss   40.3033, KL_Weight    1.0000, KL_Loss    5.1635\n",
      "TRAIN Epoch 08/10, Mean Loss   64.9263\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-05-173701_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E8.pytorch\n",
      "VALID Batch 0/77, Loss   69.6196, POS_NLL_Loss   19.1631, TEXT_NLL_Loss   45.5903, KL_Weight    1.0000, KL_Loss    4.8662\n",
      "VALID Batch 50/77, Loss   80.3417, POS_NLL_Loss   18.7168, TEXT_NLL_Loss   56.3209, KL_Weight    1.0000, KL_Loss    5.3040\n",
      "VALID Batch 77/77, Loss   64.1944, POS_NLL_Loss   16.5113, TEXT_NLL_Loss   42.8846, KL_Weight    1.0000, KL_Loss    4.7985\n",
      "VALID Epoch 08/10, Mean Loss   69.7825\n",
      "TRAIN Batch 0/1393, Loss   68.6916, POS_NLL_Loss   20.4692, TEXT_NLL_Loss   42.5935, KL_Weight    1.0000, KL_Loss    5.6289\n",
      "TRAIN Batch 50/1393, Loss   67.5581, POS_NLL_Loss   21.4372, TEXT_NLL_Loss   39.9585, KL_Weight    1.0000, KL_Loss    6.1624\n",
      "TRAIN Batch 100/1393, Loss   58.8786, POS_NLL_Loss   17.1208, TEXT_NLL_Loss   36.4776, KL_Weight    1.0000, KL_Loss    5.2801\n",
      "TRAIN Batch 150/1393, Loss   62.6120, POS_NLL_Loss   19.1702, TEXT_NLL_Loss   38.2836, KL_Weight    1.0000, KL_Loss    5.1581\n",
      "TRAIN Batch 200/1393, Loss   59.2528, POS_NLL_Loss   18.2501, TEXT_NLL_Loss   35.7166, KL_Weight    1.0000, KL_Loss    5.2860\n",
      "TRAIN Batch 250/1393, Loss   68.0392, POS_NLL_Loss   19.6974, TEXT_NLL_Loss   42.9783, KL_Weight    1.0000, KL_Loss    5.3635\n",
      "TRAIN Batch 300/1393, Loss   49.1562, POS_NLL_Loss   14.7547, TEXT_NLL_Loss   29.2458, KL_Weight    1.0000, KL_Loss    5.1557\n",
      "TRAIN Batch 350/1393, Loss   62.2958, POS_NLL_Loss   18.6516, TEXT_NLL_Loss   38.3571, KL_Weight    1.0000, KL_Loss    5.2872\n",
      "TRAIN Batch 400/1393, Loss   59.4972, POS_NLL_Loss   18.0916, TEXT_NLL_Loss   35.6758, KL_Weight    1.0000, KL_Loss    5.7297\n",
      "TRAIN Batch 450/1393, Loss   59.6629, POS_NLL_Loss   18.9240, TEXT_NLL_Loss   35.7035, KL_Weight    1.0000, KL_Loss    5.0354\n",
      "TRAIN Batch 500/1393, Loss   65.4513, POS_NLL_Loss   19.2970, TEXT_NLL_Loss   40.7991, KL_Weight    1.0000, KL_Loss    5.3551\n",
      "TRAIN Batch 550/1393, Loss   60.2525, POS_NLL_Loss   17.7107, TEXT_NLL_Loss   37.2363, KL_Weight    1.0000, KL_Loss    5.3055\n",
      "TRAIN Batch 600/1393, Loss   60.1844, POS_NLL_Loss   17.5445, TEXT_NLL_Loss   38.0601, KL_Weight    1.0000, KL_Loss    4.5797\n",
      "TRAIN Batch 650/1393, Loss   70.5809, POS_NLL_Loss   19.9899, TEXT_NLL_Loss   45.0110, KL_Weight    1.0000, KL_Loss    5.5800\n",
      "TRAIN Batch 700/1393, Loss   67.4501, POS_NLL_Loss   19.5033, TEXT_NLL_Loss   42.6487, KL_Weight    1.0000, KL_Loss    5.2981\n",
      "TRAIN Batch 750/1393, Loss   70.5989, POS_NLL_Loss   21.8997, TEXT_NLL_Loss   43.5301, KL_Weight    1.0000, KL_Loss    5.1690\n",
      "TRAIN Batch 800/1393, Loss   53.7753, POS_NLL_Loss   16.1636, TEXT_NLL_Loss   32.6490, KL_Weight    1.0000, KL_Loss    4.9627\n",
      "TRAIN Batch 850/1393, Loss   73.0761, POS_NLL_Loss   22.5736, TEXT_NLL_Loss   44.9075, KL_Weight    1.0000, KL_Loss    5.5950\n",
      "TRAIN Batch 900/1393, Loss   66.0100, POS_NLL_Loss   19.0252, TEXT_NLL_Loss   41.5786, KL_Weight    1.0000, KL_Loss    5.4062\n",
      "TRAIN Batch 950/1393, Loss   63.9144, POS_NLL_Loss   18.5374, TEXT_NLL_Loss   39.3914, KL_Weight    1.0000, KL_Loss    5.9857\n",
      "TRAIN Batch 1000/1393, Loss   66.5633, POS_NLL_Loss   19.0373, TEXT_NLL_Loss   42.0833, KL_Weight    1.0000, KL_Loss    5.4428\n",
      "TRAIN Batch 1050/1393, Loss   64.4207, POS_NLL_Loss   19.7556, TEXT_NLL_Loss   39.2343, KL_Weight    1.0000, KL_Loss    5.4308\n",
      "TRAIN Batch 1100/1393, Loss   61.9334, POS_NLL_Loss   18.5115, TEXT_NLL_Loss   37.7157, KL_Weight    1.0000, KL_Loss    5.7063\n",
      "TRAIN Batch 1150/1393, Loss   62.1319, POS_NLL_Loss   18.5594, TEXT_NLL_Loss   38.5330, KL_Weight    1.0000, KL_Loss    5.0395\n",
      "TRAIN Batch 1200/1393, Loss   59.3443, POS_NLL_Loss   18.7615, TEXT_NLL_Loss   35.9061, KL_Weight    1.0000, KL_Loss    4.6766\n",
      "TRAIN Batch 1250/1393, Loss   61.4496, POS_NLL_Loss   17.8784, TEXT_NLL_Loss   38.5267, KL_Weight    1.0000, KL_Loss    5.0445\n",
      "TRAIN Batch 1300/1393, Loss   73.9674, POS_NLL_Loss   22.7495, TEXT_NLL_Loss   45.5537, KL_Weight    1.0000, KL_Loss    5.6642\n",
      "TRAIN Batch 1350/1393, Loss   54.3322, POS_NLL_Loss   15.7725, TEXT_NLL_Loss   33.8616, KL_Weight    1.0000, KL_Loss    4.6982\n",
      "TRAIN Batch 1393/1393, Loss   62.6291, POS_NLL_Loss   19.0048, TEXT_NLL_Loss   38.6455, KL_Weight    1.0000, KL_Loss    4.9788\n",
      "TRAIN Epoch 09/10, Mean Loss   63.7517\n",
      "Model saved at /root/user/work/src/Sentence-VAE/runs/posvae_eccos_v2_TS=2020-01-05-173701_BS=32_LR=0.001_EB=300_GRU_HS=256_L=1_BI=0_LS=16_WD=0_ANN=LOGISTIC_K=0.0025_X0=2500/models/model_E9.pytorch\n",
      "VALID Batch 0/77, Loss   68.2788, POS_NLL_Loss   18.9348, TEXT_NLL_Loss   44.5405, KL_Weight    1.0000, KL_Loss    4.8036\n",
      "VALID Batch 50/77, Loss   79.3637, POS_NLL_Loss   19.3227, TEXT_NLL_Loss   54.8733, KL_Weight    1.0000, KL_Loss    5.1677\n",
      "VALID Batch 77/77, Loss   62.4910, POS_NLL_Loss   16.5421, TEXT_NLL_Loss   41.3152, KL_Weight    1.0000, KL_Loss    4.6337\n",
      "VALID Epoch 09/10, Mean Loss   68.7829\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "\n",
    "    for split in splits:\n",
    "        \n",
    "        data_loader = DataLoader(\n",
    "            dataset=_datasets[split],\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=split=='train',\n",
    "            num_workers=cpu_count(),\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "\n",
    "        tracker = defaultdict(Tensor)\n",
    "\n",
    "        # Enable/Disable Dropout\n",
    "        if split == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        for iteration, batch in enumerate(data_loader):\n",
    "            batch_size = batch['src_input'].size(0)\n",
    "            \n",
    "            for k, v in batch.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    batch[k] = to_var(v)\n",
    "            \n",
    "            # model output\n",
    "            b = label_dict = batch\n",
    "            out_dict = model(b['src_input'], b['src_length'], b['pos_input'], b['pos_length'])\n",
    "            z = out_dict['z']\n",
    "            # loss calculation\n",
    "            loss_dict = model.loss(out_dict, label_dict, step, args)\n",
    "            loss = loss_dict['Loss']\n",
    "\n",
    "            # backward + optimization\n",
    "            if split == 'train':\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                step += 1\n",
    "\n",
    "            # for log\n",
    "            loss_value_dict = {}\n",
    "            for _name, _loss in loss_dict.items():\n",
    "                loss_value = _loss.detach().item() if 'detach' in dir(_loss) else _loss\n",
    "                if _name not in ['Loss', 'KL_Weight']:\n",
    "                    loss_value /= batch_size\n",
    "                loss_value_dict[_name] = loss_value\n",
    "                \n",
    "            # bookkeepeing\n",
    "            tracker['Loss'] = torch.cat((tracker['Loss'], loss.detach().view(1)))\n",
    "\n",
    "            if args.tensorboard_logging:\n",
    "                _ = [writer.add_scalar(f'{split.upper()}/{name}', value, epoch*len(data_loader) + iteration) for name, value in loss_value_dict.items()]\n",
    "\n",
    "            if iteration % args.print_every == 0 or iteration+1 == len(data_loader):\n",
    "                print_text = f'{split.upper()} Batch {iteration}/{len(data_loader)-1}'\n",
    "                for k, v in loss_value_dict.items():\n",
    "                    print_text += f', {k} {v:9.4f}'\n",
    "                print(print_text)\n",
    "\n",
    "            if split == 'valid':\n",
    "                def add_tokens_tracker(id_key, token_key, ids, dataset_key, sep=''):\n",
    "                    tracker[id_key] = torch.cat((tracker.get(id_key, LongTensor()), ids.detach()), dim=0)\n",
    "                    tracker[token_key] = tracker.get(token_key, []) + [ids2ptext(text_ids, datasets[dataset_key].i2w, sep=sep) for text_ids in ids]\n",
    "                \n",
    "                add_tokens_tracker('target_text_ids', 'target_texts', batch['tgt_target'].data, ('train', 'tgt'))\n",
    "                add_tokens_tracker('target_pos_ids', 'target_poses', batch['pos_target'].data, ('train', 'pos'), sep=' ')\n",
    "                tracker['z'] = torch.cat((tracker['z'], z.detach()), dim=0)\n",
    "                with torch.no_grad():\n",
    "                    # Inference\n",
    "                    pos_decoded_ids = model.pos_inference(z=z) # z → POS\n",
    "                    pos_decoded_input, pos_decoded_length = model.target2input(pos_decoded_ids)  # POS → POS Input\n",
    "                    text_decoded_ids = model.text_inference(pos_decoded_input, pos_decoded_length, z) # POS Input → Text\n",
    "                    add_tokens_tracker('decoded_text_ids', 'decoded_texts',  text_decoded_ids, ('train', 'tgt'))\n",
    "                    add_tokens_tracker('decoded_pos_ids', 'decoded_poses',  pos_decoded_ids, ('train', 'pos'), sep=' ')\n",
    "\n",
    "        print(\"%s Epoch %02d/%i, Mean Loss %9.4f\"%(split.upper(), epoch, args.epochs, torch.mean(tracker['Loss'])))\n",
    "\n",
    "        if args.tensorboard_logging:\n",
    "            writer.add_scalar(\"%s-Epoch/Loss\"%split.upper(), torch.mean(tracker['Loss']), epoch)\n",
    "        \n",
    "        if split == 'valid':\n",
    "            decoded_id_list = remove_pad_index(tracker['decoded_text_ids'])\n",
    "            valid_tgt_id_list = remove_pad_index(tracker['target_text_ids'])\n",
    "            train_tgt_id_list = remove_pad_index([d['tgt_target'] for d in _datasets['train']]) # コピー率用\n",
    "            write_tensorboard_valid_metric(writer, valid_tgt_id_list, decoded_id_list, train_tgt_id_list, datasets[('train', 'tgt')].i2w, split, epoch)\n",
    "\n",
    "        # save checkpoint\n",
    "        if split == 'train':\n",
    "            checkpoint_path = os.path.join(save_model_path, f\"model_E{epoch}.pytorch\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(\"Model saved at %s\"%checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
